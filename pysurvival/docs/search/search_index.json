{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"h1, h2, h3 { color: #04A9F4; } pre { color: black !important; } Welcome to pysurvival.io What is PySurvival ? PySurvival is an open source python package for Survival Analysis modeling - the modeling concept used to analyze or predict when an event is likely to happen . It is built upon the most commonly used machine learning packages such NumPy , SciPy and PyTorch . PySurvival is compatible with Python 2.7-3.7. Content PySurvival provides a very easy way to navigate between theoretical knowledge on Survival Analysis and detailed tutorials on how to conduct a full analysis, build and use a model. Indeed, the package contains: 10+ models ranging from the Cox Proportional Hazard model , the Neural Multi-Task Logistic Regression to Random Survival Forest Summaries of the theory behind each model as well as API descriptions and examples. Tutorials displaying in great details how to perform exploratory data analysis, survival modeling, cross-validation and prediction, for churn modeling and credit risk to name a few. Performance metrics to assess the models' abilities like c-index or brier score Simple ways to load and save models ... and more ! Getting started Because of its simple API, PySurvival has been built to provide a great user experience when it comes to modeling. Here's a quick modeling example to get you started: # Loading the modules from pysurvival.models.semi_parametric import CoxPHModel from pysurvival.models.multi_task import LinearMultiTaskModel from pysurvival.datasets import Dataset from pysurvival.utils.metrics import concordance_index # Loading and splitting a simple example into train/test sets X_train, T_train, E_train, X_test, T_test, E_test = \\ Dataset( 'simple_example' ) . load_train_test() # Building a CoxPH model coxph_model = CoxPHModel() coxph_model . fit(X = X_train, T = T_train, E = E_train, init_method = 'he_uniform' , l2_reg = 1e-4 , lr = .4 , tol = 1e-4 ) # Building a MTLR model mtlr = LinearMultiTaskModel() mtlr . fit(X = X_train, T = T_train, E = E_train, init_method = 'glorot_uniform' , optimizer = 'adam' , lr = 8e-4 ) # Checking the model performance c_index1 = concordance_index(model = coxph_model, X = X_test, T = T_test, E = E_test ) print ( \"CoxPH model c-index = {:.2f}\" . format(c_index1)) c_index2 = concordance_index(model = mtlr, X = X_test, T = T_test, E = E_test ) print ( \"MTLR model c-index = {:.2f}\" . format(c_index2)) For additional models and performance metrics, checkout the documentation. Citation If you use Pysurvival in your research and we would greatly appreciate if you could use the following: @Misc{ pysurvival_cite, author = {Stephane Fotso and others}, title = {{PySurvival}: Open source package for Survival Analysis modeling}, year = {2019--}, url = \"https://www.pysurvival.io/\", note = {[Online; accessed ]} }","title":"Home"},{"location":"index.html#welcome-to-pysurvivalio","text":"","title":"Welcome to pysurvival.io"},{"location":"index.html#what-is-pysurvival","text":"PySurvival is an open source python package for Survival Analysis modeling - the modeling concept used to analyze or predict when an event is likely to happen . It is built upon the most commonly used machine learning packages such NumPy , SciPy and PyTorch . PySurvival is compatible with Python 2.7-3.7.","title":"What is PySurvival ?"},{"location":"index.html#content","text":"PySurvival provides a very easy way to navigate between theoretical knowledge on Survival Analysis and detailed tutorials on how to conduct a full analysis, build and use a model. Indeed, the package contains: 10+ models ranging from the Cox Proportional Hazard model , the Neural Multi-Task Logistic Regression to Random Survival Forest Summaries of the theory behind each model as well as API descriptions and examples. Tutorials displaying in great details how to perform exploratory data analysis, survival modeling, cross-validation and prediction, for churn modeling and credit risk to name a few. Performance metrics to assess the models' abilities like c-index or brier score Simple ways to load and save models ... and more !","title":"Content"},{"location":"index.html#getting-started","text":"Because of its simple API, PySurvival has been built to provide a great user experience when it comes to modeling. Here's a quick modeling example to get you started: # Loading the modules from pysurvival.models.semi_parametric import CoxPHModel from pysurvival.models.multi_task import LinearMultiTaskModel from pysurvival.datasets import Dataset from pysurvival.utils.metrics import concordance_index # Loading and splitting a simple example into train/test sets X_train, T_train, E_train, X_test, T_test, E_test = \\ Dataset( 'simple_example' ) . load_train_test() # Building a CoxPH model coxph_model = CoxPHModel() coxph_model . fit(X = X_train, T = T_train, E = E_train, init_method = 'he_uniform' , l2_reg = 1e-4 , lr = .4 , tol = 1e-4 ) # Building a MTLR model mtlr = LinearMultiTaskModel() mtlr . fit(X = X_train, T = T_train, E = E_train, init_method = 'glorot_uniform' , optimizer = 'adam' , lr = 8e-4 ) # Checking the model performance c_index1 = concordance_index(model = coxph_model, X = X_test, T = T_test, E = E_test ) print ( \"CoxPH model c-index = {:.2f}\" . format(c_index1)) c_index2 = concordance_index(model = mtlr, X = X_test, T = T_test, E = E_test ) print ( \"MTLR model c-index = {:.2f}\" . format(c_index2)) For additional models and performance metrics, checkout the documentation.","title":"Getting started"},{"location":"index.html#citation","text":"If you use Pysurvival in your research and we would greatly appreciate if you could use the following: @Misc{ pysurvival_cite, author = {Stephane Fotso and others}, title = {{PySurvival}: Open source package for Survival Analysis modeling}, year = {2019--}, url = \"https://www.pysurvival.io/\", note = {[Online; accessed ]} }","title":"Citation"},{"location":"installation.html","text":"h1, h2, h3 { color: #04A9F4; } pre { color: black !important; } Installation Since this package contains C++ source code, pip needs a C++ compiler to install PySurvival. 1 - Installing C++ compiler On MacOS Install brew - go on http://brew.sh/ and follow the provided instructions Install the latest version of gcc with the command : If you have never installed gcc, use: brew install gcc If you have already installed gcc and want the latest version: brew upgrade gcc Get the list all the gcc files with the command: brew ls gcc Assign the environment variables CC and CXX to the previously found addresses, by entering the following lines in the terminal: export CXX=/usr/local/Cellar/gcc/8.2.0/bin/g++-8 export CC=/usr/local/Cellar/gcc/8.2.0/bin/gcc-8 Note: As of October 18, 2018, the latest version on MacOS is 8.2.0, hence the use of 8.2.0 ; feel free to use the version that matches your situation instead. On Linux CentOS7 Install GCC with the command : sudo yum install centos-release-scl sudo yum install devtoolset-8-gcc devtoolset-8-gcc-c++ -y Note: As of March 2019, the latest version of gcc is 8, hence the use of devtoolset-8 ; feel free to use the version that matches your situation instead. Assign the environment variables CC and CXX to the appropriate files: export CXX=/opt/rh/devtoolset-8/root/usr/bin/x86_64-redhat-linux-g++ export CC=/opt/rh/devtoolset-8/root/usr/bin/x86_64-redhat-linux-gcc Ubuntu Install GCC with the command : sudo apt install gcc-8 g++-8 Note: As of March 2019, the latest version of gcc is 8, hence the use of gcc-8 g++-8 ; feel free to use the version that matches your situation instead. Assign the environment variables CC and CXX to the appropriate files: export CXX=/usr/bin/g++-8 export CC=/usr/bin/gcc-8 2 - Installation via from PyPI (recommended) The easiest way to install pySurvival is to use to pip: pip install pysurvival Potential Issues Issues with matplotlib on MacOS If you encounter the following error: Error with matplotlib RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. You'll need to ensure that there exists a file ~/.matplotlib/matplotlibrc containing the code: backend: TkAgg . To do this, run the following code in your Terminal: cd echo \"backend: TkAgg\" >> ~/.matplotlib/matplotlibrc","title":"Installation"},{"location":"installation.html#installation","text":"Since this package contains C++ source code, pip needs a C++ compiler to install PySurvival.","title":"Installation"},{"location":"installation.html#1-installing-c-compiler","text":"","title":"1 - Installing C++ compiler"},{"location":"installation.html#on-macos","text":"Install brew - go on http://brew.sh/ and follow the provided instructions Install the latest version of gcc with the command : If you have never installed gcc, use: brew install gcc If you have already installed gcc and want the latest version: brew upgrade gcc Get the list all the gcc files with the command: brew ls gcc Assign the environment variables CC and CXX to the previously found addresses, by entering the following lines in the terminal: export CXX=/usr/local/Cellar/gcc/8.2.0/bin/g++-8 export CC=/usr/local/Cellar/gcc/8.2.0/bin/gcc-8 Note: As of October 18, 2018, the latest version on MacOS is 8.2.0, hence the use of 8.2.0 ; feel free to use the version that matches your situation instead.","title":"On MacOS"},{"location":"installation.html#on-linux","text":"","title":"On Linux"},{"location":"installation.html#centos7","text":"Install GCC with the command : sudo yum install centos-release-scl sudo yum install devtoolset-8-gcc devtoolset-8-gcc-c++ -y Note: As of March 2019, the latest version of gcc is 8, hence the use of devtoolset-8 ; feel free to use the version that matches your situation instead. Assign the environment variables CC and CXX to the appropriate files: export CXX=/opt/rh/devtoolset-8/root/usr/bin/x86_64-redhat-linux-g++ export CC=/opt/rh/devtoolset-8/root/usr/bin/x86_64-redhat-linux-gcc","title":"CentOS7"},{"location":"installation.html#ubuntu","text":"Install GCC with the command : sudo apt install gcc-8 g++-8 Note: As of March 2019, the latest version of gcc is 8, hence the use of gcc-8 g++-8 ; feel free to use the version that matches your situation instead. Assign the environment variables CC and CXX to the appropriate files: export CXX=/usr/bin/g++-8 export CC=/usr/bin/gcc-8","title":"Ubuntu"},{"location":"installation.html#2-installation-via-from-pypi-recommended","text":"The easiest way to install pySurvival is to use to pip: pip install pysurvival","title":"2 - Installation via from PyPI (recommended)"},{"location":"installation.html#potential-issues","text":"","title":"Potential Issues"},{"location":"installation.html#issues-with-matplotlib-on-macos","text":"If you encounter the following error: Error with matplotlib RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. You'll need to ensure that there exists a file ~/.matplotlib/matplotlibrc containing the code: backend: TkAgg . To do this, run the following code in your Terminal: cd echo \"backend: TkAgg\" >> ~/.matplotlib/matplotlibrc","title":"Issues with matplotlib on MacOS"},{"location":"intro.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Introduction to Survival Analysis Introduction Survival analysis is used to analyze or predict when an event is likely to happen. It originated from medical research, but its use has greatly expanded to many different fields. For instance: banks, lenders and other financial institutions use it to compute the speed of repayment of loans or when a borrower will default businesses adopt it to calculate their customers LTV (lifetime value) or when a client will churn companies use it to predict when employees will decide to leave engineers/manufacturers apply it to predict when a machine will break Censoring: why regression models cannot be used? The real strength of Survival Analysis is its capacity to handle situations when the event has not happened yet. To illustrate this, let's take the example of two customers of a company and follow their active/churn status between January 2018 and April 2018: customer A started doing business prior to the time window, and as of April 2018, is still a client of the company. customer B also started doing business before January 2018, but churned in March 2018. Figure 1 - Example of censoring Here, we have an explicit depiction of the event for customer B. However, we have no information about customer A, except that he/she hasn't churned yet at the end of the January 2018 to April 2018 time window. This situation is called censoring . One might be tempted to use a regression model to predict when events are likely to happen. But to do that, one would need to disregard censored samples, which would result in a loss of important information. Fortunately, Survival models are able to take censoring into account and incorporate this uncertainty, so that instead of predicting the time of event, we are predicting the probability that an event happens at a particular time . Data format We characterize survival analysis data-points with 3 elements: \\left( X_i, E_i, T_i \\right) , \\forall i , X_i is a p\u2212dimensional feature vector. E_i is the event indicator such that E_i=1 , if an event happens and E_i=0 in case of censoring. T_i = \\min(t_i,c_i) is the observed time, with t_i the actual event time and c_i the time of censoring. This configuration differs from regression modeling, where a data-point is defined by \\left( X_i, y_i \\right) and y_i is the target variable. This means that to fit a model, you will need to provide those 3 elements. Let's look at the difference between a regression model fit and survival analysis one: Modeling type code Regression model using sklearn from sklearn.linear_model import LinearRegression reg = LinearRegression() reg.fit(X=X_train, y=y_train) Survival analysis using pysurvival from pysurvival.models.multi_task import LinearMultiTaskModel mtlr = LinearMultiTaskModel() mtlr.fit(X=X_train, T=T_train, E=E_train)","title":"Introduction to Survival Analysis"},{"location":"intro.html#introduction-to-survival-analysis","text":"","title":"Introduction to Survival Analysis"},{"location":"intro.html#introduction","text":"Survival analysis is used to analyze or predict when an event is likely to happen. It originated from medical research, but its use has greatly expanded to many different fields. For instance: banks, lenders and other financial institutions use it to compute the speed of repayment of loans or when a borrower will default businesses adopt it to calculate their customers LTV (lifetime value) or when a client will churn companies use it to predict when employees will decide to leave engineers/manufacturers apply it to predict when a machine will break","title":"Introduction"},{"location":"intro.html#censoring-why-regression-models-cannot-be-used","text":"The real strength of Survival Analysis is its capacity to handle situations when the event has not happened yet. To illustrate this, let's take the example of two customers of a company and follow their active/churn status between January 2018 and April 2018: customer A started doing business prior to the time window, and as of April 2018, is still a client of the company. customer B also started doing business before January 2018, but churned in March 2018. Figure 1 - Example of censoring Here, we have an explicit depiction of the event for customer B. However, we have no information about customer A, except that he/she hasn't churned yet at the end of the January 2018 to April 2018 time window. This situation is called censoring . One might be tempted to use a regression model to predict when events are likely to happen. But to do that, one would need to disregard censored samples, which would result in a loss of important information. Fortunately, Survival models are able to take censoring into account and incorporate this uncertainty, so that instead of predicting the time of event, we are predicting the probability that an event happens at a particular time .","title":"Censoring: why regression models cannot be used?"},{"location":"intro.html#data-format","text":"We characterize survival analysis data-points with 3 elements: \\left( X_i, E_i, T_i \\right) , \\forall i , X_i is a p\u2212dimensional feature vector. E_i is the event indicator such that E_i=1 , if an event happens and E_i=0 in case of censoring. T_i = \\min(t_i,c_i) is the observed time, with t_i the actual event time and c_i the time of censoring. This configuration differs from regression modeling, where a data-point is defined by \\left( X_i, y_i \\right) and y_i is the target variable. This means that to fit a model, you will need to provide those 3 elements. Let's look at the difference between a regression model fit and survival analysis one: Modeling type code Regression model using sklearn from sklearn.linear_model import LinearRegression reg = LinearRegression() reg.fit(X=X_train, y=y_train) Survival analysis using pysurvival from pysurvival.models.multi_task import LinearMultiTaskModel mtlr = LinearMultiTaskModel() mtlr.fit(X=X_train, T=T_train, E=E_train)","title":"Data format"},{"location":"math.html","text":"h1, h2, h3, h4 { color: #04A9F4; } The math of Survival Analysis Now that we have introduced the main notions in Survival Analysis , let's define the variables and functions that we will be using and give simple examples to provide additional insight: T , Survival Time T is a positive random variable representing the waiting time until an event occurs. Its probability density function (p.d.f.) is f(t) and cumulative distribution function (c.d.f.) is given by \\begin{equation*} F(t) = \\text{Pr} \\left[ T < t \\right] = \\int_{-\\infty}^{t} f(u) du \\end{equation*} Example: Let's take the example of credit risk and assume that the event of interest is fully repaying a loan . We can now analyze the cumulative distribution function of two distinct borrowers through time. Figure 1 - Comparing cumulative distribution functions Here, we can see that the probability that Borrower B has fully repaid his/her loan reaches 50% or 80% much faster than Borrower A's. This indicates that Borrower B is potentially less risky than Borrower A. S(t, x) , Survival function S(t) is the probability that the event of interest has not occurred by some time t \\begin{equation*} S(t) = 1 - F(t) = \\text{Pr} \\left[ T \\geq t\\right] \\end{equation*} Example: Here, we will consider the example of churn modeling , assume that the event of interest is stopping the SaaS subscription and analyze the survival function of three distinct customers through time. Figure 2 - Comparing survival functions Here, we can see that the probability of remaining a customer reaches 50% much faster for Client C than Client B. On the other hand, Client A's probability doesn't even go below 60% from week 0 to week 15 of the analysis. In a nutshell, Client C is very likely to churn within the first 2 weeks Client B is likely to churn within the next 15 weeks Client A is very likely to remain a customer within the next 15 weeks. h(t, x) , hazard function and r(x) risk score h(t) expresses the conditional probability that the event will occur within [t, t+dt) , given that it has not occurred before. \\begin{equation*} h(t) = \\lim_{dt \\to 0} \\frac{\\text{Pr} \\left[ t \\leq T < t + dt\\ | T \\geq t\\right ] }{dt} = \\frac{f(t)}{S(t)} = -\\frac{d}{dt}\\log S(t) \\end{equation*} Thus, the hazard and Survival functions are linked by the following formula: \\begin{equation*} S(t) = \\exp\\left(- \\int_{0}^{t} h(u) du \\right) \\end{equation*} where H(t) = \\int_{0}^{t} h(u) du is the cumulative hazard function However, the hazard function is rarely used in its original form. Most of the time, we subdivide the time axis in J parts and calculate the risk score of a sample x , such that: \\begin{equation*} r(x) = \\sum_{j=1}^J H(t_j, x) \\end{equation*} Example: Let's reuse our churn example. The previous conclusion can be translated into risk scores such that: r(x_A) < r(x_B) < r(x_C) . Indeed the faster you experience the event, the higher your risk score is.","title":"The math of Survival Analysis"},{"location":"math.html#the-math-of-survival-analysis","text":"Now that we have introduced the main notions in Survival Analysis , let's define the variables and functions that we will be using and give simple examples to provide additional insight:","title":"The math of Survival Analysis"},{"location":"math.html#t-survival-time","text":"T is a positive random variable representing the waiting time until an event occurs. Its probability density function (p.d.f.) is f(t) and cumulative distribution function (c.d.f.) is given by \\begin{equation*} F(t) = \\text{Pr} \\left[ T < t \\right] = \\int_{-\\infty}^{t} f(u) du \\end{equation*} Example: Let's take the example of credit risk and assume that the event of interest is fully repaying a loan . We can now analyze the cumulative distribution function of two distinct borrowers through time. Figure 1 - Comparing cumulative distribution functions Here, we can see that the probability that Borrower B has fully repaid his/her loan reaches 50% or 80% much faster than Borrower A's. This indicates that Borrower B is potentially less risky than Borrower A.","title":"T, Survival Time"},{"location":"math.html#st-x-survival-function","text":"S(t) is the probability that the event of interest has not occurred by some time t \\begin{equation*} S(t) = 1 - F(t) = \\text{Pr} \\left[ T \\geq t\\right] \\end{equation*} Example: Here, we will consider the example of churn modeling , assume that the event of interest is stopping the SaaS subscription and analyze the survival function of three distinct customers through time. Figure 2 - Comparing survival functions Here, we can see that the probability of remaining a customer reaches 50% much faster for Client C than Client B. On the other hand, Client A's probability doesn't even go below 60% from week 0 to week 15 of the analysis. In a nutshell, Client C is very likely to churn within the first 2 weeks Client B is likely to churn within the next 15 weeks Client A is very likely to remain a customer within the next 15 weeks.","title":"S(t, x), Survival function"},{"location":"math.html#ht-x-hazard-function-and-rx-risk-score","text":"h(t) expresses the conditional probability that the event will occur within [t, t+dt) , given that it has not occurred before. \\begin{equation*} h(t) = \\lim_{dt \\to 0} \\frac{\\text{Pr} \\left[ t \\leq T < t + dt\\ | T \\geq t\\right ] }{dt} = \\frac{f(t)}{S(t)} = -\\frac{d}{dt}\\log S(t) \\end{equation*} Thus, the hazard and Survival functions are linked by the following formula: \\begin{equation*} S(t) = \\exp\\left(- \\int_{0}^{t} h(u) du \\right) \\end{equation*} where H(t) = \\int_{0}^{t} h(u) du is the cumulative hazard function However, the hazard function is rarely used in its original form. Most of the time, we subdivide the time axis in J parts and calculate the risk score of a sample x , such that: \\begin{equation*} r(x) = \\sum_{j=1}^J H(t_j, x) \\end{equation*} Example: Let's reuse our churn example. The previous conclusion can be translated into risk scores such that: r(x_A) < r(x_B) < r(x_C) . Indeed the faster you experience the event, the higher your risk score is.","title":"h(t, x), hazard function and r(x) risk score"},{"location":"metrics/brier_score.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Brier Score and Integrated Brier Score Brier Score The Brier score is used to evaluate the accuracy of a predicted survival function at a given time t ; it represents the average squared distances between the observed survival status and the predicted survival probability and is always a number between 0 and 1, with 0 being the best possible value. Given a dataset of N samples, \\forall i \\in [\\![1, N ]\\!], \\left(\\vec{x}_i, \\delta_i, T_i \\right) is the format of a datapoint, and the predicted survival function is \\hat{S}(t, \\vec{x}_i), \\forall t \\in \\mathbb{R^+} : In the absence of right censoring, the Brier score can be calculated such that: \\begin{equation} BS(t) = \\frac{1}{N} \\sum_{i = 1}^{N} (\\mathbb{1}_{ T_i > t } - \\hat{S}(t, \\vec{x}_i))^2 \\end{equation} However, if the dataset contains samples that are right censored, then it is necessary to adjust the score by weighting the squared distances using the inverse probability of censoring weights method. Let \\hat{G}(t) = P[C > t ] be the estimator of the conditional survival function of the censoring times calculated using the Kaplan-Meier method, where C is the censoring time. \\begin{equation} BS(t) = \\frac{1}{N} \\sum_{i = 1}^{N} \\left( \\frac{\\left( 0 - \\hat{S}(t, \\vec{x}_i)\\right)^2 \\cdot \\mathbb{1}_{T_i \\leq t, \\delta_i = 1}}{ \\hat{G}(T_i^-)} + \\frac{ \\left( 1 - \\hat{S}(t, \\vec{x}_i)\\right)^2 \\cdot \\mathbb{1}_{T_i > t}}{ \\hat{G}(t)} \\right) \\end{equation} In terms of benchmarks, a useful model will have a Brier score below 0.25 . Indeed, it is easy to see that if \\forall i \\in [\\![1, N]\\!], \\hat{S}(t, \\vec{x}_i) = 0.5 , then BS(t) = 0.25 . Location The function can be found at pysurvival.utils.metrics.brier_score . API brier_score - Brier score computations brier_score(model, X, T, E, t_max=None) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. t_max : float ( default=None ) Maximal time for estimating the prediction error curves. If missing the largest value of the response variable is used. Returns: times : array-like. A vector of timepoints. At each timepoint the brier score is estimated brier_scores : array-like. A vector of brier scores Integrated Brier Score The Integrated Brier Score (IBS) provides an overall calculation of the model performance at all available times. \\begin{equation} \\text{IBS}(t_{\\text{max}}) = \\frac{1}{t_{\\text{max}}} \\int_{0}^{t_{\\text{max}}} BS(t) dt \\end{equation} Location The function can be found at pysurvival.utils.metrics.integrated_brier_score to output the values and pysurvival.utils.display.integrated_brier_score to display the predictive error curve. API integrated_brier_score - Integrated Brier score computations integrated_brier_score(model, X, T, E, t_max=None, figure_size=(20, 10)) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. t_max : float ( default=None ) Maximal time for estimating the prediction error curves. If missing the largest value of the response time variable is used. figure_size : tuple of double ( default=(20, 10) ) width, height in inches representing the size of the chart of the survival function. Option available if the function is being called from pysurvival.utils.display Returns: ibs : double The integrated brier score","title":"Brier Score"},{"location":"metrics/brier_score.html#brier-score-and-integrated-brier-score","text":"","title":"Brier Score and Integrated Brier Score"},{"location":"metrics/brier_score.html#brier-score","text":"The Brier score is used to evaluate the accuracy of a predicted survival function at a given time t ; it represents the average squared distances between the observed survival status and the predicted survival probability and is always a number between 0 and 1, with 0 being the best possible value. Given a dataset of N samples, \\forall i \\in [\\![1, N ]\\!], \\left(\\vec{x}_i, \\delta_i, T_i \\right) is the format of a datapoint, and the predicted survival function is \\hat{S}(t, \\vec{x}_i), \\forall t \\in \\mathbb{R^+} : In the absence of right censoring, the Brier score can be calculated such that: \\begin{equation} BS(t) = \\frac{1}{N} \\sum_{i = 1}^{N} (\\mathbb{1}_{ T_i > t } - \\hat{S}(t, \\vec{x}_i))^2 \\end{equation} However, if the dataset contains samples that are right censored, then it is necessary to adjust the score by weighting the squared distances using the inverse probability of censoring weights method. Let \\hat{G}(t) = P[C > t ] be the estimator of the conditional survival function of the censoring times calculated using the Kaplan-Meier method, where C is the censoring time. \\begin{equation} BS(t) = \\frac{1}{N} \\sum_{i = 1}^{N} \\left( \\frac{\\left( 0 - \\hat{S}(t, \\vec{x}_i)\\right)^2 \\cdot \\mathbb{1}_{T_i \\leq t, \\delta_i = 1}}{ \\hat{G}(T_i^-)} + \\frac{ \\left( 1 - \\hat{S}(t, \\vec{x}_i)\\right)^2 \\cdot \\mathbb{1}_{T_i > t}}{ \\hat{G}(t)} \\right) \\end{equation} In terms of benchmarks, a useful model will have a Brier score below 0.25 . Indeed, it is easy to see that if \\forall i \\in [\\![1, N]\\!], \\hat{S}(t, \\vec{x}_i) = 0.5 , then BS(t) = 0.25 .","title":"Brier Score"},{"location":"metrics/brier_score.html#location","text":"The function can be found at pysurvival.utils.metrics.brier_score .","title":"Location"},{"location":"metrics/brier_score.html#api","text":"brier_score - Brier score computations brier_score(model, X, T, E, t_max=None) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. t_max : float ( default=None ) Maximal time for estimating the prediction error curves. If missing the largest value of the response variable is used. Returns: times : array-like. A vector of timepoints. At each timepoint the brier score is estimated brier_scores : array-like. A vector of brier scores","title":"API"},{"location":"metrics/brier_score.html#integrated-brier-score","text":"The Integrated Brier Score (IBS) provides an overall calculation of the model performance at all available times. \\begin{equation} \\text{IBS}(t_{\\text{max}}) = \\frac{1}{t_{\\text{max}}} \\int_{0}^{t_{\\text{max}}} BS(t) dt \\end{equation}","title":"Integrated Brier Score"},{"location":"metrics/brier_score.html#location_1","text":"The function can be found at pysurvival.utils.metrics.integrated_brier_score to output the values and pysurvival.utils.display.integrated_brier_score to display the predictive error curve.","title":"Location"},{"location":"metrics/brier_score.html#api_1","text":"integrated_brier_score - Integrated Brier score computations integrated_brier_score(model, X, T, E, t_max=None, figure_size=(20, 10)) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. t_max : float ( default=None ) Maximal time for estimating the prediction error curves. If missing the largest value of the response time variable is used. figure_size : tuple of double ( default=(20, 10) ) width, height in inches representing the size of the chart of the survival function. Option available if the function is being called from pysurvival.utils.display Returns: ibs : double The integrated brier score","title":"API"},{"location":"metrics/c_index.html","text":"h1, h2, h3, h4 { color: #04A9F4; } C-index Introduction The concordance index or C-index is a generalization of the area under the ROC curve (AUC) that can take into account censored data. It represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores. It can be computed with the following formula : \\begin{equation} \\text{C-index} = \\frac{ \\sum_{i, j} \\mathbb{1}_{T_j < T_i} \\cdot \\mathbb{1}_{\\eta_j > \\eta_i} \\cdot \\delta_j }{\\sum_{i, j} \\mathbb{1}_{T_j < T_i}\\cdot \\delta_j } \\end{equation} with: \\eta_i , the risk score of a unit i \\mathbb{1}_{ T_j < T_i } = 1 if T_j < T_i else 0 \\mathbb{1}_{ \\eta_j > \\eta_i } = 1 if \\eta_j > \\eta_i else 0 Similarly to the AUC, \\text{C-index}= 1 corresponds to the best model prediction, and \\text{C-index} = 0.5 represents a random prediction. Location The function can be found at pysurvival.utils.metrics.concordance_index . API concordance_index - Concordance Index computations concordance_index(model, X, T, E, include_ties = True, additional_results=False) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. include_ties : bool (default=True) -- Specifies whether ties in risk score are included in calculations additional_results : bool (default=False) -- Specifies whether only the c-index should be returned (False) or if a dict of values should returned. In that case, the values are: c_index nb_pairs nb_concordant_pairs Returns: c_index : float or dict -- Result of the function if additional_results = False then c_index is float . if additional_results = True then c_index is dict , such that c_index = {'c_index': ., 'nb_pairs': ., 'nb_concordant_pairs': .} References: Uno, Hajime et al. \u201cOn the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data\u201d Statistics in medicine vol. 30,10 (2011): 1105-17.","title":"C-index"},{"location":"metrics/c_index.html#c-index","text":"","title":"C-index"},{"location":"metrics/c_index.html#introduction","text":"The concordance index or C-index is a generalization of the area under the ROC curve (AUC) that can take into account censored data. It represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores. It can be computed with the following formula : \\begin{equation} \\text{C-index} = \\frac{ \\sum_{i, j} \\mathbb{1}_{T_j < T_i} \\cdot \\mathbb{1}_{\\eta_j > \\eta_i} \\cdot \\delta_j }{\\sum_{i, j} \\mathbb{1}_{T_j < T_i}\\cdot \\delta_j } \\end{equation} with: \\eta_i , the risk score of a unit i \\mathbb{1}_{ T_j < T_i } = 1 if T_j < T_i else 0 \\mathbb{1}_{ \\eta_j > \\eta_i } = 1 if \\eta_j > \\eta_i else 0 Similarly to the AUC, \\text{C-index}= 1 corresponds to the best model prediction, and \\text{C-index} = 0.5 represents a random prediction.","title":"Introduction"},{"location":"metrics/c_index.html#location","text":"The function can be found at pysurvival.utils.metrics.concordance_index .","title":"Location"},{"location":"metrics/c_index.html#api","text":"concordance_index - Concordance Index computations concordance_index(model, X, T, E, include_ties = True, additional_results=False) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. include_ties : bool (default=True) -- Specifies whether ties in risk score are included in calculations additional_results : bool (default=False) -- Specifies whether only the c-index should be returned (False) or if a dict of values should returned. In that case, the values are: c_index nb_pairs nb_concordant_pairs Returns: c_index : float or dict -- Result of the function if additional_results = False then c_index is float . if additional_results = True then c_index is dict , such that c_index = {'c_index': ., 'nb_pairs': ., 'nb_concordant_pairs': .}","title":"API"},{"location":"metrics/c_index.html#references","text":"Uno, Hajime et al. \u201cOn the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data\u201d Statistics in medicine vol. 30,10 (2011): 1105-17.","title":"References:"},{"location":"miscellaneous/activation_functions.html","text":"Here is the list of all the activation functions currently available: Function Representation Atan \\begin{equation} f(x) = \\text{atan}(x) \\end{equation} BentIdentity \\begin{equation} f(x) = \\frac{\\sqrt{x^2 + 1} -1 }{2}+ x \\end{equation} BipolarSigmoid \\begin{equation} f(x) = \\frac{1- e^{-x}}{1+ e^x} \\end{equation} ELU \\begin{equation} f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha (e^x -1), & \\text{if } x \\leq 0 \\\\\\end{cases} \\end{equation} Gaussian \\begin{equation} f(x) = \\exp(-x^2) \\end{equation} Hardtanh \\begin{equation} f(x) = \\begin{cases} +1, & \\text{if } x > 1 \\\\ -1, & \\text{if } x < 1 \\\\ x, & \\text{otherwise} \\\\\\end{cases} \\end{equation} Identity \\begin{equation} f(x) = x \\end{equation} InverseSqrt \\begin{equation} f(x) = \\frac{x}{ \\sqrt{1 + \\alpha x^2}} \\end{equation} LeakyReLU \\begin{equation} f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ 0.01x, & \\text{if } x \\leq 0 \\\\\\end{cases} \\end{equation} LeCunTanh \\begin{equation} f(x) = 1.7159 \\tanh \\left( \\frac{2}{3} x \\right) \\end{equation} LogLog \\begin{equation} f(x) = 1 -\\exp\\left( - \\exp(x) \\right) \\end{equation} LogSigmoid \\begin{equation} f(x) = \\log\\left( \\frac{1}{1 + \\exp(-x)} \\right) \\end{equation} ReLU \\begin{equation} f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ 0, & \\text{if } x \\leq 0 \\\\\\end{cases} \\end{equation} SELU \\begin{equation} f(x) = 1.0507 \\times \\begin{cases} x, & \\text{if } x > 0 \\\\ 1.67326(e^x -1), & \\text{if } x \\leq 0 \\\\\\end{cases} \\end{equation} Sigmoid \\begin{equation} f(x) = \\frac{1}{1 + \\exp(-x)} \\end{equation} Sinc \\begin{equation} f(x) = \\begin{cases} \\frac{sin(x)}{x}, & \\text{if } x \\neq 0 \\\\ 1, & \\text{if } x = 0 \\\\\\end{cases} \\end{equation} Softmax \\begin{equation} f( \\vec{x} ) = \\left[ \\frac{\\exp(x_1)}{\\sum_{k=1}^K \\exp(x_k)}, \\frac{\\exp(x_2)}{\\sum_{k=1}^K \\exp(x_k)}, ... ,\\frac{\\exp(x_K)}{\\sum_{k=1}^K \\exp(x_k)} \\right] \\end{equation} with \\vec{x} = [x_1, x_2, ..., x_K] Softplus \\begin{equation} f(x) = \\log\\left( 1 + \\exp(x) \\right) \\end{equation} Softsign \\begin{equation} f(x) = \\frac{x}{1 + \\mid x \\mid} \\end{equation} Swish \\begin{equation} f(x) = \\frac{x}{ 1 + \\exp(-x)} \\end{equation} Tanh \\begin{equation} f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\end{equation}","title":"Activation Functions"},{"location":"miscellaneous/save_load.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Saving and Loading a model Saving or Loading a model is very straighforward in pySurvival. Saving To save a model, use the function save_model and provide the full path of the future location of the file as the argument; the model is then compressed into a .zip file. The function is located at pysurvival.utils.save_model . API save_model - Save and compress the model and its parameters into a .zip file save_model(path_file) Parameters: model : Pysurvival object -- Pysurvival model path_file , str -- full address of the file where the model will be saved Example # Importing modules from pysurvival.models.svm import KernelSVMModel from pysurvival.datasets import Dataset # Loading and splitting a simple example into train/test sets X_train, T_train, E_train, \\ X_test, T_test, E_test = Dataset( 'simple_example' ) . load_train_test() # Let's assume we want to build the following SVM model svm_model = KernelSVMModel( 'gaussian' ) svm_model . fit(X_train, T_train, E_train, init_method = 'glorot_uniform' , l2_reg = 1e-5 , lr = 0.5 ) # Let's now save our model from pysurvival.utils import save_model save_model(svm_model, '/Users/xxx/Desktop/svm_model_2018_08_26.zip' ) Loading To load a model, use the function load_model and provide the full path of the location of the file as the argument. The function is located at pysurvival.utils.load_model . API load_model - Load the model and its parameters from a .zip file load_model(path_file) Parameters: path_file : str -- full address of the file where the model will be loaded from Example # Let's assume we have built and saved a SVM model at the following location # /Users/xxx/Desktop/svm_model_2018_08_26.zip from pysurvival.utils import load_model svm_model = load_model( '/Users/xxx/Desktop/svm_model_2018_08_26.zip' )","title":"Saving/Loading models"},{"location":"miscellaneous/save_load.html#saving-and-loading-a-model","text":"Saving or Loading a model is very straighforward in pySurvival.","title":"Saving and Loading a model"},{"location":"miscellaneous/save_load.html#saving","text":"To save a model, use the function save_model and provide the full path of the future location of the file as the argument; the model is then compressed into a .zip file. The function is located at pysurvival.utils.save_model .","title":"Saving"},{"location":"miscellaneous/save_load.html#api","text":"save_model - Save and compress the model and its parameters into a .zip file save_model(path_file) Parameters: model : Pysurvival object -- Pysurvival model path_file , str -- full address of the file where the model will be saved","title":"API"},{"location":"miscellaneous/save_load.html#example","text":"# Importing modules from pysurvival.models.svm import KernelSVMModel from pysurvival.datasets import Dataset # Loading and splitting a simple example into train/test sets X_train, T_train, E_train, \\ X_test, T_test, E_test = Dataset( 'simple_example' ) . load_train_test() # Let's assume we want to build the following SVM model svm_model = KernelSVMModel( 'gaussian' ) svm_model . fit(X_train, T_train, E_train, init_method = 'glorot_uniform' , l2_reg = 1e-5 , lr = 0.5 ) # Let's now save our model from pysurvival.utils import save_model save_model(svm_model, '/Users/xxx/Desktop/svm_model_2018_08_26.zip' )","title":"Example"},{"location":"miscellaneous/save_load.html#loading","text":"To load a model, use the function load_model and provide the full path of the location of the file as the argument. The function is located at pysurvival.utils.load_model .","title":"Loading"},{"location":"miscellaneous/save_load.html#api_1","text":"load_model - Load the model and its parameters from a .zip file load_model(path_file) Parameters: path_file : str -- full address of the file where the model will be loaded from","title":"API"},{"location":"miscellaneous/save_load.html#example_1","text":"# Let's assume we have built and saved a SVM model at the following location # /Users/xxx/Desktop/svm_model_2018_08_26.zip from pysurvival.utils import load_model svm_model = load_model( '/Users/xxx/Desktop/svm_model_2018_08_26.zip' )","title":"Example"},{"location":"miscellaneous/tips.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Tips on building models Reaching complete optimization It is advised to check the loss function values of the model once fitted to ensure that the best model parameters were computed. This can be done with the function pysurvival.utils.display.display_loss_values . If the loss function reached a minimum, it will display a plateau after a certain number of epochs. If it is not the case the model will provide sub-optimal results. To solve this, it is advised to : either increase the learning rate lr or increase the number of epochs num_epochs . Let's illustrate this with an example: Here we will build a model with a low learning rate; as a result the model will yield poor performances from pysurvival.models.semi_parametric import NonLinearCoxPHModel from pysurvival.utils.display import display_loss_values from pysurvival.utils.metrics import concordance_index from pysurvival.datasets import Dataset % pylab inline # Loading and splitting a simple example into train/test sets X_train, T_train, E_train, \\ X_test, T_test, E_test = Dataset( 'simple_example' ) . load_train_test() # Let's build a Nonlinear CoxPH model structure = [ { 'activation' : 'Atan' , 'num_units' : 150 }, ] nonlinear_coxph = NonLinearCoxPHModel(structure = structure) nonlinear_coxph . fit(X_train, T_train, E_train, lr =1e-6 , dropout =0. ) # We can now display the loss function values in respect to the epochs display_loss_values(nonlinear_coxph) Figure 1 - Loss values of a model with sub-optimal performances # Computing the c-index c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test) #0.57 print ( 'C-index: {:.2f}' . format(c_index)) The c-index is 0.57, it is very likely that the model will yield poor results. By speeding the optimization, we managed to get a much better model. To achieve this, we can simply use a higher learning rate. # Let's rebuild the model with a bigger learning # This will accelerate the optimization nonlinear_coxph . fit(X_train, T_train, E_train, lr =1e-3 , dropout =0. ) # We can now display the loss function values in respect to the epochs display_loss_values(nonlinear_coxph) Figure 2 - Loss values of a model with better performances # Computing the c-index c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test) #0.97 print ( 'C-index: {:.2f}' . format(c_index)) The c-index is 0.97, and we can see that the loss function has plateaued around epoch 400. Actual vs Predicted Once a model is built, it is always a good idea to compare the time series of the actual and predicted number of units that experienced the event at each time t. To do so, we compute the actual density/survival function of the data, which can be obtained using the Kaplan-Meier estimator and compare it to the average of all predicted density/survival functions. The function pysurvival.utils.metrics.compare_to_actual can provide the comparison as well as the performance metrics between the two time series, such as: RMSE Median Absolute Error Mean Absolute Error The function pysurvival.utils.display.compare_to_actual will also provide the charts. API compare_to_actual - Comparing the actual and predicted number of units at risk and units experiencing an event at each time t. compare_to_actual(model, X, T, E, times = None, is_at_risk = False, figure_size=(16, 6), metrics = ['rmse', 'mean', 'median']) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. times : array-like -- (default=None) A vector of time-points. is_at_risk : bool -- (default=True) Whether the function returns Expected number of units at risk or the Expected number of units experiencing the events. figure_size : tuple of double -- (default= (16, 6)) width, height in inches representing the size of the chart metrics : str or list of str (default='all') Indicates the performance metrics to compute: if None, then no metric is computed if str, then the metric is computed if list of str, then all the given metrics are computed The available metrics are: RMSE: root mean squared error Mean Abs Error: mean absolute error Median Abs Error: median absolute error Returns: results: dict -- dictionary containing the performance metrics Example All the tutorials display an instance of how to use the function. Let's take the example of the Credit Risk tutorial : the time series of the actual and predicted number of loans that were fully repaid, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 3 - Actual vs Predicted - Number of loans that were fully repaid the time series of the actual and predicted number of loans that were still active, for each time t. results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = True , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 4 - Actual vs Predicted - Number of loans that were still active","title":"Tips for fitting models"},{"location":"miscellaneous/tips.html#tips-on-building-models","text":"","title":"Tips on building models"},{"location":"miscellaneous/tips.html#reaching-complete-optimization","text":"It is advised to check the loss function values of the model once fitted to ensure that the best model parameters were computed. This can be done with the function pysurvival.utils.display.display_loss_values . If the loss function reached a minimum, it will display a plateau after a certain number of epochs. If it is not the case the model will provide sub-optimal results. To solve this, it is advised to : either increase the learning rate lr or increase the number of epochs num_epochs . Let's illustrate this with an example: Here we will build a model with a low learning rate; as a result the model will yield poor performances from pysurvival.models.semi_parametric import NonLinearCoxPHModel from pysurvival.utils.display import display_loss_values from pysurvival.utils.metrics import concordance_index from pysurvival.datasets import Dataset % pylab inline # Loading and splitting a simple example into train/test sets X_train, T_train, E_train, \\ X_test, T_test, E_test = Dataset( 'simple_example' ) . load_train_test() # Let's build a Nonlinear CoxPH model structure = [ { 'activation' : 'Atan' , 'num_units' : 150 }, ] nonlinear_coxph = NonLinearCoxPHModel(structure = structure) nonlinear_coxph . fit(X_train, T_train, E_train, lr =1e-6 , dropout =0. ) # We can now display the loss function values in respect to the epochs display_loss_values(nonlinear_coxph) Figure 1 - Loss values of a model with sub-optimal performances # Computing the c-index c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test) #0.57 print ( 'C-index: {:.2f}' . format(c_index)) The c-index is 0.57, it is very likely that the model will yield poor results. By speeding the optimization, we managed to get a much better model. To achieve this, we can simply use a higher learning rate. # Let's rebuild the model with a bigger learning # This will accelerate the optimization nonlinear_coxph . fit(X_train, T_train, E_train, lr =1e-3 , dropout =0. ) # We can now display the loss function values in respect to the epochs display_loss_values(nonlinear_coxph) Figure 2 - Loss values of a model with better performances # Computing the c-index c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test) #0.97 print ( 'C-index: {:.2f}' . format(c_index)) The c-index is 0.97, and we can see that the loss function has plateaued around epoch 400.","title":"Reaching complete optimization"},{"location":"miscellaneous/tips.html#actual-vs-predicted","text":"Once a model is built, it is always a good idea to compare the time series of the actual and predicted number of units that experienced the event at each time t. To do so, we compute the actual density/survival function of the data, which can be obtained using the Kaplan-Meier estimator and compare it to the average of all predicted density/survival functions. The function pysurvival.utils.metrics.compare_to_actual can provide the comparison as well as the performance metrics between the two time series, such as: RMSE Median Absolute Error Mean Absolute Error The function pysurvival.utils.display.compare_to_actual will also provide the charts.","title":"Actual vs Predicted"},{"location":"miscellaneous/tips.html#api","text":"compare_to_actual - Comparing the actual and predicted number of units at risk and units experiencing an event at each time t. compare_to_actual(model, X, T, E, times = None, is_at_risk = False, figure_size=(16, 6), metrics = ['rmse', 'mean', 'median']) Parameters: model : Pysurvival object -- Pysurvival model X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. times : array-like -- (default=None) A vector of time-points. is_at_risk : bool -- (default=True) Whether the function returns Expected number of units at risk or the Expected number of units experiencing the events. figure_size : tuple of double -- (default= (16, 6)) width, height in inches representing the size of the chart metrics : str or list of str (default='all') Indicates the performance metrics to compute: if None, then no metric is computed if str, then the metric is computed if list of str, then all the given metrics are computed The available metrics are: RMSE: root mean squared error Mean Abs Error: mean absolute error Median Abs Error: median absolute error Returns: results: dict -- dictionary containing the performance metrics","title":"API"},{"location":"miscellaneous/tips.html#example","text":"All the tutorials display an instance of how to use the function. Let's take the example of the Credit Risk tutorial : the time series of the actual and predicted number of loans that were fully repaid, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 3 - Actual vs Predicted - Number of loans that were fully repaid the time series of the actual and predicted number of loans that were still active, for each time t. results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = True , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 4 - Actual vs Predicted - Number of loans that were still active","title":"Example"},{"location":"models/conditional_survival_forest.html","text":"h1, h2, h3 { color: #04A9F4; } Conditional Survival Forest model The Conditional Survival Forest model was developed by Wright et al. in 2017 to improve the Random Survival Forest training, whose objective function tends to favor splitting variables with many possible split points. Instance To create an instance, use pysurvival.models.survival_forest.ConditionalSurvivalForestModel . Attributes max_features : str or int -- The number of features randomly chosen at each split. num_trees : int -- number of trees contained in the forest times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) variable_importance : dict -- importance of each feature (the higher, the more important the feature is). The importance is the difference between the perturbed and unperturbed error rate for each feature. Methods __init__ - Initialize the estimator ConditionalSurvivalForestModel(num_trees = 10) Parameters: num_trees : int (default=10) -- number of trees that will be built in the forest. fit - Fit the estimator based on the given parameters fit(X, T, E, max_features = 'sqrt', max_depth = 5, min_node_size = 10, alpha = 0.05, minprop= 0.1, num_threads = -1, weights = None, sample_size_pct = 0.63, importance_mode = 'normalized_permutation', seed = None, save_memory=False ) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. max_features : int, float or string (default=\"sqrt\") -- number of features to consider when looking for the best split: If int , then consider the given value at each split. If float , then max_features is a fraction and int(max_features * n_features) features are considered at each split. If \"sqrt\" , then max_features=sqrt(n_features) If \"log2\" , then max_features=log2(n_features) . If \"all\" , then max_features=n_features . min_node_size : int (default=10) -- minimum number of samples required to be at a leaf node alpha : float (default=0.05) -- significance threshold to allow splitting. minprop : float (default=0.1) -- lower quantile of covariate distribution to be considered for splitting num_threads : int (default= -1) -- number of jobs to run in parallel during training. If -1, then the number of jobs is set to the total number of available cores. weights : array-like (default = None) -- weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap. The sum of the weights needs to be 1. sample_size_pct : double (default = 0.63) -- percentage of original samples used in each tree building importance_mode : str (default='impurity_corrected') -- variable importance mode. Here are the available options: impurity or impurity_corrected : it's the unbiased heterogeneity reduction developed by Sandri & Zuccolotto (2008) permutation it's unnormalized as recommended by Nicodemus et al . normalized_permutation it's normalized version of the permutation importance computations by Breiman et al. seed : int (default=None) -- seed used by the random number generator. If None, the current timestamp converted in UNIX is used. save_memory : bool (default=False) -- Use memory saving splitting mode. This will slow down the model training. So, only set to True if you encounter memory problems. Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score Example Let's now take a look at how to use the Conditional Survival Forest (CSF) model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.survival_forest import ConditionalSurvivalForestModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Exponential parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'exponential' , risk_type = 'linear' , censored_parameter = 1 , alpha = 3 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 0.061498 7.065547 0.004457 0.131379 15.412209 0. 0.079149 6.732271 0.008654 0.090398 0.000700 1. PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Conditional model and fitting the data. # Building the model csf = ConditionalSurvivalForestModel(num_trees =200 ) csf . fit(X_train, T_train, E_train, max_features = \"sqrt\" , max_depth =5 , min_node_size =20 , alpha = 0.05 , minprop =0.1 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(csf, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(csf, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the CSF respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = csf . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(csf . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Conditional Survival Forest (API)"},{"location":"models/conditional_survival_forest.html#conditional-survival-forest-model","text":"The Conditional Survival Forest model was developed by Wright et al. in 2017 to improve the Random Survival Forest training, whose objective function tends to favor splitting variables with many possible split points.","title":"Conditional Survival Forest model"},{"location":"models/conditional_survival_forest.html#instance","text":"To create an instance, use pysurvival.models.survival_forest.ConditionalSurvivalForestModel .","title":"Instance"},{"location":"models/conditional_survival_forest.html#attributes","text":"max_features : str or int -- The number of features randomly chosen at each split. num_trees : int -- number of trees contained in the forest times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) variable_importance : dict -- importance of each feature (the higher, the more important the feature is). The importance is the difference between the perturbed and unperturbed error rate for each feature.","title":"Attributes"},{"location":"models/conditional_survival_forest.html#methods","text":"__init__ - Initialize the estimator ConditionalSurvivalForestModel(num_trees = 10) Parameters: num_trees : int (default=10) -- number of trees that will be built in the forest. fit - Fit the estimator based on the given parameters fit(X, T, E, max_features = 'sqrt', max_depth = 5, min_node_size = 10, alpha = 0.05, minprop= 0.1, num_threads = -1, weights = None, sample_size_pct = 0.63, importance_mode = 'normalized_permutation', seed = None, save_memory=False ) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. max_features : int, float or string (default=\"sqrt\") -- number of features to consider when looking for the best split: If int , then consider the given value at each split. If float , then max_features is a fraction and int(max_features * n_features) features are considered at each split. If \"sqrt\" , then max_features=sqrt(n_features) If \"log2\" , then max_features=log2(n_features) . If \"all\" , then max_features=n_features . min_node_size : int (default=10) -- minimum number of samples required to be at a leaf node alpha : float (default=0.05) -- significance threshold to allow splitting. minprop : float (default=0.1) -- lower quantile of covariate distribution to be considered for splitting num_threads : int (default= -1) -- number of jobs to run in parallel during training. If -1, then the number of jobs is set to the total number of available cores. weights : array-like (default = None) -- weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap. The sum of the weights needs to be 1. sample_size_pct : double (default = 0.63) -- percentage of original samples used in each tree building importance_mode : str (default='impurity_corrected') -- variable importance mode. Here are the available options: impurity or impurity_corrected : it's the unbiased heterogeneity reduction developed by Sandri & Zuccolotto (2008) permutation it's unnormalized as recommended by Nicodemus et al . normalized_permutation it's normalized version of the permutation importance computations by Breiman et al. seed : int (default=None) -- seed used by the random number generator. If None, the current timestamp converted in UNIX is used. save_memory : bool (default=False) -- Use memory saving splitting mode. This will slow down the model training. So, only set to True if you encounter memory problems. Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score","title":"Methods"},{"location":"models/conditional_survival_forest.html#example","text":"Let's now take a look at how to use the Conditional Survival Forest (CSF) model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.survival_forest import ConditionalSurvivalForestModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Exponential parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'exponential' , risk_type = 'linear' , censored_parameter = 1 , alpha = 3 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 0.061498 7.065547 0.004457 0.131379 15.412209 0. 0.079149 6.732271 0.008654 0.090398 0.000700 1. PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Conditional model and fitting the data. # Building the model csf = ConditionalSurvivalForestModel(num_trees =200 ) csf . fit(X_train, T_train, E_train, max_features = \"sqrt\" , max_depth =5 , min_node_size =20 , alpha = 0.05 , minprop =0.1 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(csf, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(csf, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the CSF respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = csf . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(csf . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/coxph.html","text":"h1, h2, h3 { color: #04A9F4; } Cox Proportional Hazard model The Cox Proportional Hazard model (CoxPH) is a semi-parametric model that focuses on modeling the hazard function h(t, x_i) , by assuming that its time component \\lambda_0(t) and feature component \\eta(\\vec{x_i}) are proportional such that: \\begin{equation*} h(t, \\vec{x_i}) = h_0(t)\\eta(\\vec{x_i}) \\end{equation*} with: h_0(t) , is the baseline function, which is usually not specified. \\eta(\\vec{x_i}) , is the risk function usually expressed via a linear representation such that \\eta(\\vec{x_i}) = \\exp \\left( \\sum_{j=1}^p x^i_j\\omega_j \\right) . \\omega_j are the coefficients to determine Instance To create an instance, use pysurvival.models.semi_parametric.CoxPHModel . Attributes baseline_hazard : array-like -- values of the hazard function when x \\equiv 0 baseline_survival : array-like -- values of the survival function when x \\equiv 0 summary : pandas.DataFrame -- summary of the modeling results times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) weights : array-like -- model coefficients Methods fit - Fit the estimator based on the given parameters fit(X, T, E, init_method='glorot_normal', lr = 1e-2, max_iter = 100, l2_reg = 1e-4, alpha = 0.95, tol = 1e-3, epsilon=1e-9, verbose = True, display_loss=True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, lr : float (default=1e-4) -- learning rate used in the optimization max_iter : int (default=100) -- maximum number of iterations in the Newton optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients alpha : float (default=0.95) -- confidence level tol : float (default=1e-3) -- tolerance for stopping criteria verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function Example Let's now take a look at how to use the Cox PH model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.semi_parametric import CoxPHModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score #%pylab inline #### 2 - Generating the dataset from a Log-Logistic parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'log-logistic' , risk_type = 'linear' , censored_parameter = 10.1 , alpha = 0.1 , beta =3.2 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 14.711368 123.0 0.022755 114.0 2.0 0. 14.584616 117.0 0.011464 116.0 9.0 0. Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Cox PH model and fitting the data. # Building the model coxph = CoxPHModel() coxph . fit(X_train, T_train, E_train, lr =0.5 , l2_reg =1e-2 , init_method = 'zeros' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(coxph, X_test, T_test, E_test) #0.92 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(coxph, X_test, T_test, E_test, t_max =10 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the CoxPH model respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = coxph . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(coxph . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Standard CoxPH  (API)"},{"location":"models/coxph.html#cox-proportional-hazard-model","text":"The Cox Proportional Hazard model (CoxPH) is a semi-parametric model that focuses on modeling the hazard function h(t, x_i) , by assuming that its time component \\lambda_0(t) and feature component \\eta(\\vec{x_i}) are proportional such that: \\begin{equation*} h(t, \\vec{x_i}) = h_0(t)\\eta(\\vec{x_i}) \\end{equation*} with: h_0(t) , is the baseline function, which is usually not specified. \\eta(\\vec{x_i}) , is the risk function usually expressed via a linear representation such that \\eta(\\vec{x_i}) = \\exp \\left( \\sum_{j=1}^p x^i_j\\omega_j \\right) . \\omega_j are the coefficients to determine","title":"Cox Proportional Hazard model"},{"location":"models/coxph.html#instance","text":"To create an instance, use pysurvival.models.semi_parametric.CoxPHModel .","title":"Instance"},{"location":"models/coxph.html#attributes","text":"baseline_hazard : array-like -- values of the hazard function when x \\equiv 0 baseline_survival : array-like -- values of the survival function when x \\equiv 0 summary : pandas.DataFrame -- summary of the modeling results times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) weights : array-like -- model coefficients","title":"Attributes"},{"location":"models/coxph.html#methods","text":"fit - Fit the estimator based on the given parameters fit(X, T, E, init_method='glorot_normal', lr = 1e-2, max_iter = 100, l2_reg = 1e-4, alpha = 0.95, tol = 1e-3, epsilon=1e-9, verbose = True, display_loss=True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, lr : float (default=1e-4) -- learning rate used in the optimization max_iter : int (default=100) -- maximum number of iterations in the Newton optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients alpha : float (default=0.95) -- confidence level tol : float (default=1e-3) -- tolerance for stopping criteria verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function","title":"Methods"},{"location":"models/coxph.html#example","text":"Let's now take a look at how to use the Cox PH model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.semi_parametric import CoxPHModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score #%pylab inline #### 2 - Generating the dataset from a Log-Logistic parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'log-logistic' , risk_type = 'linear' , censored_parameter = 10.1 , alpha = 0.1 , beta =3.2 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 14.711368 123.0 0.022755 114.0 2.0 0. 14.584616 117.0 0.011464 116.0 9.0 0. Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Cox PH model and fitting the data. # Building the model coxph = CoxPHModel() coxph . fit(X_train, T_train, E_train, lr =0.5 , l2_reg =1e-2 , init_method = 'zeros' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(coxph, X_test, T_test, E_test) #0.92 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(coxph, X_test, T_test, E_test, t_max =10 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the CoxPH model respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = coxph . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(coxph . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/extra_survival_trees.html","text":"h1, h2, h3 { color: #04A9F4; } Extremely Randomized (Extra) Survival Trees model The Extra Survival Trees model is an extension of the Extremely Randomized trees model, introduced by Geurts et al in 2005 , that can take into account censoring. Instance To create an instance, use pysurvival.models.survival_forest.ExtraSurvivalTreesModel . Attributes max_features : str or int -- The number of features randomly chosen at each split. num_trees : int -- number of trees contained in the forest times : array-like -- representation of the time axis for the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) variable_importance : dict -- importance of each feature (the higher, the more important the feature is). The importance is the difference between the perturbed and unperturbed error rate for each feature. Methods __init__ - Initialize the estimator ExtraSurvivalTreesModel(num_trees = 10) Parameters: num_trees : int (default=10) -- number of trees that will be built in the forest. fit - Fit the estimator based on the given parameters fit(X, T, E, max_features = 'sqrt', max_depth = 5, min_node_size = 10, num_random_splits = 100, num_threads = -1, weights = None, sample_size_pct = 0.63, importance_mode = 'normalized_permutation', seed = None, save_memory=False ) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. max_features : int, float or string (default=\"sqrt\") -- number of features to consider when looking for the best split: If int , then consider the given value at each split. If float , then max_features is a fraction and int(max_features * n_features) features are considered at each split. If \"sqrt\" , then max_features=sqrt(n_features) If \"log2\" , then max_features=log2(n_features) . If \"all\" , then max_features=n_features . min_node_size : int (default=10) -- minimum number of samples required to be at a leaf node num_random_splits : int (default=100) -- number of random splits to consider for each candidate splitting variable. num_threads : int (default= -1) -- number of jobs to run in parallel during training. If -1, then the number of jobs is set to the total number of available cores. weights : array-like (default = None) -- weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap. The sum of the weights needs to be 1. sample_size_pct : double (default = 0.63) -- percentage of original samples used in each tree building importance_mode : str (default='impurity_corrected') -- variable importance mode. Here are the available options: impurity or impurity_corrected : it's the unbiased heterogeneity reduction developed by Sandri & Zuccolotto (2008) permutation it's unnormalized as recommended by Nicodemus et al . normalized_permutation it's normalized version of the permutation importance computations by Breiman et al. seed : int (default=None) -- seed used by the random number generator. If None, the current timestamp converted in UNIX is used. save_memory : bool (default=False) -- Use memory saving splitting mode. This will slow down the model training. So, only set to True if you encounter memory problems. Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score Example Let's now take a look at how to use the Extra Survival Trees (XST) model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.survival_forest import ExtraSurvivalTreesModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Exponential parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'exponential' , risk_type = 'linear' , censored_parameter = 1 , alpha = 3 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 0.061498 7.065547 0.004457 0.131379 15.412209 0. 0.079149 6.732271 0.008654 0.090398 0.000700 1. PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Conditional model and fitting the data. # Building the model xst = ExtraSurvivalTreesModel(num_trees =200 ) xst . fit(X_train, T_train, E_train, max_features = \"sqrt\" , max_depth =5 , min_node_size =20 , num_random_splits = 1000 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(xst, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(xst, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the XST respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = xst . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(xst . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Extra Survival Trees (API)"},{"location":"models/extra_survival_trees.html#extremely-randomized-extra-survival-trees-model","text":"The Extra Survival Trees model is an extension of the Extremely Randomized trees model, introduced by Geurts et al in 2005 , that can take into account censoring.","title":"Extremely Randomized (Extra) Survival Trees model"},{"location":"models/extra_survival_trees.html#instance","text":"To create an instance, use pysurvival.models.survival_forest.ExtraSurvivalTreesModel .","title":"Instance"},{"location":"models/extra_survival_trees.html#attributes","text":"max_features : str or int -- The number of features randomly chosen at each split. num_trees : int -- number of trees contained in the forest times : array-like -- representation of the time axis for the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) variable_importance : dict -- importance of each feature (the higher, the more important the feature is). The importance is the difference between the perturbed and unperturbed error rate for each feature.","title":"Attributes"},{"location":"models/extra_survival_trees.html#methods","text":"__init__ - Initialize the estimator ExtraSurvivalTreesModel(num_trees = 10) Parameters: num_trees : int (default=10) -- number of trees that will be built in the forest. fit - Fit the estimator based on the given parameters fit(X, T, E, max_features = 'sqrt', max_depth = 5, min_node_size = 10, num_random_splits = 100, num_threads = -1, weights = None, sample_size_pct = 0.63, importance_mode = 'normalized_permutation', seed = None, save_memory=False ) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. max_features : int, float or string (default=\"sqrt\") -- number of features to consider when looking for the best split: If int , then consider the given value at each split. If float , then max_features is a fraction and int(max_features * n_features) features are considered at each split. If \"sqrt\" , then max_features=sqrt(n_features) If \"log2\" , then max_features=log2(n_features) . If \"all\" , then max_features=n_features . min_node_size : int (default=10) -- minimum number of samples required to be at a leaf node num_random_splits : int (default=100) -- number of random splits to consider for each candidate splitting variable. num_threads : int (default= -1) -- number of jobs to run in parallel during training. If -1, then the number of jobs is set to the total number of available cores. weights : array-like (default = None) -- weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap. The sum of the weights needs to be 1. sample_size_pct : double (default = 0.63) -- percentage of original samples used in each tree building importance_mode : str (default='impurity_corrected') -- variable importance mode. Here are the available options: impurity or impurity_corrected : it's the unbiased heterogeneity reduction developed by Sandri & Zuccolotto (2008) permutation it's unnormalized as recommended by Nicodemus et al . normalized_permutation it's normalized version of the permutation importance computations by Breiman et al. seed : int (default=None) -- seed used by the random number generator. If None, the current timestamp converted in UNIX is used. save_memory : bool (default=False) -- Use memory saving splitting mode. This will slow down the model training. So, only set to True if you encounter memory problems. Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score","title":"Methods"},{"location":"models/extra_survival_trees.html#example","text":"Let's now take a look at how to use the Extra Survival Trees (XST) model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.survival_forest import ExtraSurvivalTreesModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Exponential parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'exponential' , risk_type = 'linear' , censored_parameter = 1 , alpha = 3 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 0.061498 7.065547 0.004457 0.131379 15.412209 0. 0.079149 6.732271 0.008654 0.090398 0.000700 1. PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Conditional model and fitting the data. # Building the model xst = ExtraSurvivalTreesModel(num_trees =200 ) xst . fit(X_train, T_train, E_train, max_features = \"sqrt\" , max_depth =5 , min_node_size =20 , num_random_splits = 1000 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(xst, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(xst, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the XST respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = xst . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(xst . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/kaplan_meier.html","text":"h1, h2, h3 { color: #04A9F4; } Kaplan Meier model The Kaplan\u2013Meier estimator, introduced by Kaplan et al. in 1958 , also known as the product limit estimator, is a non-parametric model used to estimate the survival function of a cohort. Instance To create an instance, use pysurvival.models.non_parametric.KaplanMeierModel . Attributes cumulative_hazard : array-like -- representation of the cumulative hazard function of the model hazard : array-like -- representation of the hazard function of the model survival : array-like -- representation of the Survival function of the model times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) Methods fit - Fit the estimator based on the given parameters fit(T, E, weights = None, alpha=0.05) Parameters: T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. weights : array-like (default = None) -- array of weights that are assigned to individual samples. If not provided, then each sample is given a unit weight. alpha : float (default = 0.05) -- confidence level Returns: self : object predict_density - Predicts the probability density function p(t) at a specified time t predict_density(t) Parameters: t : double -- time at which the prediction should be performed. Returns: density : double -- prediction of the probability density function at t predict_hazard - Predicts the hazard function h(t) at a specified time t predict_hazard(t) Parameters: t : double -- time at which the prediction should be performed. Returns: hazard : double -- prediction of the hazard function at t predict_survival - Predicts the survival function S(t) at a specified time t predict_survival(t) Parameters: t : double -- time at which the prediction should be performed. Returns: survival : double -- prediction of the survival function at t Example # Importing modules import numpy as np from matplotlib import pyplot as plt from pysurvival.models.non_parametric import KaplanMeierModel from pysurvival.utils.display import display_non_parametric # %matplotlib inline #Uncomment when using Jupyter # Generating random times and event indicators T = np . round(np . abs(np . random . normal( 10 , 10 , 1000 )), 1 ) E = np . random . binomial( 1 , 0.3 , 1000 ) # Initializing the KaplanMeierModel km_model = KaplanMeierModel() # Fitting the model km_model . fit(T, E, alpha =0.95 ) # Displaying the survival function and confidence intervals display_non_parametric(km_model) Figure 1 - Representation of the Kaplan Meier Survival function","title":"Kaplan Meier (API)"},{"location":"models/kaplan_meier.html#kaplan-meier-model","text":"The Kaplan\u2013Meier estimator, introduced by Kaplan et al. in 1958 , also known as the product limit estimator, is a non-parametric model used to estimate the survival function of a cohort.","title":"Kaplan Meier model"},{"location":"models/kaplan_meier.html#instance","text":"To create an instance, use pysurvival.models.non_parametric.KaplanMeierModel .","title":"Instance"},{"location":"models/kaplan_meier.html#attributes","text":"cumulative_hazard : array-like -- representation of the cumulative hazard function of the model hazard : array-like -- representation of the hazard function of the model survival : array-like -- representation of the Survival function of the model times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/kaplan_meier.html#methods","text":"fit - Fit the estimator based on the given parameters fit(T, E, weights = None, alpha=0.05) Parameters: T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. weights : array-like (default = None) -- array of weights that are assigned to individual samples. If not provided, then each sample is given a unit weight. alpha : float (default = 0.05) -- confidence level Returns: self : object predict_density - Predicts the probability density function p(t) at a specified time t predict_density(t) Parameters: t : double -- time at which the prediction should be performed. Returns: density : double -- prediction of the probability density function at t predict_hazard - Predicts the hazard function h(t) at a specified time t predict_hazard(t) Parameters: t : double -- time at which the prediction should be performed. Returns: hazard : double -- prediction of the hazard function at t predict_survival - Predicts the survival function S(t) at a specified time t predict_survival(t) Parameters: t : double -- time at which the prediction should be performed. Returns: survival : double -- prediction of the survival function at t","title":"Methods"},{"location":"models/kaplan_meier.html#example","text":"# Importing modules import numpy as np from matplotlib import pyplot as plt from pysurvival.models.non_parametric import KaplanMeierModel from pysurvival.utils.display import display_non_parametric # %matplotlib inline #Uncomment when using Jupyter # Generating random times and event indicators T = np . round(np . abs(np . random . normal( 10 , 10 , 1000 )), 1 ) E = np . random . binomial( 1 , 0.3 , 1000 ) # Initializing the KaplanMeierModel km_model = KaplanMeierModel() # Fitting the model km_model . fit(T, E, alpha =0.95 ) # Displaying the survival function and confidence intervals display_non_parametric(km_model) Figure 1 - Representation of the Kaplan Meier Survival function","title":"Example"},{"location":"models/kernel_svm.html","text":"h1, h2, h3 { color: #04A9F4; } Kernel SVM model The Kernel SVM model available in PySurvival is an adaptation of the work of Sebastian Polsterl et al. . Instance To create an instance, use pysurvival.models.svm.KernelSVMModel . Methods __init__ - Initialization KernelSVMModel(kernel = \"gaussian\", scale=1., offset=0., degree=1., auto_scaler = True) Parameters: kernel : str (default=\"gaussian\") -- The type of kernel used to fit the model. Here's the list of available kernels: Polynomial Gaussian Exponential Tanh Sigmoid Rational Quadratic Inverse Multiquadratic Multiquadratic scale : float (default=1) -- Scale parameter of the kernel function offset : float (default=0) -- Offset parameter of the kernel function degree : float (default=1) -- Degree parameter of the polynomial/kernel function fit - Fit the estimator based on the given parameters fit(X, T, E, with_bias = True, init_method='glorot_normal', lr = 1e-2, max_iter = 100, l2_reg = 1e-4, tol = 1e-3, verbose = True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. with_bias : bool (default=True) -- Whether a bias should be added init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, lr : float (default=1e-4) -- learning rate used in the optimization max_iter : int (default=100) -- maximum number of iterations in the Newton optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients alpha : float (default=0.95) -- confidence level tol : float (default=1e-3) -- tolerance for stopping criteria verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_risk - Predicts the risk score r(x) predict_risk(x, use_log=True) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it use_log : bool (default=False) -- whether or not appliying the log function to the risk values Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score Example Let's now see how to use the KernelSVMModel models on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from pysurvival.models.svm import KernelSVMModel from pysurvival.models.simulations import SimulationModel from pysurvival.utils.metrics import concordance_index from sklearn.model_selection import train_test_split from scipy.stats.stats import pearsonr # %pylab inline # to use in jupyter notebooks #### 2 - Generating the dataset from the parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Log-Logistic' , risk_type = 'square' , censored_parameter = 1.1 , alpha = 1.5 , beta = 4 ) # Generating N Random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 13.234733 10.0 12.0 11.0 0.264510 1.0 4.694893 14.0 11.0 7.0 0.000026 1.0 Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Splitting the dataset into training and testing sets # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the SurvivalSVM model and fitting the data. svm_model = KernelSVMModel(kernel = 'Gaussian' , scale =0.25 ) svm_model . fit(X_train, T_train, E_train, init_method = 'orthogonal' , with_bias = True , lr = 0.8 , tol = 1e-3 , l2_reg = 1e-4 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(svm_model, X_test, T_test, E_test) #0.89 print ( 'C-index: {:.2f}' . format(c_index)) Because we cannot predict a survival function with KernelSVMModel , let's look at the risk scores and see how correlated they are to the actual risk scores generated from the Simulation model. #### 6 - Comparing the model predictions to Actual risk score # Comparing risk scores svm_risks = svm_model . predict_risk(X_test) actual_risks = np . log(sim . predict_risk(X_test) . flatten()) print ( \"corr={:.4f}, p_value={:.5f}\" . format( * pearsonr(svm_risks, actual_risks))) # corr=-0.7519, p_value=0.00000","title":"Kernel Survival SVM (API)"},{"location":"models/kernel_svm.html#kernel-svm-model","text":"The Kernel SVM model available in PySurvival is an adaptation of the work of Sebastian Polsterl et al. .","title":"Kernel SVM model"},{"location":"models/kernel_svm.html#instance","text":"To create an instance, use pysurvival.models.svm.KernelSVMModel .","title":"Instance"},{"location":"models/kernel_svm.html#methods","text":"__init__ - Initialization KernelSVMModel(kernel = \"gaussian\", scale=1., offset=0., degree=1., auto_scaler = True) Parameters: kernel : str (default=\"gaussian\") -- The type of kernel used to fit the model. Here's the list of available kernels: Polynomial Gaussian Exponential Tanh Sigmoid Rational Quadratic Inverse Multiquadratic Multiquadratic scale : float (default=1) -- Scale parameter of the kernel function offset : float (default=0) -- Offset parameter of the kernel function degree : float (default=1) -- Degree parameter of the polynomial/kernel function fit - Fit the estimator based on the given parameters fit(X, T, E, with_bias = True, init_method='glorot_normal', lr = 1e-2, max_iter = 100, l2_reg = 1e-4, tol = 1e-3, verbose = True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. with_bias : bool (default=True) -- Whether a bias should be added init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, lr : float (default=1e-4) -- learning rate used in the optimization max_iter : int (default=100) -- maximum number of iterations in the Newton optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients alpha : float (default=0.95) -- confidence level tol : float (default=1e-3) -- tolerance for stopping criteria verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_risk - Predicts the risk score r(x) predict_risk(x, use_log=True) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it use_log : bool (default=False) -- whether or not appliying the log function to the risk values Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score","title":"Methods"},{"location":"models/kernel_svm.html#example","text":"Let's now see how to use the KernelSVMModel models on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from pysurvival.models.svm import KernelSVMModel from pysurvival.models.simulations import SimulationModel from pysurvival.utils.metrics import concordance_index from sklearn.model_selection import train_test_split from scipy.stats.stats import pearsonr # %pylab inline # to use in jupyter notebooks #### 2 - Generating the dataset from the parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Log-Logistic' , risk_type = 'square' , censored_parameter = 1.1 , alpha = 1.5 , beta = 4 ) # Generating N Random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 13.234733 10.0 12.0 11.0 0.264510 1.0 4.694893 14.0 11.0 7.0 0.000026 1.0 Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Splitting the dataset into training and testing sets # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the SurvivalSVM model and fitting the data. svm_model = KernelSVMModel(kernel = 'Gaussian' , scale =0.25 ) svm_model . fit(X_train, T_train, E_train, init_method = 'orthogonal' , with_bias = True , lr = 0.8 , tol = 1e-3 , l2_reg = 1e-4 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(svm_model, X_test, T_test, E_test) #0.89 print ( 'C-index: {:.2f}' . format(c_index)) Because we cannot predict a survival function with KernelSVMModel , let's look at the risk scores and see how correlated they are to the actual risk scores generated from the Simulation model. #### 6 - Comparing the model predictions to Actual risk score # Comparing risk scores svm_risks = svm_model . predict_risk(X_test) actual_risks = np . log(sim . predict_risk(X_test) . flatten()) print ( \"corr={:.4f}, p_value={:.5f}\" . format( * pearsonr(svm_risks, actual_risks))) # corr=-0.7519, p_value=0.00000","title":"Example"},{"location":"models/linear_mtlr.html","text":"h1, h2, h3 { color: #04A9F4; } Linear MTLR model The Multi-Task Logistic Regression (MTLR) model is an alternative to the Cox\u2019s proportional hazard model. It can be seen as a series of logistic regression models built on different time intervals so as to estimate the probability that the event of interest happened within each interval. The model was introduced by Yu, Chun-Nam, et al. in 2011 Instance To create an instance, use pysurvival.models.multi_task.LinearMultiTaskModel . Attributes times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) Methods __init__ - Initialization LinearMultiTaskModel(bins=100, auto_scaler=True) Parameters: bins : int (default=100) -- Number of subdivisions of the time axis auto_scaler : bool (default=True) -- Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, l2_reg=1e-2, l2_smooth=1e-2, verbose=True, extra_pct_time = 0.1, is_min_time_zero=True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- number of iterations in the optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients l2_smooth : float (default=1e-4) -- Second L2 regularizer that ensures the parameters vary smoothly across consecutive time points. verbose : bool (default=True) -- whether or not producing detailed logging about the modeling extra_pct_time : float (default=0.1) -- providing an extra fraction of time in the time axis is_min_time_zero : bool (default=True) -- whether the time axis starts at 0 Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function Example Let's now take a look at how to use the Linear MTLR model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.multi_task import LinearMultiTaskModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score #%matplotlib inline # To use with Jupyter notebooks #### 2 - Generating the dataset from a Weibull parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Weibull' , risk_type = 'linear' , censored_parameter = 10.0 , alpha = .01 , beta = 3.0 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 3 ) # Showing a few data-points time_column = 'time' event_column = 'event' dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event 1.841646 -0.670071 1.157705 4.4983 1.0 2.825421 -9.562958 0.462503 0.0000 0.0 Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Linear MTLR model and fitting the data. # Building the model l_mtlr = LinearMultiTaskModel(bins =50 ) l_mtlr . fit(X_train, T_train, E_train, lr =1e-3 , init_method = 'orthogonal' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(l_mtlr, X_test, T_test, E_test) #0.95 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(l_mtlr, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Linear MTLR respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = l_mtlr . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(l_mtlr . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Linear MTLR (API)"},{"location":"models/linear_mtlr.html#linear-mtlr-model","text":"The Multi-Task Logistic Regression (MTLR) model is an alternative to the Cox\u2019s proportional hazard model. It can be seen as a series of logistic regression models built on different time intervals so as to estimate the probability that the event of interest happened within each interval. The model was introduced by Yu, Chun-Nam, et al. in 2011","title":"Linear MTLR model"},{"location":"models/linear_mtlr.html#instance","text":"To create an instance, use pysurvival.models.multi_task.LinearMultiTaskModel .","title":"Instance"},{"location":"models/linear_mtlr.html#attributes","text":"times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/linear_mtlr.html#methods","text":"__init__ - Initialization LinearMultiTaskModel(bins=100, auto_scaler=True) Parameters: bins : int (default=100) -- Number of subdivisions of the time axis auto_scaler : bool (default=True) -- Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, l2_reg=1e-2, l2_smooth=1e-2, verbose=True, extra_pct_time = 0.1, is_min_time_zero=True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- number of iterations in the optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients l2_smooth : float (default=1e-4) -- Second L2 regularizer that ensures the parameters vary smoothly across consecutive time points. verbose : bool (default=True) -- whether or not producing detailed logging about the modeling extra_pct_time : float (default=0.1) -- providing an extra fraction of time in the time axis is_min_time_zero : bool (default=True) -- whether the time axis starts at 0 Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function","title":"Methods"},{"location":"models/linear_mtlr.html#example","text":"Let's now take a look at how to use the Linear MTLR model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.multi_task import LinearMultiTaskModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score #%matplotlib inline # To use with Jupyter notebooks #### 2 - Generating the dataset from a Weibull parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Weibull' , risk_type = 'linear' , censored_parameter = 10.0 , alpha = .01 , beta = 3.0 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 3 ) # Showing a few data-points time_column = 'time' event_column = 'event' dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event 1.841646 -0.670071 1.157705 4.4983 1.0 2.825421 -9.562958 0.462503 0.0000 0.0 Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Linear MTLR model and fitting the data. # Building the model l_mtlr = LinearMultiTaskModel(bins =50 ) l_mtlr . fit(X_train, T_train, E_train, lr =1e-3 , init_method = 'orthogonal' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(l_mtlr, X_test, T_test, E_test) #0.95 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(l_mtlr, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Linear MTLR respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = l_mtlr . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(l_mtlr . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/linear_svm.html","text":"h1, h2, h3 { color: #04A9F4; } Linear SVM model The Linear SVM model available in PySurvival is an adaptation of the work of Sebastian Polsterl et al. . Instance To create an instance, use pysurvival.models.svm.LinearSVMModel . Methods fit - Fit the estimator based on the given parameters fit(X, T, E, with_bias = True, init_method='glorot_normal', lr = 1e-2, max_iter = 100, l2_reg = 1e-4, tol = 1e-3, verbose = True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. with_bias : bool (default=True) -- whether a bias should be added init_method : str (default = 'glorot_uniform') -- initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, lr : float (default=1e-4) -- learning rate used in the optimization max_iter : int (default=100) -- maximum number of iterations in the Newton optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients alpha : float (default=0.95) -- confidence level tol : float (default=1e-3) -- tolerance for stopping criteria verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_risk - Predicts the risk score r(x) predict_risk(x, use_log=True) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it use_log : bool (default=False) -- whether or not appliying the log function to the risk values Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score Example Let's now see how to use the LinearSVMModel models on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from pysurvival.models.svm import LinearSVMModel from pysurvival.models.simulations import SimulationModel from pysurvival.utils.metrics import concordance_index from sklearn.model_selection import train_test_split from scipy.stats.stats import pearsonr # %pylab inline # to use in jupyter notebooks #### 2 - Generating the dataset from the parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Log-Logistic' , risk_type = 'linear' , censored_parameter = 1.1 , alpha = 1.5 , beta = 4 ) # Generating N Random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 113.0 15.526830 0.002320 116.0 6.3 0. 118.0 5.293601 0.005194 110.0.0 0.0 0. Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Splitting the dataset into training and testing sets # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Linear SVM model and fitting the data. svm_model = LinearSVMModel() svm_model . fit(X_train, T_train, E_train, init_method = 'he_uniform' , with_bias = True , lr = 0.5 , tol = 1e-3 , l2_reg = 1e-3 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(svm_model, X_test, T_test, E_test) #0.93 print ( 'C-index: {:.2f}' . format(c_index)) Because we cannot predict a survival function with LinearSVMModel , let's look at the risk scores and see how correlated they are to the actual risk scores generated from the Simulation model. #### 6 - Comparing the model predictions to Actual risk score # Comparing risk scores svm_risks = svm_model . predict_risk(X_test) actual_risks = sim . predict_risk(X_test) . flatten() print ( \"corr={:.4f}, p_value={:.5f}\" . format( * pearsonr(svm_risks, actual_risks))) # corr=-0.9992, p_value=0.00000 Let's create risk groups based on the risk score distributions from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = svm_model, X = X_test, use_log = True , num_bins =20 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : -3.5 , 'upper_bound' : -0.5 , 'color' : 'red' }, medium = { 'lower_bound' : -0.5 , 'upper_bound' : 0.5 , 'color' : 'green' }, high = { 'lower_bound' : 0.5 , 'upper_bound' : 2.1 , 'color' : 'blue' } ) Figure 2 - Creating risk groups","title":"Linear Survival SVM (API)"},{"location":"models/linear_svm.html#linear-svm-model","text":"The Linear SVM model available in PySurvival is an adaptation of the work of Sebastian Polsterl et al. .","title":"Linear SVM model"},{"location":"models/linear_svm.html#instance","text":"To create an instance, use pysurvival.models.svm.LinearSVMModel .","title":"Instance"},{"location":"models/linear_svm.html#methods","text":"fit - Fit the estimator based on the given parameters fit(X, T, E, with_bias = True, init_method='glorot_normal', lr = 1e-2, max_iter = 100, l2_reg = 1e-4, tol = 1e-3, verbose = True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. with_bias : bool (default=True) -- whether a bias should be added init_method : str (default = 'glorot_uniform') -- initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, lr : float (default=1e-4) -- learning rate used in the optimization max_iter : int (default=100) -- maximum number of iterations in the Newton optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients alpha : float (default=0.95) -- confidence level tol : float (default=1e-3) -- tolerance for stopping criteria verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_risk - Predicts the risk score r(x) predict_risk(x, use_log=True) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it use_log : bool (default=False) -- whether or not appliying the log function to the risk values Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score","title":"Methods"},{"location":"models/linear_svm.html#example","text":"Let's now see how to use the LinearSVMModel models on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from pysurvival.models.svm import LinearSVMModel from pysurvival.models.simulations import SimulationModel from pysurvival.utils.metrics import concordance_index from sklearn.model_selection import train_test_split from scipy.stats.stats import pearsonr # %pylab inline # to use in jupyter notebooks #### 2 - Generating the dataset from the parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Log-Logistic' , risk_type = 'linear' , censored_parameter = 1.1 , alpha = 1.5 , beta = 4 ) # Generating N Random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 113.0 15.526830 0.002320 116.0 6.3 0. 118.0 5.293601 0.005194 110.0.0 0.0 0. Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Splitting the dataset into training and testing sets # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Linear SVM model and fitting the data. svm_model = LinearSVMModel() svm_model . fit(X_train, T_train, E_train, init_method = 'he_uniform' , with_bias = True , lr = 0.5 , tol = 1e-3 , l2_reg = 1e-3 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(svm_model, X_test, T_test, E_test) #0.93 print ( 'C-index: {:.2f}' . format(c_index)) Because we cannot predict a survival function with LinearSVMModel , let's look at the risk scores and see how correlated they are to the actual risk scores generated from the Simulation model. #### 6 - Comparing the model predictions to Actual risk score # Comparing risk scores svm_risks = svm_model . predict_risk(X_test) actual_risks = sim . predict_risk(X_test) . flatten() print ( \"corr={:.4f}, p_value={:.5f}\" . format( * pearsonr(svm_risks, actual_risks))) # corr=-0.9992, p_value=0.00000 Let's create risk groups based on the risk score distributions from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = svm_model, X = X_test, use_log = True , num_bins =20 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : -3.5 , 'upper_bound' : -0.5 , 'color' : 'red' }, medium = { 'lower_bound' : -0.5 , 'upper_bound' : 0.5 , 'color' : 'green' }, high = { 'lower_bound' : 0.5 , 'upper_bound' : 2.1 , 'color' : 'blue' } ) Figure 2 - Creating risk groups","title":"Example"},{"location":"models/mtlr_theory.html","text":"h1, h2, h3 { color: #04A9F4; } Multi-Task Logistic Regression models When it comes to predicting the survival function for a specific unit, the Cox Proportional Hazard Model is usually the go-to model. However, it presents some important drawbacks: It relies on the proportional hazard assumption, which specifies that the ratio of the hazards for any two individuals is constant over time. The exact formula of the model that can handle ties isn\u2019t computationally efficient, and is often rewritten using approximations, such as the Efron\u2019s or Breslow\u2019s approximations, in order to fit the model in a reasonable time. The fact that the time component of the hazard function remains unspecified makes the CoxPH model ill-suited for actual survival function predictions. The Multi-Task Logistic Regression models were therefore introduced to provide great alternatives. Linear MTLR model The Multi-Task Logistic Regression (MTLR) model created by Yu, Chun-Nam, et al. in 2011 can be seen as a series of logistic regression models built on different time intervals so as to estimate the probability that the event of interest happened within each interval. The model can be built using the following steps: We start by dividing the time axis into J time intervals such that \\forall j \\in [\\![1, J ]\\!] , a_j = [ \\tau_{j-1}, \\tau_j ) with \\tau_0 = 0 and \\tau_J = \\infty . Figure 1 - Subdivisions of the time axis in J intervals We then build a logistic regression model on each interval a_j , with the parameters \\left( \\vec{\\theta_j}, b_j \\right) and the response variable \\begin{equation} y_j = \\begin{cases} 1 \\text{ , if } T \\in a_j \\text{ i.e., the event happened in the interval } a_j \\\\ 0 \\text{ , otherwise} \\\\ \\end{cases} \\end{equation} But, because we are not analyzing the effects of recurrent events, we need to make sure that when a unit experiences an event on interval a_s with s \\in [\\![1, J ]\\!] , its status for the remaining intervals stays the same. Thus, the response vector {Y} is: \\begin{equation*} \\vec{Y} = \\begin{bmatrix}y_1 = 0 \\\\ y_2 = 0 \\\\ \\vdots \\\\ y_{s-1} = 0 \\\\ y_{s} = 1 \\\\ \\vdots \\\\ y_{J} =1 \\end{bmatrix} \\end{equation*} Chun-Nam Yu et al. proposed the following definitions for the density and survival functions: Density function : \\begin{equation} \\tag{MTLR - density} \\begin{split} f(a_s, \\vec{x}) & = P\\left[ T \\in [\\tau_{s-1}, \\tau_s) | \\vec{x} \\right] \\\\ & = \\frac{\\exp\\left( \\sum_{j = s}^{J-1} \\vec{x} \\cdot \\vec{\\theta}_j + b_j \\right) }{ Z(\\vec{x}) } \\\\ & = \\frac{\\exp\\left( \\left(\\vec{x} \\cdot \\textbf{$\\Theta$} + \\vec{b} \\right)\\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z(\\vec{x}) } \\end{split} \\end{equation} Survival function : \\begin{equation} \\tag{MTLR - survival} \\begin{split} S(\\tau_{s-1}, \\vec{x}) = P\\left[ T \\geq \\tau_{s-1}|\\vec{x} \\right ] & = \\sum^{J}_{k = s} P\\left[ T \\in [\\tau_{k-1}, \\tau_k) | \\vec{x} \\right ] \\\\ & = \\sum^{J}_{k = s} \\frac{ \\exp \\left( \\sum^{J-1}_{j=s} (\\vec{x} \\cdot \\vec{ \\theta}_j + b_j) \\right) }{ Z( \\vec{x})} \\\\ & = \\sum^{J}_{k = s} \\frac{ \\exp \\left( \\left( \\vec{x} \\cdot \\textbf{$\\Theta$} + \\vec{b} \\right) \\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z\\left( \\vec{x}\\right)} \\end{split} \\end{equation} with: \\circ , the Hadamard product the coefficients matrix, \\displaystyle \\Theta = \\begin{bmatrix} \\theta_{1, 1} & \\theta_{1, 2} & ... & \\theta_{1, J-1}& \\theta_{1, J} \\\\ \\theta_{2, 1} & \\theta_{2, 2} & ... & \\theta_{2, J-1}& \\theta_{2, J} \\\\ ... & ... & ... & ... & ... \\\\ \\theta_{p, 1} & \\theta_{p, 2} & ... & \\theta_{p, J-1}& \\theta_{p, J} \\\\ \\end{bmatrix} = \\left[\\vec{\\theta}_1, \\vec{\\theta}_2, ..., \\vec{\\theta}_{J-1}, \\vec{\\theta}_J \\right] the bias vector: \\vec{b} = \\begin{bmatrix} b_1, & b_2, & ... & b_{J-1}, & b_J \\end{bmatrix} the (J \\times J+1) -triangular matrix: \\Delta = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 & 0 \\\\ 1 & 1 & 0 & ... & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots &... & \\vdots & \\vdots \\\\ 1 & 1 & 1 & ... & 0 & 0 \\\\ 1 & 1 & 1 & ... & 1 & 0 \\\\ \\end{bmatrix} the normalization constant Z \\left( \\vec{x}\\right) = \\sum_{j=1}^J \\exp \\left( \\sum_{l=j+1}^J \\vec{\\theta}_l \\cdot \\vec{x} + b_l\\right) We can therefore compute the loss function and minimize it to find the optimal model parameters. \\begin{equation*} l(\\Theta, b) = \\sum_{i=1}^N \\delta_i \\log\\left(f(a_s, \\vec{x_i}) \\right) + (1-\\delta_i)\\log\\left( S(\\tau_{s-1}, \\vec{x_i}) \\right) + \\alpha_{l2}\\left(|| \\Theta ||^2 + || b ||^2 \\right) \\end{equation*} Neural MTLR model Although the MTLR model provides similar results as the CoxPH model without having to rely on the assumptions required by the latter, at its core, it is still powered by a linear transformation. Thus, in the presence of nonlinear elements in the data, it will stop yielding satisfactory performances. The Neural Multi-Task Logistic Regression (N-MTLR) which allows the use of Neural Networks within the original MTLR design, will help solve this issue. In the case of Neural Multi-Task Logistic Regression, the density and survival functions become: Density function : \\begin{equation} \\tag{N-MTLR - density} \\begin{split} f(a_s, \\vec{x}) = P\\left[ T \\in [\\tau_{s-1}, \\tau_s) | \\vec{x} \\right ] & = \\frac{ \\exp \\left( \\psi( \\vec{x}) \\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z\\left( \\psi( \\vec{x})\\right) } \\end{split} \\end{equation} Survival function : \\begin{equation} \\tag{N-MTLR - survival} \\begin{split} S(\\tau_{s-1}, \\vec{x}) = \\sum^{J}_{k = s} \\frac{ \\exp \\left( \\psi( \\vec{x}) \\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z\\left( \\psi( \\vec{x})\\right) } \\end{split} \\end{equation} with \\psi: \\mathbb{R}^p \\mapsto \\mathbb{R}^{J} is the nonlinear transformation using \\vec{x} \\in \\mathbb{R}^{p} feature vector as its input. Its output is a \\mathbb{R}^{J} vector whose values are mapped to the J subdivisions of the time axis Figure 2 - Representation of a 2-hidden layer transformation References \"Learning patient-specific cancer survival distributions as a sequence of dependent regressors.\" Yu, Chun-Nam, et al. Advances in Neural Information Processing Systems. 2011. \"Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework.\" Fotso, S. (2018). arXiv:1801.05512. Square's Technical Blog - Deep Learning + Survival Analysis: Our Approach to Multi-Task Frameworks","title":"Theory"},{"location":"models/mtlr_theory.html#multi-task-logistic-regression-models","text":"When it comes to predicting the survival function for a specific unit, the Cox Proportional Hazard Model is usually the go-to model. However, it presents some important drawbacks: It relies on the proportional hazard assumption, which specifies that the ratio of the hazards for any two individuals is constant over time. The exact formula of the model that can handle ties isn\u2019t computationally efficient, and is often rewritten using approximations, such as the Efron\u2019s or Breslow\u2019s approximations, in order to fit the model in a reasonable time. The fact that the time component of the hazard function remains unspecified makes the CoxPH model ill-suited for actual survival function predictions. The Multi-Task Logistic Regression models were therefore introduced to provide great alternatives.","title":"Multi-Task Logistic Regression models"},{"location":"models/mtlr_theory.html#linear-mtlr-model","text":"The Multi-Task Logistic Regression (MTLR) model created by Yu, Chun-Nam, et al. in 2011 can be seen as a series of logistic regression models built on different time intervals so as to estimate the probability that the event of interest happened within each interval. The model can be built using the following steps: We start by dividing the time axis into J time intervals such that \\forall j \\in [\\![1, J ]\\!] , a_j = [ \\tau_{j-1}, \\tau_j ) with \\tau_0 = 0 and \\tau_J = \\infty . Figure 1 - Subdivisions of the time axis in J intervals We then build a logistic regression model on each interval a_j , with the parameters \\left( \\vec{\\theta_j}, b_j \\right) and the response variable \\begin{equation} y_j = \\begin{cases} 1 \\text{ , if } T \\in a_j \\text{ i.e., the event happened in the interval } a_j \\\\ 0 \\text{ , otherwise} \\\\ \\end{cases} \\end{equation} But, because we are not analyzing the effects of recurrent events, we need to make sure that when a unit experiences an event on interval a_s with s \\in [\\![1, J ]\\!] , its status for the remaining intervals stays the same. Thus, the response vector {Y} is: \\begin{equation*} \\vec{Y} = \\begin{bmatrix}y_1 = 0 \\\\ y_2 = 0 \\\\ \\vdots \\\\ y_{s-1} = 0 \\\\ y_{s} = 1 \\\\ \\vdots \\\\ y_{J} =1 \\end{bmatrix} \\end{equation*} Chun-Nam Yu et al. proposed the following definitions for the density and survival functions: Density function : \\begin{equation} \\tag{MTLR - density} \\begin{split} f(a_s, \\vec{x}) & = P\\left[ T \\in [\\tau_{s-1}, \\tau_s) | \\vec{x} \\right] \\\\ & = \\frac{\\exp\\left( \\sum_{j = s}^{J-1} \\vec{x} \\cdot \\vec{\\theta}_j + b_j \\right) }{ Z(\\vec{x}) } \\\\ & = \\frac{\\exp\\left( \\left(\\vec{x} \\cdot \\textbf{$\\Theta$} + \\vec{b} \\right)\\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z(\\vec{x}) } \\end{split} \\end{equation} Survival function : \\begin{equation} \\tag{MTLR - survival} \\begin{split} S(\\tau_{s-1}, \\vec{x}) = P\\left[ T \\geq \\tau_{s-1}|\\vec{x} \\right ] & = \\sum^{J}_{k = s} P\\left[ T \\in [\\tau_{k-1}, \\tau_k) | \\vec{x} \\right ] \\\\ & = \\sum^{J}_{k = s} \\frac{ \\exp \\left( \\sum^{J-1}_{j=s} (\\vec{x} \\cdot \\vec{ \\theta}_j + b_j) \\right) }{ Z( \\vec{x})} \\\\ & = \\sum^{J}_{k = s} \\frac{ \\exp \\left( \\left( \\vec{x} \\cdot \\textbf{$\\Theta$} + \\vec{b} \\right) \\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z\\left( \\vec{x}\\right)} \\end{split} \\end{equation} with: \\circ , the Hadamard product the coefficients matrix, \\displaystyle \\Theta = \\begin{bmatrix} \\theta_{1, 1} & \\theta_{1, 2} & ... & \\theta_{1, J-1}& \\theta_{1, J} \\\\ \\theta_{2, 1} & \\theta_{2, 2} & ... & \\theta_{2, J-1}& \\theta_{2, J} \\\\ ... & ... & ... & ... & ... \\\\ \\theta_{p, 1} & \\theta_{p, 2} & ... & \\theta_{p, J-1}& \\theta_{p, J} \\\\ \\end{bmatrix} = \\left[\\vec{\\theta}_1, \\vec{\\theta}_2, ..., \\vec{\\theta}_{J-1}, \\vec{\\theta}_J \\right] the bias vector: \\vec{b} = \\begin{bmatrix} b_1, & b_2, & ... & b_{J-1}, & b_J \\end{bmatrix} the (J \\times J+1) -triangular matrix: \\Delta = \\begin{bmatrix} 1 & 0 & 0 & ... & 0 & 0 \\\\ 1 & 1 & 0 & ... & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots &... & \\vdots & \\vdots \\\\ 1 & 1 & 1 & ... & 0 & 0 \\\\ 1 & 1 & 1 & ... & 1 & 0 \\\\ \\end{bmatrix} the normalization constant Z \\left( \\vec{x}\\right) = \\sum_{j=1}^J \\exp \\left( \\sum_{l=j+1}^J \\vec{\\theta}_l \\cdot \\vec{x} + b_l\\right) We can therefore compute the loss function and minimize it to find the optimal model parameters. \\begin{equation*} l(\\Theta, b) = \\sum_{i=1}^N \\delta_i \\log\\left(f(a_s, \\vec{x_i}) \\right) + (1-\\delta_i)\\log\\left( S(\\tau_{s-1}, \\vec{x_i}) \\right) + \\alpha_{l2}\\left(|| \\Theta ||^2 + || b ||^2 \\right) \\end{equation*}","title":"Linear MTLR model"},{"location":"models/mtlr_theory.html#neural-mtlr-model","text":"Although the MTLR model provides similar results as the CoxPH model without having to rely on the assumptions required by the latter, at its core, it is still powered by a linear transformation. Thus, in the presence of nonlinear elements in the data, it will stop yielding satisfactory performances. The Neural Multi-Task Logistic Regression (N-MTLR) which allows the use of Neural Networks within the original MTLR design, will help solve this issue. In the case of Neural Multi-Task Logistic Regression, the density and survival functions become: Density function : \\begin{equation} \\tag{N-MTLR - density} \\begin{split} f(a_s, \\vec{x}) = P\\left[ T \\in [\\tau_{s-1}, \\tau_s) | \\vec{x} \\right ] & = \\frac{ \\exp \\left( \\psi( \\vec{x}) \\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z\\left( \\psi( \\vec{x})\\right) } \\end{split} \\end{equation} Survival function : \\begin{equation} \\tag{N-MTLR - survival} \\begin{split} S(\\tau_{s-1}, \\vec{x}) = \\sum^{J}_{k = s} \\frac{ \\exp \\left( \\psi( \\vec{x}) \\cdot \\textbf{$\\Delta$} \\right) \\circ \\vec{Y} }{ Z\\left( \\psi( \\vec{x})\\right) } \\end{split} \\end{equation} with \\psi: \\mathbb{R}^p \\mapsto \\mathbb{R}^{J} is the nonlinear transformation using \\vec{x} \\in \\mathbb{R}^{p} feature vector as its input. Its output is a \\mathbb{R}^{J} vector whose values are mapped to the J subdivisions of the time axis Figure 2 - Representation of a 2-hidden layer transformation","title":"Neural MTLR model"},{"location":"models/mtlr_theory.html#references","text":"\"Learning patient-specific cancer survival distributions as a sequence of dependent regressors.\" Yu, Chun-Nam, et al. Advances in Neural Information Processing Systems. 2011. \"Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework.\" Fotso, S. (2018). arXiv:1801.05512. Square's Technical Blog - Deep Learning + Survival Analysis: Our Approach to Multi-Task Frameworks","title":"References"},{"location":"models/neural_mtlr.html","text":"h1, h2, h3 { color: #04A9F4; } Neural MTLR model In order to introduce more modeling flexibility, the Neural Multi-Task Logistic Regression model (N-MTLR) was presented in Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework allowing the use of Neural Networks within the original MTLR design. Instance To create an instance, use pysurvival.models.multi_task.NeuralMultiTaskModel . Attributes structure : list of dictionaries -- Provides the structure of the MLP built within the N-MTLR times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) Methods __init__ - Initialization NeuralMultiTaskModel(structure, bins=100, auto_scaler=True) Parameters: structure : list of dictionaries -- Provides the structure of the MLP built within the N-MTLR. ex: structure = [ {'activation': 'ReLU', 'num_units': 128}, ] . Each dictionary corresponds to a fully connected hidden layer: num_units is the number of hidden units in this layer activation is the activation function that will be used. The list of all available activation functions can be found here . In case there are more than one dictionary, each hidden layer will be applied in the resulting MLP, using the order it is provided in the structure . bins : int (default=100) -- Number of subdivisions of the time axis auto_scaler : boolean (default=True) -- Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, dropout = 0.2, l2_reg=1e-2, l2_smooth=1e-2, batch_normalization=False, bn_and_dropout=False, verbose=True, extra_pct_time = 0.1, is_min_time_zero=True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- The number of iterations in the optimization dropout : float (default=0.5) -- Randomly sets a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients l2_smooth : float (default=1e-4) -- Second L2 regularizer that ensures the parameters vary smoothly across consecutive time points. batch_normalization : bool (default=True) -- Applying Batch Normalization or not bn_and_dropout : bool (default=False) -- Applying Batch Normalization and Dropout at the same time display_loss : bool (default=True) -- Whether or not showing the loss function values at each update verbose : bool (default=True) -- Whether or not producing detailed logging about the modeling extra_pct_time : float (default=0.1) -- Providing an extra fraction of time in the time axis is_min_time_zero : bool (default=True) -- Whether the the time axis starts at 0 Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function Example Let's now take a look at how to use the Neural MTLR model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.multi_task import NeuralMultiTaskModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a nonlinear Log-Logistic parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Log-Logistic' , risk_type = 'gaussian' , censored_parameter = 10.1 , alpha = 0.1 , beta =3.2 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 3 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event 0.01540 0.719263 84.827599 2.886262 1.0 0.00486 4.628235 66.257702 1.813925 0.0 Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Neural MTLR model and fitting the data. # Defining the MLP structure. Here we will build a 1-hidden layer # with 150 units and `Swish` as its activation function structure = [ { 'activation' : 'ReLU' , 'num_units' : 150 }, ] # Building the model n_mtlr = NeuralMultiTaskModel(structure = structure, bins =150 ) n_mtlr . fit(X_train, T_train, E_train, lr =1e-3 , num_epochs = 500 , init_method = 'orthogonal' , optimizer = 'rprop' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(n_mtlr, X_test, T_test, E_test) #0.68 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(n_mtlr, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields good performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Neural MTLR respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = n_mtlr . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(n_mtlr . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Neural MTLR (API)"},{"location":"models/neural_mtlr.html#neural-mtlr-model","text":"In order to introduce more modeling flexibility, the Neural Multi-Task Logistic Regression model (N-MTLR) was presented in Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework allowing the use of Neural Networks within the original MTLR design.","title":"Neural MTLR model"},{"location":"models/neural_mtlr.html#instance","text":"To create an instance, use pysurvival.models.multi_task.NeuralMultiTaskModel .","title":"Instance"},{"location":"models/neural_mtlr.html#attributes","text":"structure : list of dictionaries -- Provides the structure of the MLP built within the N-MTLR times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/neural_mtlr.html#methods","text":"__init__ - Initialization NeuralMultiTaskModel(structure, bins=100, auto_scaler=True) Parameters: structure : list of dictionaries -- Provides the structure of the MLP built within the N-MTLR. ex: structure = [ {'activation': 'ReLU', 'num_units': 128}, ] . Each dictionary corresponds to a fully connected hidden layer: num_units is the number of hidden units in this layer activation is the activation function that will be used. The list of all available activation functions can be found here . In case there are more than one dictionary, each hidden layer will be applied in the resulting MLP, using the order it is provided in the structure . bins : int (default=100) -- Number of subdivisions of the time axis auto_scaler : boolean (default=True) -- Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, dropout = 0.2, l2_reg=1e-2, l2_smooth=1e-2, batch_normalization=False, bn_and_dropout=False, verbose=True, extra_pct_time = 0.1, is_min_time_zero=True) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- The number of iterations in the optimization dropout : float (default=0.5) -- Randomly sets a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients l2_smooth : float (default=1e-4) -- Second L2 regularizer that ensures the parameters vary smoothly across consecutive time points. batch_normalization : bool (default=True) -- Applying Batch Normalization or not bn_and_dropout : bool (default=False) -- Applying Batch Normalization and Dropout at the same time display_loss : bool (default=True) -- Whether or not showing the loss function values at each update verbose : bool (default=True) -- Whether or not producing detailed logging about the modeling extra_pct_time : float (default=0.1) -- Providing an extra fraction of time in the time axis is_min_time_zero : bool (default=True) -- Whether the the time axis starts at 0 Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function","title":"Methods"},{"location":"models/neural_mtlr.html#example","text":"Let's now take a look at how to use the Neural MTLR model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.multi_task import NeuralMultiTaskModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a nonlinear Log-Logistic parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Log-Logistic' , risk_type = 'gaussian' , censored_parameter = 10.1 , alpha = 0.1 , beta =3.2 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 3 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event 0.01540 0.719263 84.827599 2.886262 1.0 0.00486 4.628235 66.257702 1.813925 0.0 Pysurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Neural MTLR model and fitting the data. # Defining the MLP structure. Here we will build a 1-hidden layer # with 150 units and `Swish` as its activation function structure = [ { 'activation' : 'ReLU' , 'num_units' : 150 }, ] # Building the model n_mtlr = NeuralMultiTaskModel(structure = structure, bins =150 ) n_mtlr . fit(X_train, T_train, E_train, lr =1e-3 , num_epochs = 500 , init_method = 'orthogonal' , optimizer = 'rprop' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(n_mtlr, X_test, T_test, E_test) #0.68 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(n_mtlr, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields good performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Neural MTLR respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = n_mtlr . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(n_mtlr . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/non_parametric.html","text":"h1, h2, h3 { color: #04A9F4; } Non Parametric models Non Parametric models offer a straightforward and easy-to-interpret way to compute the survival and hazard functions without imposing any assumptions. Pysurvival provides the following non-parametric models: Kaplan-Meier model ( KaplanMeierModel ) Smooth Kaplan-Meier model ( SmoothKaplanMeierModel ) Kaplan-Meier model One of the most straight-forward ways to estimate the Survival function S(t) of an entire group, is by using the Kaplan-Meier method. Given N units in a cohort, let's assume that there are J distinct actual event times such that t_1 < t_2 < ... < t_J with J \\leq N , then the Survival function estimator S_{KM}(t) is given by: \\begin{equation*} S_{KM}(t) = \\prod_{t_j \\leq t} \\left(1-\\frac{d_j}{r_j} \\right) \\end{equation*} with: S_{KM}(0) = 1 d_j is the number of individuals experiencing an event at t_j r_j is the number of individuals at risk within [t_{j-1}, t_j) - those who have not been censored or experienced an event Smooth Kaplan-Meier Despite its ease of use, the main drawback of the Kaplan-Meier estimator is that it is a step function with jumps. Kernel smoothing can therefore solve this issue, provided that the best kernel and bandwidth are properly chosen. Let S_b(t) be a Smooth estimator of the Kaplan-Meier survival function. S_b(t) can be written such that: \\begin{equation*} S_{b}(t) = \\sum_j s_j K\\left( \\frac{t-T_j}{b}\\right) \\end{equation*} with: s_j , the height of the jump of the Kaplan-Meier estimator at T_j K , the infinite order kernel function. Here are the most common kernel functions: Biweight : f(x) = 0 if |x| \\leq 1 else f(x)=\\frac{15}{16}(1-x^2)^2 Cosine : f(x) = 0 if |x| \\leq 1 else f(x)=\\frac{\\pi}{4}\\cos( \\frac{\\pi x}{2} ) Epanechnikov : f(x) = 0 if |x| \\leq 1 else f(x) = 0.75 \\cdot (1 - x^2 ) Normal : f(x) = \\frac{\\exp( -x^2/2)}{ \\sqrt{ 2 \\pi }} Triweight : f(x) = 0 if |x| \\leq 1 else f(x)=\\frac{35}{32}(1-x^2)^3 Uniform : f(x) = 0 if |x|<1 else f(x) = 0.5 b , the kernel function bandwidth References https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator Kaplan, E. L.; Meier, P. (1958). \"Nonparametric estimation from incomplete observations\". J. Amer. Statist. Assoc. 53 (282): 457\u2013481. doi:10.2307/2281868. JSTOR 2281868. https://www.researchgate.net/publication/50940632_Understanding_survival_analysis_Kaplan-Meier_estimate survPresmooth: An R Package for PreSmooth Estimation in Survival Analysis Nonparametric density estimation from censored data CDF and survival function estimation with infinite-order kernels","title":"Theory"},{"location":"models/non_parametric.html#non-parametric-models","text":"Non Parametric models offer a straightforward and easy-to-interpret way to compute the survival and hazard functions without imposing any assumptions. Pysurvival provides the following non-parametric models: Kaplan-Meier model ( KaplanMeierModel ) Smooth Kaplan-Meier model ( SmoothKaplanMeierModel )","title":"Non Parametric models"},{"location":"models/non_parametric.html#kaplan-meier-model","text":"One of the most straight-forward ways to estimate the Survival function S(t) of an entire group, is by using the Kaplan-Meier method. Given N units in a cohort, let's assume that there are J distinct actual event times such that t_1 < t_2 < ... < t_J with J \\leq N , then the Survival function estimator S_{KM}(t) is given by: \\begin{equation*} S_{KM}(t) = \\prod_{t_j \\leq t} \\left(1-\\frac{d_j}{r_j} \\right) \\end{equation*} with: S_{KM}(0) = 1 d_j is the number of individuals experiencing an event at t_j r_j is the number of individuals at risk within [t_{j-1}, t_j) - those who have not been censored or experienced an event","title":"Kaplan-Meier model"},{"location":"models/non_parametric.html#smooth-kaplan-meier","text":"Despite its ease of use, the main drawback of the Kaplan-Meier estimator is that it is a step function with jumps. Kernel smoothing can therefore solve this issue, provided that the best kernel and bandwidth are properly chosen. Let S_b(t) be a Smooth estimator of the Kaplan-Meier survival function. S_b(t) can be written such that: \\begin{equation*} S_{b}(t) = \\sum_j s_j K\\left( \\frac{t-T_j}{b}\\right) \\end{equation*} with: s_j , the height of the jump of the Kaplan-Meier estimator at T_j K , the infinite order kernel function. Here are the most common kernel functions: Biweight : f(x) = 0 if |x| \\leq 1 else f(x)=\\frac{15}{16}(1-x^2)^2 Cosine : f(x) = 0 if |x| \\leq 1 else f(x)=\\frac{\\pi}{4}\\cos( \\frac{\\pi x}{2} ) Epanechnikov : f(x) = 0 if |x| \\leq 1 else f(x) = 0.75 \\cdot (1 - x^2 ) Normal : f(x) = \\frac{\\exp( -x^2/2)}{ \\sqrt{ 2 \\pi }} Triweight : f(x) = 0 if |x| \\leq 1 else f(x)=\\frac{35}{32}(1-x^2)^3 Uniform : f(x) = 0 if |x|<1 else f(x) = 0.5 b , the kernel function bandwidth","title":"Smooth Kaplan-Meier"},{"location":"models/non_parametric.html#references","text":"https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator Kaplan, E. L.; Meier, P. (1958). \"Nonparametric estimation from incomplete observations\". J. Amer. Statist. Assoc. 53 (282): 457\u2013481. doi:10.2307/2281868. JSTOR 2281868. https://www.researchgate.net/publication/50940632_Understanding_survival_analysis_Kaplan-Meier_estimate survPresmooth: An R Package for PreSmooth Estimation in Survival Analysis Nonparametric density estimation from censored data CDF and survival function estimation with infinite-order kernels","title":"References"},{"location":"models/nonlinear_coxph.html","text":"h1, h2, h3 { color: #04A9F4; } DeepSurv/Non-Linear model The NonLinear CoxPH model was popularized by Katzman et al. in DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network by allowing the use of Neural Networks within the original design and therefore introducing more modeling flexibility. Instance To create an instance, use pysurvival.models.semi_parametric.NonLinearCoxPHModel . Attributes baseline_hazard : array-like -- values of the hazard function when x \\equiv 0 baseline_survival : array-like -- values of the survival function when x \\equiv 0 structure : list of dictionaries -- Provides the structure of the MLP within the model times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) Methods __init__ - Initialization NonLinearCoxPH(structure, auto_scaler=True) Parameters: structure : list of dictionaries -- Provides the structure of the MLP within the model. ex: structure = [ {'activation': 'ReLU', 'num_units': 128}, {'activation': 'Tanh', 'num_units': 128}, ] . Each dictionary corresponds to a fully connected hidden layer: num_units is the number of hidden units in this layer activation is the activation function that will be used. The list of all available activation functions can be found here . In case there are more than one dictionary, each hidden layer will be applied in the resulting MLP, using the order it is provided in the structure . auto_scaler : boolean (default=True) -- Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, dropout = 0.2, l2_smooth=1e-2, batch_normalization=False, bn_and_dropout=False, display_loss = True, verbose=True, extra_pct_time = 0.1) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- number of iterations in the optimization dropout : float (default=0.5) -- randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients batch_normalization : bool (default=True) -- whether or not applying Batch Normalization or not bn_and_dropout : bool (default=False) -- whether or not applying Batch Normalization and Dropout at the same time verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function Example Let's now take a look at how to use the NonLinear CoxPH model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.semi_parametric import NonLinearCoxPHModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score #%pylab inline #### 2 - Generating the dataset from a nonlinear Weibull parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'weibull' , risk_type = 'gaussian' , censored_parameter = 2.1 , alpha = 0.1 , beta =3.2 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =3 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event -13.851194 0.324245 -0.153511 1.925692 1. -0.094204 0.050048 -0.214262 0.000000 0. pySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 10 , 5 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the NonLinear CoxPH model and fitting the data. # Defining the MLP structure. Here we will build a 1-hidden layer # with 150 units and `BentIdentity` as its activation function structure = [ { 'activation' : 'BentIdentity' , 'num_units' : 150 }, ] # Building the model nonlinear_coxph = NonLinearCoxPHModel(structure = structure) nonlinear_coxph . fit(X_train, T_train, E_train, lr =1e-3 , init_method = 'xav_uniform' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(nonlinear_coxph, X_test, T_test, E_test, t_max =10 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. thus the model is likely to yield great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Nonlinear CoxPH respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = nonlinear_coxph . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(nonlinear_coxph . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual and Predicted survival functions","title":"DeepSurv/Nonlinear CoxPH (API)"},{"location":"models/nonlinear_coxph.html#deepsurvnon-linear-model","text":"The NonLinear CoxPH model was popularized by Katzman et al. in DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network by allowing the use of Neural Networks within the original design and therefore introducing more modeling flexibility.","title":"DeepSurv/Non-Linear model"},{"location":"models/nonlinear_coxph.html#instance","text":"To create an instance, use pysurvival.models.semi_parametric.NonLinearCoxPHModel .","title":"Instance"},{"location":"models/nonlinear_coxph.html#attributes","text":"baseline_hazard : array-like -- values of the hazard function when x \\equiv 0 baseline_survival : array-like -- values of the survival function when x \\equiv 0 structure : list of dictionaries -- Provides the structure of the MLP within the model times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/nonlinear_coxph.html#methods","text":"__init__ - Initialization NonLinearCoxPH(structure, auto_scaler=True) Parameters: structure : list of dictionaries -- Provides the structure of the MLP within the model. ex: structure = [ {'activation': 'ReLU', 'num_units': 128}, {'activation': 'Tanh', 'num_units': 128}, ] . Each dictionary corresponds to a fully connected hidden layer: num_units is the number of hidden units in this layer activation is the activation function that will be used. The list of all available activation functions can be found here . In case there are more than one dictionary, each hidden layer will be applied in the resulting MLP, using the order it is provided in the structure . auto_scaler : boolean (default=True) -- Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, dropout = 0.2, l2_smooth=1e-2, batch_normalization=False, bn_and_dropout=False, display_loss = True, verbose=True, extra_pct_time = 0.1) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- Initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- number of iterations in the optimization dropout : float (default=0.5) -- randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients batch_normalization : bool (default=True) -- whether or not applying Batch Normalization or not bn_and_dropout : bool (default=False) -- whether or not applying Batch Normalization and Dropout at the same time verbose : bool (default=True) -- whether or not producing detailed logging about the modeling Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function","title":"Methods"},{"location":"models/nonlinear_coxph.html#example","text":"Let's now take a look at how to use the NonLinear CoxPH model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.semi_parametric import NonLinearCoxPHModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score #%pylab inline #### 2 - Generating the dataset from a nonlinear Weibull parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'weibull' , risk_type = 'gaussian' , censored_parameter = 2.1 , alpha = 0.1 , beta =3.2 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =3 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event -13.851194 0.324245 -0.153511 1.925692 1. -0.094204 0.050048 -0.214262 0.000000 0. pySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 10 , 5 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the NonLinear CoxPH model and fitting the data. # Defining the MLP structure. Here we will build a 1-hidden layer # with 150 units and `BentIdentity` as its activation function structure = [ { 'activation' : 'BentIdentity' , 'num_units' : 150 }, ] # Building the model nonlinear_coxph = NonLinearCoxPHModel(structure = structure) nonlinear_coxph . fit(X_train, T_train, E_train, lr =1e-3 , init_method = 'xav_uniform' ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(nonlinear_coxph, X_test, T_test, E_test, t_max =10 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. thus the model is likely to yield great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Nonlinear CoxPH respectively. #### 6 - Comparing actual and predictions # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t predicted = nonlinear_coxph . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(nonlinear_coxph . times, predicted, color = 'blue' , label = 'predicted' , lw =2 ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual and Predicted survival functions","title":"Example"},{"location":"models/parametric.html","text":"h1, h2, h3 { color: #04A9F4; } Parametric models Unlike Semi-Parametric models , Parametric models are better suited for forecasting and will return smooth functions of h(t, x) or S(t, x) . The most common parametric models available in PySurvival are: Exponential Weibull Gompertz Log-Logistic Log-Normal Instance To create an instance: use pysurvival.models.parametric.ExponentialModel to build an Exponential model use pysurvival.models.parametric.WeibullModel to build a Weibull model use pysurvival.models.parametric.GompertzModel to build a Gompertz model use pysurvival.models.parametric.LogLogisticModel to build a Log-Logistic model use pysurvival.models.parametric.LogNormalModel to build a Log-Normal model Attributes aic : double -- value of the Akaike information criterion times : array-like -- representation of the time axis for the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) API All the models share the same API. __init__ - Initialization __init__(bins=100, auto_scaler=True) Parameters: bins : int (default=100) Number of subdivisions of the time axis auto_scaler : boolean (default=True) Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, l2_reg=1e-2, verbose=True, is_min_time_zero = True, extra_pct_time = 0.1)) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- number of iterations in the optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients verbose : bool (default=True) -- whether or not producing detailed logging about the modeling extra_pct_time : float (default=0.1) -- providing an extra fraction of time in the time axis is_min_time_zero : bool (default=True) -- whether the the time axis starts at 0 Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function Example Let's now take a look at how to use Parametric models on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.parametric import GompertzModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Gompertz parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Gompertz' , risk_type = 'linear' , censored_parameter = 10.0 , alpha = .01 , beta = 3.0 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 3 ) # Showing a few data-points time_column = 'time' event_column = 'event' dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event 1.841646 -0.670071 1.157705 4.4983 1.0 2.825421 -9.562958 0.462503 0.0000 0.0 PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Gompertz model and fitting the data. # Building the model gomp_model = GompertzModel() gomp_model . fit(X_train, T_train, E_train, lr =1e-2 , init_method = 'zeros' , optimizer = 'adam' , l2_reg = 1e-3 , num_epochs =2000 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(gomp_model, X_test, T_test, E_test) #0.77 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(gomp_model, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Parametric model respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = gomp_model . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(gomp_model . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Parametric models (API)"},{"location":"models/parametric.html#parametric-models","text":"Unlike Semi-Parametric models , Parametric models are better suited for forecasting and will return smooth functions of h(t, x) or S(t, x) . The most common parametric models available in PySurvival are: Exponential Weibull Gompertz Log-Logistic Log-Normal","title":"Parametric models"},{"location":"models/parametric.html#instance","text":"To create an instance: use pysurvival.models.parametric.ExponentialModel to build an Exponential model use pysurvival.models.parametric.WeibullModel to build a Weibull model use pysurvival.models.parametric.GompertzModel to build a Gompertz model use pysurvival.models.parametric.LogLogisticModel to build a Log-Logistic model use pysurvival.models.parametric.LogNormalModel to build a Log-Normal model","title":"Instance"},{"location":"models/parametric.html#attributes","text":"aic : double -- value of the Akaike information criterion times : array-like -- representation of the time axis for the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/parametric.html#api","text":"All the models share the same API. __init__ - Initialization __init__(bins=100, auto_scaler=True) Parameters: bins : int (default=100) Number of subdivisions of the time axis auto_scaler : boolean (default=True) Determines whether a sklearn scaler should be automatically applied fit - Fit the estimator based on the given parameters fit(X, T, E, init_method = 'glorot_uniform', optimizer ='adam', lr = 1e-4, num_epochs = 1000, l2_reg=1e-2, verbose=True, is_min_time_zero = True, extra_pct_time = 0.1)) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. init_method : str (default = 'glorot_uniform') -- initialization method to use. Here are the possible options: glorot_uniform : Glorot/Xavier uniform initializer he_uniform : He uniform variance scaling initializer uniform : Initializing tensors with uniform (-1, 1) distribution glorot_normal : Glorot normal initializer, he_normal : He normal initializer. normal : Initializing tensors with standard normal distribution ones : Initializing tensors to 1 zeros : Initializing tensors to 0 orthogonal : Initializing tensors with a orthogonal matrix, optimizer : str (default = 'adam') -- iterative method for optimizing a differentiable objective function. Here are the possible options: adadelta adagrad adam adamax rmsprop sparseadam sgd lr : float (default=1e-4) -- learning rate used in the optimization num_epochs : int (default=1000) -- number of iterations in the optimization l2_reg : float (default=1e-4) -- L2 regularization parameter for the model coefficients verbose : bool (default=True) -- whether or not producing detailed logging about the modeling extra_pct_time : float (default=0.1) -- providing an extra fraction of time in the time axis is_min_time_zero : bool (default=True) -- whether the the time axis starts at 0 Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function","title":"API"},{"location":"models/parametric.html#example","text":"Let's now take a look at how to use Parametric models on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.parametric import GompertzModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Gompertz parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'Gompertz' , risk_type = 'linear' , censored_parameter = 10.0 , alpha = .01 , beta = 3.0 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features = 3 ) # Showing a few data-points time_column = 'time' event_column = 'event' dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 time event 1.841646 -0.670071 1.157705 4.4983 1.0 2.825421 -9.562958 0.462503 0.0000 0.0 PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Gompertz model and fitting the data. # Building the model gomp_model = GompertzModel() gomp_model . fit(X_train, T_train, E_train, lr =1e-2 , init_method = 'zeros' , optimizer = 'adam' , l2_reg = 1e-3 , num_epochs =2000 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(gomp_model, X_test, T_test, E_test) #0.77 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(gomp_model, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the Parametric model respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = gomp_model . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(gomp_model . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/parametric_theory.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Parametric models Introduction We've seen that with Semi-Parametric models the time component of the hazard function is left unspecified. In case the hazard function or the Survival function are known to follow or closely approximate a known distribution, it is better to use Parametric models . Unlike Semi-Parametric models, Parametric models are better suited for forecasting and will return smooth functions of h(t, x) or S(t, x) . The most common parametric models are: Exponential Weibull Gompertz Log-Logistic Lognormal Models Exponential The exponential distribution is the simplest and most important distribution in survival studies. Being independent of prior information, it is known as a \"lack of memory\" distribution requiring that the present age of the living organism does not influence its future survival. In this model, the hazard rate is constant over time such as: \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\lambda \\\\ S(t, x_i) & = e^{- \\lambda \\cdot t} \\\\ \\end{split} \\end{equation*} with \\alpha and \\vec{\\omega} the coefficients to find. Weibull The Weibull distribution is a generalized form of the exponential distribution and is de facto more flexible than the exponential model. \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\lambda \\beta (\\lambda t)^{\\beta-1} \\\\ S(t, x_i) & = e^{- (\\lambda t)^\\beta} \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find. Gompertz The Gompertz distribution is a continuous probability distribution, that has an exponentially increasing failure rate, and is often applied to analyze survival data. \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\lambda e^{\\beta t} \\\\ S(t, x_i) & = e^{-\\frac{\\lambda}{\\beta }(e^{\\beta t}-1)} \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find. Log-Logistic As the name suggests, the log-logistic distribution is the distribution of a variable whose logarithm has the logistic distribution. \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\frac{\\beta\\lambda^\\beta t^{\\beta-1}}{1+(\\lambda t)^\\beta} \\\\ S(t, x_i) & = \\frac{1}{1+(\\lambda t)^\\beta} \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find. Log-Normal The lognormal distribution is used to model continuous random quantities when the distribution is believed to be skewed, such as lifetime variables \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\frac{\\frac{1}{t \\beta \\sqrt{2\\pi}} \\exp \\left( -\\left[ \\frac{\\log(t) - \\log(\\lambda)}{\\beta \\sqrt{2}} \\right]^2 \\right)}{1 - \\Phi \\left( \\frac{\\log(t) - \\log(\\lambda) }{\\beta}\\right)} \\\\ S(t, x_i) & = 1 - \\Phi \\left( \\frac{\\log(t) - \\log(\\lambda) }{\\beta}\\right) \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find; \\Phi is the standard normal cdf. Building and selecting models Likelihood All the parametric models will be fitted using the maximum likelihood estimation (MLE). Regardless of the model used, the likelihood is the product over all of the observations such that: \\begin{equation*} \\begin{split} L & = \\prod_{i=1}^{N} f(T_i, x_i)^{\\delta_i} S(T_i, x_i)^{1-\\delta_i} \\\\ & = \\prod_{i=1}^{N} h(T_i, x_i)^{\\delta_i} S(T_i, x_i) \\\\ \\end{split} \\end{equation*} Selecting the best model To select the best model, we might use the Akaike\u2019s Information Criterion (AIC) to distinguish between different parametric models. Typically, we will pick the model whose log-likelihood is the smallest. Akaike\u2019s method penalizes each model\u2019s log likelihood, \\log(L) , to reflect the number of parameters that are being estimated and then compares them: \\begin{equation*} AIC = \u22122 \\log(L) + 2*\\text{num_coefficients} \\end{equation*} References Princeton Lecture - Parametric Survival Models UCSD Lecture - Parametric Survival Models The Log-Logistic Distribution The Log-Normal Distribution","title":"Theory"},{"location":"models/parametric_theory.html#parametric-models","text":"","title":"Parametric models"},{"location":"models/parametric_theory.html#introduction","text":"We've seen that with Semi-Parametric models the time component of the hazard function is left unspecified. In case the hazard function or the Survival function are known to follow or closely approximate a known distribution, it is better to use Parametric models . Unlike Semi-Parametric models, Parametric models are better suited for forecasting and will return smooth functions of h(t, x) or S(t, x) . The most common parametric models are: Exponential Weibull Gompertz Log-Logistic Lognormal","title":"Introduction"},{"location":"models/parametric_theory.html#models","text":"","title":"Models"},{"location":"models/parametric_theory.html#exponential","text":"The exponential distribution is the simplest and most important distribution in survival studies. Being independent of prior information, it is known as a \"lack of memory\" distribution requiring that the present age of the living organism does not influence its future survival. In this model, the hazard rate is constant over time such as: \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\lambda \\\\ S(t, x_i) & = e^{- \\lambda \\cdot t} \\\\ \\end{split} \\end{equation*} with \\alpha and \\vec{\\omega} the coefficients to find.","title":"Exponential"},{"location":"models/parametric_theory.html#weibull","text":"The Weibull distribution is a generalized form of the exponential distribution and is de facto more flexible than the exponential model. \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\lambda \\beta (\\lambda t)^{\\beta-1} \\\\ S(t, x_i) & = e^{- (\\lambda t)^\\beta} \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find.","title":"Weibull"},{"location":"models/parametric_theory.html#gompertz","text":"The Gompertz distribution is a continuous probability distribution, that has an exponentially increasing failure rate, and is often applied to analyze survival data. \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\lambda e^{\\beta t} \\\\ S(t, x_i) & = e^{-\\frac{\\lambda}{\\beta }(e^{\\beta t}-1)} \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find.","title":"Gompertz"},{"location":"models/parametric_theory.html#log-logistic","text":"As the name suggests, the log-logistic distribution is the distribution of a variable whose logarithm has the logistic distribution. \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\frac{\\beta\\lambda^\\beta t^{\\beta-1}}{1+(\\lambda t)^\\beta} \\\\ S(t, x_i) & = \\frac{1}{1+(\\lambda t)^\\beta} \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find.","title":"Log-Logistic"},{"location":"models/parametric_theory.html#log-normal","text":"The lognormal distribution is used to model continuous random quantities when the distribution is believed to be skewed, such as lifetime variables \\begin{equation*} \\begin{split} \\lambda & = \\alpha e^{ \\vec{ x_i} \\cdot \\vec{ \\omega} }\\\\ h(t, x_i) & = \\frac{\\frac{1}{t \\beta \\sqrt{2\\pi}} \\exp \\left( -\\left[ \\frac{\\log(t) - \\log(\\lambda)}{\\beta \\sqrt{2}} \\right]^2 \\right)}{1 - \\Phi \\left( \\frac{\\log(t) - \\log(\\lambda) }{\\beta}\\right)} \\\\ S(t, x_i) & = 1 - \\Phi \\left( \\frac{\\log(t) - \\log(\\lambda) }{\\beta}\\right) \\\\ \\end{split} \\end{equation*} with \\alpha , \\beta and \\vec{\\omega} the coefficients to find; \\Phi is the standard normal cdf.","title":"Log-Normal"},{"location":"models/parametric_theory.html#building-and-selecting-models","text":"","title":"Building and selecting models"},{"location":"models/parametric_theory.html#likelihood","text":"All the parametric models will be fitted using the maximum likelihood estimation (MLE). Regardless of the model used, the likelihood is the product over all of the observations such that: \\begin{equation*} \\begin{split} L & = \\prod_{i=1}^{N} f(T_i, x_i)^{\\delta_i} S(T_i, x_i)^{1-\\delta_i} \\\\ & = \\prod_{i=1}^{N} h(T_i, x_i)^{\\delta_i} S(T_i, x_i) \\\\ \\end{split} \\end{equation*}","title":"Likelihood"},{"location":"models/parametric_theory.html#selecting-the-best-model","text":"To select the best model, we might use the Akaike\u2019s Information Criterion (AIC) to distinguish between different parametric models. Typically, we will pick the model whose log-likelihood is the smallest. Akaike\u2019s method penalizes each model\u2019s log likelihood, \\log(L) , to reflect the number of parameters that are being estimated and then compares them: \\begin{equation*} AIC = \u22122 \\log(L) + 2*\\text{num_coefficients} \\end{equation*}","title":"Selecting the best model"},{"location":"models/parametric_theory.html#references","text":"Princeton Lecture - Parametric Survival Models UCSD Lecture - Parametric Survival Models The Log-Logistic Distribution The Log-Normal Distribution","title":"References"},{"location":"models/random_survival_forest.html","text":"h1, h2, h3 { color: #04A9F4; } Random Survival Forest model The Random Survival Forest or RSF is an extension of the Random Forest model, introduced by Breiman et al in 2001 , that can take into account censoring. The RSF models was developped by Ishwaran et al. in 2008 . Instance To create an instance, use pysurvival.models.survival_forest.RandomSurvivalForestModel . Attributes max_features : str or int -- The number of features randomly chosen at each split. num_trees : int -- number of trees contained in the forest times : array-like -- representation of the time axis for the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) variable_importance : dict -- importance of each feature (the higher, the more important the feature is). The importance is the difference between the perturbed and unperturbed error rate for each feature. Methods __init__ - Initialize the estimator RandomSurvivalForestModel(num_trees = 10) Parameters: num_trees : int (default=10) -- number of trees that will be built in the forest. fit - Fit the estimator based on the given parameters fit(X, T, E, max_features = 'sqrt', max_depth = 5, min_node_size = 10, num_threads = -1, weights = None, sample_size_pct = 0.63, importance_mode = 'normalized_permutation', seed = None, save_memory=False ) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. max_features : int, float or string (default=\"sqrt\") -- number of features to consider when looking for the best split: If int , then consider the given value at each split. If float , then max_features is a fraction and int(max_features * n_features) features are considered at each split. If \"sqrt\" , then max_features=sqrt(n_features) If \"log2\" , then max_features=log2(n_features) . If \"all\" , then max_features=n_features . min_node_size : int (default=10) -- minimum number of samples required to be at a leaf node num_threads : int (default= -1) -- number of jobs to run in parallel during training. If -1, then the number of jobs is set to the total number of available cores. weights : array-like (default = None) -- weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap. The sum of the weights needs to be 1. sample_size_pct : double (default = 0.63) -- percentage of original samples used in each tree building importance_mode : str (default='impurity_corrected') -- variable importance mode. Here are the available options: impurity or impurity_corrected : it's the unbiased heterogeneity reduction developed by Sandri & Zuccolotto (2008) permutation it's unnormalized as recommended by Nicodemus et al . normalized_permutation it's normalized version of the permutation importance computations by Breiman et al. seed : int (default=None) -- seed used by the random number generator. If None, the current timestamp converted in UNIX is used. save_memory : bool (default=False) -- Use memory saving splitting mode. This will slow down the model training. So, only set to True if you encounter memory problems. Returns: self : object Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score Example Let's now take a look at how to use the Random Survival Forest model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.survival_forest import RandomSurvivalForestModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Exponential parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'exponential' , risk_type = 'linear' , censored_parameter = 1 , alpha = 3 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 0.061498 7.065547 0.004457 0.131379 15.412209 0. 0.079149 6.732271 0.008654 0.090398 0.000700 1. PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Conditional model and fitting the data. # Building the model rsf = RandomSurvivalForestModel(num_trees =200 ) rsf . fit(X_train, T_train, E_train, max_features = \"sqrt\" , max_depth =5 , min_node_size =20 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(rsf, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(rsf, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the RSF respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = rsf . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(rsf . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Random Survival Forest (API)"},{"location":"models/random_survival_forest.html#random-survival-forest-model","text":"The Random Survival Forest or RSF is an extension of the Random Forest model, introduced by Breiman et al in 2001 , that can take into account censoring. The RSF models was developped by Ishwaran et al. in 2008 .","title":"Random Survival Forest model"},{"location":"models/random_survival_forest.html#instance","text":"To create an instance, use pysurvival.models.survival_forest.RandomSurvivalForestModel .","title":"Instance"},{"location":"models/random_survival_forest.html#attributes","text":"max_features : str or int -- The number of features randomly chosen at each split. num_trees : int -- number of trees contained in the forest times : array-like -- representation of the time axis for the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) variable_importance : dict -- importance of each feature (the higher, the more important the feature is). The importance is the difference between the perturbed and unperturbed error rate for each feature.","title":"Attributes"},{"location":"models/random_survival_forest.html#methods","text":"__init__ - Initialize the estimator RandomSurvivalForestModel(num_trees = 10) Parameters: num_trees : int (default=10) -- number of trees that will be built in the forest. fit - Fit the estimator based on the given parameters fit(X, T, E, max_features = 'sqrt', max_depth = 5, min_node_size = 10, num_threads = -1, weights = None, sample_size_pct = 0.63, importance_mode = 'normalized_permutation', seed = None, save_memory=False ) Parameters: X : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. max_features : int, float or string (default=\"sqrt\") -- number of features to consider when looking for the best split: If int , then consider the given value at each split. If float , then max_features is a fraction and int(max_features * n_features) features are considered at each split. If \"sqrt\" , then max_features=sqrt(n_features) If \"log2\" , then max_features=log2(n_features) . If \"all\" , then max_features=n_features . min_node_size : int (default=10) -- minimum number of samples required to be at a leaf node num_threads : int (default= -1) -- number of jobs to run in parallel during training. If -1, then the number of jobs is set to the total number of available cores. weights : array-like (default = None) -- weights for sampling of training observations. Observations with larger weights will be selected with higher probability in the bootstrap. The sum of the weights needs to be 1. sample_size_pct : double (default = 0.63) -- percentage of original samples used in each tree building importance_mode : str (default='impurity_corrected') -- variable importance mode. Here are the available options: impurity or impurity_corrected : it's the unbiased heterogeneity reduction developed by Sandri & Zuccolotto (2008) permutation it's unnormalized as recommended by Nicodemus et al . normalized_permutation it's normalized version of the permutation importance computations by Breiman et al. seed : int (default=None) -- seed used by the random number generator. If None, the current timestamp converted in UNIX is used. save_memory : bool (default=False) -- Use memory saving splitting mode. This will slow down the model training. So, only set to True if you encounter memory problems. Returns: self : object Returns: self : object predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score","title":"Methods"},{"location":"models/random_survival_forest.html#example","text":"Let's now take a look at how to use the Random Survival Forest model on a simulation dataset generated from a parametric model . #### 1 - Importing packages import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.model_selection import train_test_split from pysurvival.models.simulations import SimulationModel from pysurvival.models.survival_forest import RandomSurvivalForestModel from pysurvival.utils.metrics import concordance_index from pysurvival.utils.display import integrated_brier_score % pylab inline #### 2 - Generating the dataset from a Exponential parametric model # Initializing the simulation model sim = SimulationModel( survival_distribution = 'exponential' , risk_type = 'linear' , censored_parameter = 1 , alpha = 3 ) # Generating N random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =4 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 time event 0.061498 7.065547 0.004457 0.131379 15.412209 0. 0.079149 6.732271 0.008654 0.090398 0.000700 1. PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model #### 3 - Creating the modeling dataset # Defining the features features = sim . features # Building training and testing sets # index_train, index_test = train_test_split( range (N), test_size = 0.2 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E input X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[ 'time' ] . values, data_test[ 'time' ] . values E_train, E_test = data_train[ 'event' ] . values, data_test[ 'event' ] . values #### 4 - Creating an instance of the Conditional model and fitting the data. # Building the model rsf = RandomSurvivalForestModel(num_trees =200 ) rsf . fit(X_train, T_train, E_train, max_features = \"sqrt\" , max_depth =5 , min_node_size =20 ) #### 5 - Cross Validation / Model Performances c_index = concordance_index(rsf, X_test, T_test, E_test) #0.81 print ( 'C-index: {:.2f}' . format(c_index)) ibs = integrated_brier_score(rsf, X_test, T_test, E_test, t_max =30 , figure_size = ( 20 , 6.5 ) ) print ( 'IBS: {:.2f}' . format(ibs)) We can see that the c-index is well above 0.5 and that the Prediction error curve is below the 0.25 limit, thus the model yields great performances. Figure 2 - Prediction error curve We can show this by randomly selecting datapoints and comparing the actual and predicted survival functions, computed by the simulation model and the RSF respectively. # Initializing the figure fig, ax = plt . subplots(figsize = ( 8 , 4 )) # Randomly extracting a data-point that experienced an event choices = np . argwhere((E_test ==1. ) & (T_test >=1 )) . flatten() k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T_test[k] # Computing the Survival function for all times t survival = rsf . predict_survival(X_test . values[k, :]) . flatten() actual = sim . predict_survival(X_test . values[k, :]) . flatten() # Displaying the functions plt . plot(rsf . times, survival, color = 'blue' , label = 'predicted' , lw =4 , ls = '-.' ) plt . plot(sim . times, actual, color = 'red' , label = 'actual' , lw =2 ) # Actual time plt . axvline(x = t, color = 'black' , ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5 ), xytext = (t, 0.5 ), fontsize =12 ) # Show everything title = \"Comparing Survival functions between Actual and Predicted\" plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 3 - Comparing Actual vs Predicted","title":"Example"},{"location":"models/semi_parametric.html","text":"h1, h2, h3 { color: #04A9F4; } Semi-Parametric/Cox Proportional Hazard models Cox Proportional Hazard model Hazard function's formula When it comes to predicting the survival function for a specific unit, the Cox Proportional Hazard Model (CoxPH) is usually the go-to model. The CoxPH model is a semi-parametric model that focuses on modeling the hazard function h(t, x_i) , by assuming that its time component \\lambda_0(t) and feature component \\eta(\\vec{x_i}) are proportional such that: \\begin{equation*} h(t, \\vec{x_i}) = \\lambda_0(t)\\eta(\\vec{x_i}) \\end{equation*} with: \\lambda_0(t) , is the baseline function, which is usually not specified. \\eta(\\vec{x_i}) , is the risk function usually expressed via a linear representation such that \\eta(\\vec{x_i}) = \\exp \\left( \\sum_{j=1}^p x^i_j\\omega_j \\right) . \\omega_j are the coefficients to determine Building the model The model can be built by calculating the Efron's partial likelihood to take ties into account. The partial likelihood L(\\omega) can be written such that: \\displaystyle L(\\omega )=\\prod _{j}{\\frac {\\prod _{i\\in H_{j}}\\theta _{i}}{\\prod _{s =0}^{m_j-1}[\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}-{\\frac {s}{m_j}}\\sum _{i\\in H_{j}}\\theta _{i}]}}. the log partial likelihood is \\displaystyle l (\\omega )=\\sum _{j}\\left(\\sum _{i\\in H_{j}}X_{i}\\cdot \\omega -\\sum _{s =0}^{m_j-1}\\log(\\phi _{j,s ,m_j})\\right). the gradient is \\displaystyle \\vec{\\nabla l(\\omega )}=\\sum _{j}\\left(\\sum _{i\\in H_{j}}X_{i}-\\sum _{s =0}^{m_j-1}{\\frac {z_{j,s ,m_j}}{\\phi _{j,s ,m_j}}}\\right). the Hessian matrix is \\displaystyle \\nabla^2 l(\\omega )=-\\sum _{j}\\sum _{s =0}^{m_j-1}\\left({\\frac {{Z}_{j,s ,m_j}}{\\phi _{j,s ,m_j}}}-{\\frac {z_{j,s ,m}z_{j,s ,m_j}^{T}}{\\phi _{j,s ,m_j}^{2}}}\\right). We can now use the Newton-Optimization schema to fit the model: \\begin{equation*} \\omega_{new} = \\omega_{old} - \\nabla^2 l(\\omega )^{-1} \\cdot \\vec{\\nabla l(\\omega )} \\end{equation*} with: H_j = \\left\\{ i; T_i = t_j \\text{ and } E_i = 1 \\right \\} , {m_j} = \\left|H_j \\right|. \\theta _{i}= \\exp\\left(X_{i} \\cdot \\omega \\right) \\phi _{j,s ,m_j}=\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}-{\\frac {s}{m_j}}\\sum _{i\\in H_{j}}\\theta _{i} z_{j,s ,m_j}=\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}X_{i}-{\\frac {s }{m_j}}\\sum _{i\\in H_{j}}\\theta _{i}X_{i}. {Z}_{j,s ,m_j}=\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}X_{i}X_{i}^{T }-{\\frac {s }{m_j}}\\sum _{i\\in H_{j}}\\theta _{i}X_{i}X_{i}^{T} DeepSurv/NonLinear CoxPH model Hazard function's formula The NonLinear CoxPH model was popularized by Katzman et al. in DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network by allowing the use of Neural Networks within the original design. Here the hazard function h(t, x_i) can be written as \\begin{equation*} h(t, \\vec{x_i}) = \\lambda_0(t)\\Psi(\\vec{x_i}) \\end{equation*} with: \\Psi(\\vec{x_i}) = \\exp(\\psi(\\vec{x_i})) , where \\psi is a non-linear risk function. Building the model We are still using the Efron's partial likelihood to take ties into account, but here the hazard function is h(t, \\vec{x_i}) = \\lambda_0(t)\\Psi(\\vec{x_i}) . Thus, the log partial likelihood is \\begin{equation*} l (\\omega )=\\sum _{j}\\left(\\sum _{i\\in H_{j}}\\log(\\Psi(\\vec{x_i})) -\\sum _{s =0}^{m_j-1}\\log \\left(\\sum _{i:T_{i}\\geq t_{j}}\\Psi(\\vec{x_i})-{\\frac {s }{m_j}}\\sum _{i\\in H_{j}}\\Psi(\\vec{x_i})\\right)\\right). \\end{equation*} As the Hessian matrix will be too complicated to calculate, we will use PyTorch to compute the gradient and perform a First-Order optimization. References Wikipedia - Proportional hazards model Cox, David R. \"Regression models and life\u2010tables.\" Journal of the Royal Statistical Society: Series B (Methodological) 34.2 (1972): 187-202. Katzman, Jared, et al. \"DeepSurv: Personalized treatment recommender system using A Cox proportional hazards deep neural network.\" arXiv preprint arXiv:1606.00931 (2016).","title":"Theory"},{"location":"models/semi_parametric.html#semi-parametriccox-proportional-hazard-models","text":"","title":"Semi-Parametric/Cox Proportional Hazard models"},{"location":"models/semi_parametric.html#cox-proportional-hazard-model","text":"","title":"Cox Proportional Hazard model"},{"location":"models/semi_parametric.html#hazard-functions-formula","text":"When it comes to predicting the survival function for a specific unit, the Cox Proportional Hazard Model (CoxPH) is usually the go-to model. The CoxPH model is a semi-parametric model that focuses on modeling the hazard function h(t, x_i) , by assuming that its time component \\lambda_0(t) and feature component \\eta(\\vec{x_i}) are proportional such that: \\begin{equation*} h(t, \\vec{x_i}) = \\lambda_0(t)\\eta(\\vec{x_i}) \\end{equation*} with: \\lambda_0(t) , is the baseline function, which is usually not specified. \\eta(\\vec{x_i}) , is the risk function usually expressed via a linear representation such that \\eta(\\vec{x_i}) = \\exp \\left( \\sum_{j=1}^p x^i_j\\omega_j \\right) . \\omega_j are the coefficients to determine","title":"Hazard function's formula"},{"location":"models/semi_parametric.html#building-the-model","text":"The model can be built by calculating the Efron's partial likelihood to take ties into account. The partial likelihood L(\\omega) can be written such that: \\displaystyle L(\\omega )=\\prod _{j}{\\frac {\\prod _{i\\in H_{j}}\\theta _{i}}{\\prod _{s =0}^{m_j-1}[\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}-{\\frac {s}{m_j}}\\sum _{i\\in H_{j}}\\theta _{i}]}}. the log partial likelihood is \\displaystyle l (\\omega )=\\sum _{j}\\left(\\sum _{i\\in H_{j}}X_{i}\\cdot \\omega -\\sum _{s =0}^{m_j-1}\\log(\\phi _{j,s ,m_j})\\right). the gradient is \\displaystyle \\vec{\\nabla l(\\omega )}=\\sum _{j}\\left(\\sum _{i\\in H_{j}}X_{i}-\\sum _{s =0}^{m_j-1}{\\frac {z_{j,s ,m_j}}{\\phi _{j,s ,m_j}}}\\right). the Hessian matrix is \\displaystyle \\nabla^2 l(\\omega )=-\\sum _{j}\\sum _{s =0}^{m_j-1}\\left({\\frac {{Z}_{j,s ,m_j}}{\\phi _{j,s ,m_j}}}-{\\frac {z_{j,s ,m}z_{j,s ,m_j}^{T}}{\\phi _{j,s ,m_j}^{2}}}\\right). We can now use the Newton-Optimization schema to fit the model: \\begin{equation*} \\omega_{new} = \\omega_{old} - \\nabla^2 l(\\omega )^{-1} \\cdot \\vec{\\nabla l(\\omega )} \\end{equation*} with: H_j = \\left\\{ i; T_i = t_j \\text{ and } E_i = 1 \\right \\} , {m_j} = \\left|H_j \\right|. \\theta _{i}= \\exp\\left(X_{i} \\cdot \\omega \\right) \\phi _{j,s ,m_j}=\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}-{\\frac {s}{m_j}}\\sum _{i\\in H_{j}}\\theta _{i} z_{j,s ,m_j}=\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}X_{i}-{\\frac {s }{m_j}}\\sum _{i\\in H_{j}}\\theta _{i}X_{i}. {Z}_{j,s ,m_j}=\\sum _{i:T_{i}\\geq t_{j}}\\theta _{i}X_{i}X_{i}^{T }-{\\frac {s }{m_j}}\\sum _{i\\in H_{j}}\\theta _{i}X_{i}X_{i}^{T}","title":"Building the model"},{"location":"models/semi_parametric.html#deepsurvnonlinear-coxph-model","text":"","title":"DeepSurv/NonLinear CoxPH model"},{"location":"models/semi_parametric.html#hazard-functions-formula_1","text":"The NonLinear CoxPH model was popularized by Katzman et al. in DeepSurv: Personalized Treatment Recommender System Using A Cox Proportional Hazards Deep Neural Network by allowing the use of Neural Networks within the original design. Here the hazard function h(t, x_i) can be written as \\begin{equation*} h(t, \\vec{x_i}) = \\lambda_0(t)\\Psi(\\vec{x_i}) \\end{equation*} with: \\Psi(\\vec{x_i}) = \\exp(\\psi(\\vec{x_i})) , where \\psi is a non-linear risk function.","title":"Hazard function's formula"},{"location":"models/semi_parametric.html#building-the-model_1","text":"We are still using the Efron's partial likelihood to take ties into account, but here the hazard function is h(t, \\vec{x_i}) = \\lambda_0(t)\\Psi(\\vec{x_i}) . Thus, the log partial likelihood is \\begin{equation*} l (\\omega )=\\sum _{j}\\left(\\sum _{i\\in H_{j}}\\log(\\Psi(\\vec{x_i})) -\\sum _{s =0}^{m_j-1}\\log \\left(\\sum _{i:T_{i}\\geq t_{j}}\\Psi(\\vec{x_i})-{\\frac {s }{m_j}}\\sum _{i\\in H_{j}}\\Psi(\\vec{x_i})\\right)\\right). \\end{equation*} As the Hessian matrix will be too complicated to calculate, we will use PyTorch to compute the gradient and perform a First-Order optimization.","title":"Building the model"},{"location":"models/semi_parametric.html#references","text":"Wikipedia - Proportional hazards model Cox, David R. \"Regression models and life\u2010tables.\" Journal of the Royal Statistical Society: Series B (Methodological) 34.2 (1972): 187-202. Katzman, Jared, et al. \"DeepSurv: Personalized treatment recommender system using A Cox proportional hazards deep neural network.\" arXiv preprint arXiv:1606.00931 (2016).","title":"References"},{"location":"models/simulations.html","text":"h1, h2, h3 { color: #04A9F4; } Simulation models PySurvival can generate random survival times based on the most commonly used distributions such as: Exponential Weibull Gompertz Log-Logistic Lognormal Instance To create an instance, use pysurvival.models.simulations.SimulationModel . Attributes alpha : double -- the scale parameter beta : double -- the shape parameter censored_parameter : double -- coefficient used to calculate the censored distribution. risk_type : string -- Defines the type of risk function. risk_parameter : double -- scaling coefficient of the risk score survival_distribution : string -- Defines a known survival distribution. times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) API __init__ - Initialization SimulationModel( survival_distribution = 'exponential', risk_type = 'linear', censored_parameter = 1., alpha = 1, beta = 1., bins = 100, risk_parameter = 1.) Parameters: survival_distribution : string (default = 'exponential') -- Defines a known survival distribution. The available distributions are: Exponential Weibull Gompertz Log-Logistic Log-Normal risk_type : string (default='linear') -- Defines the type of risk function: - Linear - Square - Gaussian censored_parameter : double (default = 1.) -- Coefficient used to calculate the censored distribution. This distribution is a normal such that N(loc=censored_parameter, scale=5) alpha : double (default = 1.) -- the scale parameter beta : double (default = 1.) -- the shape parameter bins : int (default=100) -- the number of bins of the time axis risk_parameter : double (default = 1.) -- Scaling coefficient for the risk score which can be written as follow: linear : r(x) = \\exp(x \\cdot \\omega) square : r(x) = \\exp( \\text{risk_parameter} *(x \\cdot \\omega)^2) gaussian : r(x) = \\exp \\left( e^{-(x \\cdot \\omega)^2*\\text{risk_parameter}} \\right) generate_data - Generating a dataset of simulated survival times from a given distribution through the hazard function using the Cox model generate_data(num_samples = 100, num_features = 3, feature_weights=None) Parameters: num_samples : int (default=100) -- Number of samples to generate num_features : int (default=3) -- Number of features to generate feature_weights : array-like (default=None) -- list of the coefficients of the underlying Cox-Model. The features linked to each coefficient are generated from random distribution from the following list: binomial chisquare exponential gamma normal uniform laplace If None then feature_weights = [1.]*num_features Returns: dataset : pandas.DataFrame -- dataset of simulated survival times, event status and features predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function Example Let's now see how to generate a dataset designed for survival analysis. import pandas as pd from pysurvival.models.simulations import SimulationModel % pylab inline # Initializing the simulation model sim = SimulationModel( survival_distribution = 'gompertz' , risk_type = 'linear' , censored_parameter = 5.0 , alpha = 0.01 , beta = 5. , ) # Generating N Random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =5 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 x_5 time event 3.956896 124.0 0.018274 57.480199 -5.42258 0.024329 1.0 4.106100 117.0 0.111276 51.770875 4.105588 0.175530 1.0 PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model","title":"Simulation models (API)"},{"location":"models/simulations.html#simulation-models","text":"PySurvival can generate random survival times based on the most commonly used distributions such as: Exponential Weibull Gompertz Log-Logistic Lognormal","title":"Simulation models"},{"location":"models/simulations.html#instance","text":"To create an instance, use pysurvival.models.simulations.SimulationModel .","title":"Instance"},{"location":"models/simulations.html#attributes","text":"alpha : double -- the scale parameter beta : double -- the shape parameter censored_parameter : double -- coefficient used to calculate the censored distribution. risk_type : string -- Defines the type of risk function. risk_parameter : double -- scaling coefficient of the risk score survival_distribution : string -- Defines a known survival distribution. times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/simulations.html#api","text":"__init__ - Initialization SimulationModel( survival_distribution = 'exponential', risk_type = 'linear', censored_parameter = 1., alpha = 1, beta = 1., bins = 100, risk_parameter = 1.) Parameters: survival_distribution : string (default = 'exponential') -- Defines a known survival distribution. The available distributions are: Exponential Weibull Gompertz Log-Logistic Log-Normal risk_type : string (default='linear') -- Defines the type of risk function: - Linear - Square - Gaussian censored_parameter : double (default = 1.) -- Coefficient used to calculate the censored distribution. This distribution is a normal such that N(loc=censored_parameter, scale=5) alpha : double (default = 1.) -- the scale parameter beta : double (default = 1.) -- the shape parameter bins : int (default=100) -- the number of bins of the time axis risk_parameter : double (default = 1.) -- Scaling coefficient for the risk score which can be written as follow: linear : r(x) = \\exp(x \\cdot \\omega) square : r(x) = \\exp( \\text{risk_parameter} *(x \\cdot \\omega)^2) gaussian : r(x) = \\exp \\left( e^{-(x \\cdot \\omega)^2*\\text{risk_parameter}} \\right) generate_data - Generating a dataset of simulated survival times from a given distribution through the hazard function using the Cox model generate_data(num_samples = 100, num_features = 3, feature_weights=None) Parameters: num_samples : int (default=100) -- Number of samples to generate num_features : int (default=3) -- Number of features to generate feature_weights : array-like (default=None) -- list of the coefficients of the underlying Cox-Model. The features linked to each coefficient are generated from random distribution from the following list: binomial chisquare exponential gamma normal uniform laplace If None then feature_weights = [1.]*num_features Returns: dataset : pandas.DataFrame -- dataset of simulated survival times, event status and features predict_hazard - Predicts the hazard function h(t, x) predict_hazard(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then it returns the function for all available t. Returns: hazard : numpy.ndarray -- array-like representing the prediction of the hazard function predict_risk - Predicts the risk score r(x) predict_risk(x) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it Returns: risk_score : numpy.ndarray -- array-like representing the prediction of the risk score predict_survival - Predicts the survival function S(t, x) predict_survival(x, t = None) Parameters: x : array-like -- input samples; where the rows correspond to an individual sample and the columns represent the features (shape=[n_samples, n_features]) . x should not be standardized before, the model will take care of it t : double (default=None) -- time at which the prediction should be performed. If None, then return the function for all available t. Returns: survival : numpy.ndarray -- array-like representing the prediction of the survival function","title":"API"},{"location":"models/simulations.html#example","text":"Let's now see how to generate a dataset designed for survival analysis. import pandas as pd from pysurvival.models.simulations import SimulationModel % pylab inline # Initializing the simulation model sim = SimulationModel( survival_distribution = 'gompertz' , risk_type = 'linear' , censored_parameter = 5.0 , alpha = 0.01 , beta = 5. , ) # Generating N Random samples N = 1000 dataset = sim . generate_data(num_samples = N, num_features =5 ) # Showing a few data-points dataset . head( 2 ) We can now see an overview of the data: x_1 x_2 x_3 x_4 x_5 time event 3.956896 124.0 0.018274 57.480199 -5.42258 0.024329 1.0 4.106100 117.0 0.111276 51.770875 4.105588 0.175530 1.0 PySurvival also displays the Base Survival function of the Simulation model: from pysurvival.utils.display import display_baseline_simulations display_baseline_simulations(sim, figure_size = ( 20 , 6 )) Figure 1 - Base Survival function of the Simulation model","title":"Example"},{"location":"models/simulations_theory.html","text":"h1, h2, h3 { color: #04A9F4; } Generating random survival times Simulation studies represent an important statistical tool to investigate the performance, properties and adequacy of statistical models. Here, we will see how to generate random survival times based on the most commonly used distributions: Exponential Weibull Gompertz Log-Logistic Lognormal Distribution function of the Cox model Thanks to the Cox proportional hazard model, it is convenient to model survival times through the hazard function, with h_0(t) the baseline function: \\begin{equation*} h(t, x_i) = h_0(t)\\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\end{equation*} The survival function of the Cox proportional hazards models given by \\begin{equation*} S(t, x_i) = \\exp \\left( - H_0(t) \\cdot \\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\end{equation*} with H_0(t) = \\int_0^t h(u) du And thus, the distribution function of the Cox model is \\begin{equation*} F(t, x_i) = 1 - \\exp \\left( - H_0(t) \\cdot \\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\end{equation*} Random survival times formula Let Y be a random variable with distribution function F , then U =F(Y ) follows a uniform distribution on the interval [0,1] , abbreviated as U \\sim Uni[0,1] . Moreover, if U \\sim Uni[0,1] , then (1-U) \\sim Uni[0,1] , too. Thus, U = \\exp \\left( - H_0(t) \\cdot \\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\sim Uni[0,1] Therefore, the survival time T of the Cox model can be expressed as \\begin{equation*} T_i = H_0^{-1} \\left[ -\\frac{\\log(U)}{\\lambda_i} \\right] \\end{equation*} with: \\lambda_i = \\alpha \\exp\\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) , U \\sim Uni[0,1] and Z \\sim Normal(0, 1) Therefore, as long as it is possible to compute H_0^{-1} , we can generate random survival times. Exponential : \\displaystyle T_i = -\\frac{\\log(U)}{\\lambda_i} Weibull : \\displaystyle T_i = \\left(-\\frac{\\log(U)}{\\lambda_i}\\right)^{1/\\beta} Gompertz : \\displaystyle T_i = \\frac{1}{\\beta} \\log\\left(1-\\beta \\frac{\\log(U)}{\\lambda_i} \\right) Log-Logistic : \\displaystyle T_i = \\frac{1}{\\lambda_i}\\left( \\frac{U}{1-U} \\right)^{1/\\beta} Log-Normal : \\displaystyle T_i = \\lambda_i \\exp(\\beta Z) \\alpha and \\beta are tuning parameters. Linear and Nonlinear hazard function It is possible to use nonlinear hazard functions to generate random survival times such that: \\begin{equation*} h(t, x_i) = h_0(t)\\exp \\left( \\psi\\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\end{equation*} where \\psi is a nonlinear function. References Bender, R., Augustin, T., & Blettner, M. (2005). Generating survival times to simulate Cox proportional hazards models. Statistics in medicine, 24(11), 1713-1723.","title":"Theory"},{"location":"models/simulations_theory.html#generating-random-survival-times","text":"Simulation studies represent an important statistical tool to investigate the performance, properties and adequacy of statistical models. Here, we will see how to generate random survival times based on the most commonly used distributions: Exponential Weibull Gompertz Log-Logistic Lognormal","title":"Generating random survival times"},{"location":"models/simulations_theory.html#distribution-function-of-the-cox-model","text":"Thanks to the Cox proportional hazard model, it is convenient to model survival times through the hazard function, with h_0(t) the baseline function: \\begin{equation*} h(t, x_i) = h_0(t)\\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\end{equation*} The survival function of the Cox proportional hazards models given by \\begin{equation*} S(t, x_i) = \\exp \\left( - H_0(t) \\cdot \\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\end{equation*} with H_0(t) = \\int_0^t h(u) du And thus, the distribution function of the Cox model is \\begin{equation*} F(t, x_i) = 1 - \\exp \\left( - H_0(t) \\cdot \\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\end{equation*}","title":"Distribution function of the Cox model"},{"location":"models/simulations_theory.html#random-survival-times-formula","text":"Let Y be a random variable with distribution function F , then U =F(Y ) follows a uniform distribution on the interval [0,1] , abbreviated as U \\sim Uni[0,1] . Moreover, if U \\sim Uni[0,1] , then (1-U) \\sim Uni[0,1] , too. Thus, U = \\exp \\left( - H_0(t) \\cdot \\exp \\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\sim Uni[0,1] Therefore, the survival time T of the Cox model can be expressed as \\begin{equation*} T_i = H_0^{-1} \\left[ -\\frac{\\log(U)}{\\lambda_i} \\right] \\end{equation*} with: \\lambda_i = \\alpha \\exp\\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) , U \\sim Uni[0,1] and Z \\sim Normal(0, 1) Therefore, as long as it is possible to compute H_0^{-1} , we can generate random survival times. Exponential : \\displaystyle T_i = -\\frac{\\log(U)}{\\lambda_i} Weibull : \\displaystyle T_i = \\left(-\\frac{\\log(U)}{\\lambda_i}\\right)^{1/\\beta} Gompertz : \\displaystyle T_i = \\frac{1}{\\beta} \\log\\left(1-\\beta \\frac{\\log(U)}{\\lambda_i} \\right) Log-Logistic : \\displaystyle T_i = \\frac{1}{\\lambda_i}\\left( \\frac{U}{1-U} \\right)^{1/\\beta} Log-Normal : \\displaystyle T_i = \\lambda_i \\exp(\\beta Z) \\alpha and \\beta are tuning parameters.","title":"Random survival times formula"},{"location":"models/simulations_theory.html#linear-and-nonlinear-hazard-function","text":"It is possible to use nonlinear hazard functions to generate random survival times such that: \\begin{equation*} h(t, x_i) = h_0(t)\\exp \\left( \\psi\\left( \\vec{x_i} \\cdot \\vec{\\omega} \\right) \\right) \\end{equation*} where \\psi is a nonlinear function.","title":"Linear and Nonlinear hazard function"},{"location":"models/simulations_theory.html#references","text":"Bender, R., Augustin, T., & Blettner, M. (2005). Generating survival times to simulate Cox proportional hazards models. Statistics in medicine, 24(11), 1713-1723.","title":"References"},{"location":"models/smooth_kaplan_meier.html","text":"h1, h2, h3 { color: #04A9F4; } Smooth Kaplan Meier model Smooth Kaplan Meier model is computed by using kernel smoothing to obtained a smooth estimator. Instance To create an instance, use pysurvival.models.non_parametric.SmoothKaplanMeierModel . Attributes cumulative_hazard : array-like -- representation of the cumulative hazard function of the model hazard : array-like -- representation of the hazard function of the model survival : array-like -- representation of the Survival function of the model times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k ) Methods __init__ - Initialize the estimator SmoothKaplanMeierModel(bandwidth=0.1, kernel='normal') Parameters: bandwidth : double (default=0.1) -- controls the degree of the smoothing. The smaller it is the closer to the original KM the function will be, but it will increase the computation time. If it is very large, the resulting model will be smoother than the estimator of KM, but it will stop being as accurate. kernel : str (default='normal') -- defines the type of kernel the model will be using. Here are the possible options: Uniform : f(x) = 0 if |x|<1 else f(x) = 0.5 Epanechnikov : f(x) = 0 if |x| \\leq 1 else f(x) = 0.75 \\cdot (1 - x^2 ) Normal : f(x) = \\exp( -x^2/2) / \\sqrt{2 \\cdot \\pi} Biweight : f(x) = 0 if |x| \\leq 1 else f(x)=(15/16) \\cdot (1-x^2)^2 Triweight : f(x) = 0 if |x| \\leq 1 else f(x)=(35/32) \\cdot (1-x^2)^3 Cosine : f(x) = 0 if |x| \\leq 1 else f(x)=(\\pi/4) \\cdot \\cos( \\pi \\cdot x/2. ) fit - Fit the estimator based on the given parameters fit(T, E, weights = None, alpha=0.05) Parameters: T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. weights : array-like (default = None) -- array of weights that are assigned to individual samples. If not provided, then each sample is given a unit weight. alpha : float (default = 0.05) -- confidence level Returns: self : object predict_density - Predicts the probability density function p(t) at a specified time t predict_density(t) Parameters: t : double -- time at which the prediction should be performed. Returns: density : double -- prediction of the probability density function at t predict_hazard - Predicts the hazard function h(t) at a specified time t predict_hazard(t) Parameters: t : double -- time at which the prediction should be performed. Returns: hazard : double -- prediction of the hazard function at t predict_survival - Predicts the survival function S(t) at a specified time t predict_survival(t) Parameters: t : double -- time at which the prediction should be performed. Returns: survival : double -- prediction of the survival function at t Example # Importing modules import numpy as np from matplotlib import pyplot as plt from pysurvival.models.non_parametric import SmoothKaplanMeierModel from pysurvival.utils.display import display_non_parametric # %matplotlib inline #Uncomment when using Jupyter # Generating random times and event indicators T = np . round(np . abs(np . random . normal( 10 , 10 , 1000 )), 1 ) E = np . random . binomial( 1 , 0.3 , 1000 ) # Initializing the SmoothKaplanMeierModel skm_model = SmoothKaplanMeierModel(bandwidth =1. , kernel = 'Cosine' ) # Fitting the model and display the survival function and confidence intervals skm_model . fit(T, E, alpha =0.95 ) # Displaying the survival function and confidence intervals display_non_parametric(skm_model) Figure 1 - Representation of the Smooth Kaplan Meier Survival function","title":"Smoothed Kaplan Meier (API)"},{"location":"models/smooth_kaplan_meier.html#smooth-kaplan-meier-model","text":"Smooth Kaplan Meier model is computed by using kernel smoothing to obtained a smooth estimator.","title":"Smooth Kaplan Meier model"},{"location":"models/smooth_kaplan_meier.html#instance","text":"To create an instance, use pysurvival.models.non_parametric.SmoothKaplanMeierModel .","title":"Instance"},{"location":"models/smooth_kaplan_meier.html#attributes","text":"cumulative_hazard : array-like -- representation of the cumulative hazard function of the model hazard : array-like -- representation of the hazard function of the model survival : array-like -- representation of the Survival function of the model times : array-like -- representation of the time axis of the model time_buckets : array-like -- representation of the time axis of the model using time bins, which are represented by [ t_{k-1}, t_k )","title":"Attributes"},{"location":"models/smooth_kaplan_meier.html#methods","text":"__init__ - Initialize the estimator SmoothKaplanMeierModel(bandwidth=0.1, kernel='normal') Parameters: bandwidth : double (default=0.1) -- controls the degree of the smoothing. The smaller it is the closer to the original KM the function will be, but it will increase the computation time. If it is very large, the resulting model will be smoother than the estimator of KM, but it will stop being as accurate. kernel : str (default='normal') -- defines the type of kernel the model will be using. Here are the possible options: Uniform : f(x) = 0 if |x|<1 else f(x) = 0.5 Epanechnikov : f(x) = 0 if |x| \\leq 1 else f(x) = 0.75 \\cdot (1 - x^2 ) Normal : f(x) = \\exp( -x^2/2) / \\sqrt{2 \\cdot \\pi} Biweight : f(x) = 0 if |x| \\leq 1 else f(x)=(15/16) \\cdot (1-x^2)^2 Triweight : f(x) = 0 if |x| \\leq 1 else f(x)=(35/32) \\cdot (1-x^2)^3 Cosine : f(x) = 0 if |x| \\leq 1 else f(x)=(\\pi/4) \\cdot \\cos( \\pi \\cdot x/2. ) fit - Fit the estimator based on the given parameters fit(T, E, weights = None, alpha=0.05) Parameters: T : array-like -- target values describing the time when the event of interest or censoring occurred. E : array-like -- values that indicate if the event of interest occurred i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, for all i. weights : array-like (default = None) -- array of weights that are assigned to individual samples. If not provided, then each sample is given a unit weight. alpha : float (default = 0.05) -- confidence level Returns: self : object predict_density - Predicts the probability density function p(t) at a specified time t predict_density(t) Parameters: t : double -- time at which the prediction should be performed. Returns: density : double -- prediction of the probability density function at t predict_hazard - Predicts the hazard function h(t) at a specified time t predict_hazard(t) Parameters: t : double -- time at which the prediction should be performed. Returns: hazard : double -- prediction of the hazard function at t predict_survival - Predicts the survival function S(t) at a specified time t predict_survival(t) Parameters: t : double -- time at which the prediction should be performed. Returns: survival : double -- prediction of the survival function at t","title":"Methods"},{"location":"models/smooth_kaplan_meier.html#example","text":"# Importing modules import numpy as np from matplotlib import pyplot as plt from pysurvival.models.non_parametric import SmoothKaplanMeierModel from pysurvival.utils.display import display_non_parametric # %matplotlib inline #Uncomment when using Jupyter # Generating random times and event indicators T = np . round(np . abs(np . random . normal( 10 , 10 , 1000 )), 1 ) E = np . random . binomial( 1 , 0.3 , 1000 ) # Initializing the SmoothKaplanMeierModel skm_model = SmoothKaplanMeierModel(bandwidth =1. , kernel = 'Cosine' ) # Fitting the model and display the survival function and confidence intervals skm_model . fit(T, E, alpha =0.95 ) # Displaying the survival function and confidence intervals display_non_parametric(skm_model) Figure 1 - Representation of the Smooth Kaplan Meier Survival function","title":"Example"},{"location":"models/survival_forest.html","text":"h1, h2, h3 { color: #04A9F4; } Survival Forest models The Ensemble models that use decision trees as its base learners can be extended to take into account censored datasets. These types of models can be regrouped under the name Survival Forest models . PySurvival contains 3 types of Survival Forest models: Random Survival Forest model ( RandomSurvivalForestModel ) Extremely Randomized (Extra) Survival Trees model ( ExtraSurvivalTreesModel ) Conditional Survival Forest model ( ConditionalSurvivalForestModel ) These models have been adapted to python from the package ranger , which is a fast implementation of random forests in C++. General algorithm Ishwaran et al. provides a general framework that can be used to describe the underlying algorithm that powers the Survival Forest models: Draw B random samples of the same size from the original dataset with replacement. The samples that are not drawn are said to be out-of-bag (OOB). Grow a survival tree on each of the b = 1, ..., B samples. a. At each node, select a random subset of predictor variables and find the best predictor and splitting value that provide two subsets (the daughter nodes) which maximizes the difference in the objective function. b. Repeat a. recursively on each daughter node until a stopping criterion is met. Calculate a cumulative hazard function (CHF) for each tree and average over all CHFs for the B trees to obtain the ensemble CHF. Compute the prediction error for the ensemble CHF using only the OOB data. All the Survival Forest models in PySurvival use this framework as the basis of the model fitting algorithm. The objective function is the main element that can differentiate then from one another. Random Survival Forest model At each node, we choose a predictor x from a subset of randomly selected predictor variables and a split value c . c is one of the unique values of x We assign each individual sample i to either the right node, if x_i \\leq c or left daughter node if x_i > c . Then we calculate the value of the log rank test such that: \\begin{equation} L(x, c) = \\frac{ \\sum^{N}_{i=1} \\left( d_{i, 1} - Y_{i,1} \\frac{d_i}{Y_i} \\right) } { \\sqrt{ \\sum^{N}_{i=1} \\frac{Y_{i,1}}{Y_i} \\left( 1 - \\frac{Y_{i,1}}{Y_i} \\right) \\left( \\frac{Y_i-d_i}{Y_i-1} \\right) d_i } } \\end{equation} with: j : Daughter node, j \\in \\{1, 2\\} d_{i,j} : Number of events at time t_i in daughter node j . Y_{i,j} : Number of units that experienced an event or are at risk at time t_i in daughter node j . d_i : Number of events at time t_i , so d_i=\\sum_j d_{i,j} Y_i : Number of units that experienced an event or at risk at time t_i , so Y_i=\\sum_j Y_{i,j} We loop through every x and c until we find x^{*} and c^{*} that satisfy |L(x^{*}, c^{*})| \\geq |L(x, c)| for every x and c . Extremely Randomized (Extra) Survival Trees model Extra Survival Trees models use the same objective function as the Random Survival Forest models. But for each predictor x , instead of using the unique values of x to find the best split value c^{*} , we use N_{splits} values drawn from a uniform distribution over the interval \\left[\\min(x), \\max(x)\\right] . Conditional Survival Forest model Conditional Survival Forest models are constructed in a way that is a bit different from Random Survival Forest models: The objective function is given by testing the null hypothesis that there is independence between the response and the predictor. To do so, for each predictor variable x , compute the logrank score test statistic and its associated p-value: Let's consider n observations (T_1, \\delta_1), ... , (T_n, \\delta_n) . We will assume the predictor x has been ordered so that x_1 \\leq x_2 \\leq ... \\leq x_n . With \\gamma_j = \\sum^n_{i=1} 1_{T_i \\leq T_j} , we compute the logrank scores a_1, ..., a_n such that : \\begin{equation} a_i = \\delta_i - \\sum^{\\gamma_i}_{j=1} \\frac{\\delta_j}{(n-\\gamma_j + 1)} \\end{equation} For a predictor x and split value c , and within the right node ( x_i \\leq c ), we can now calculate : the sum of all scores S_{n, c} = \\sum_{i=1}^n 1_{x_i \\leq c} \\cdot a_i its expectation \\text{E}\\left[ S_{n, c} \\right] = m_c \\cdot \\overline{a} with m_c=\\sum_{i=1}^n 1_{x_i \\leq c} and \\overline{a}=\\frac{1}{n}\\sum_{i=1}^n a_i its variance \\text{Var}\\left[ S_{n, c} \\right] = \\frac{m_c n_c}{n(n-1)} \\sum_{i=1}^n \\left( a_i - \\overline{a} \\right)^2 with n_c = n-m_c We can obtain the score test statistic T_{n,c} = \\frac{ S_{n, c} - \\text{E}\\left[ S_{n, c} \\right] }{ \\sqrt{\\text{Var}\\left[ S_{n, c} \\right] } } and look for c^{*} such that |T_{n, c^{*}}| \\geq |T_{n, c}| . Finally, we compute the p-value associated with T_{n, c^{*}} . At each node, only for the predictors whose associated p-value is smaller than a specified value \\alpha , the predictor with the smallest p-value is selected as splitting candidate. However, if no predictor can be used then no split is performed. References Ishwaran H, Kogalur U, Blackstone E, Lauer M. Random survival forests. The Annals of Applied Statistics. 2008; 2(3):841\u2013860. ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R Weathers, Brandon and Cutler, Richard Dr., \"Comparison of Survival Curves Between Cox Proportional Hazards, Random Forests, and Conditional Inference Forests in Survival Analysis\" (2017). All Graduate Plan B and other Reports. 927. Wright, Marvin N., Theresa Dankowski and Andreas Ziegler. \"Random forests for survival analysis using maximally selected rank statistics.\"\" Statistics in medicine 36 8 (2017): 1272-1284. Geurts, Pierre & Ernst, Damien & Wehenkel, Louis. (2006). Extremely Randomized Trees. Machine Learning. 63. 3-42. 10.1007/s10994-006-6226-1.","title":"Theory"},{"location":"models/survival_forest.html#survival-forest-models","text":"The Ensemble models that use decision trees as its base learners can be extended to take into account censored datasets. These types of models can be regrouped under the name Survival Forest models . PySurvival contains 3 types of Survival Forest models: Random Survival Forest model ( RandomSurvivalForestModel ) Extremely Randomized (Extra) Survival Trees model ( ExtraSurvivalTreesModel ) Conditional Survival Forest model ( ConditionalSurvivalForestModel ) These models have been adapted to python from the package ranger , which is a fast implementation of random forests in C++.","title":"Survival Forest models"},{"location":"models/survival_forest.html#general-algorithm","text":"Ishwaran et al. provides a general framework that can be used to describe the underlying algorithm that powers the Survival Forest models: Draw B random samples of the same size from the original dataset with replacement. The samples that are not drawn are said to be out-of-bag (OOB). Grow a survival tree on each of the b = 1, ..., B samples. a. At each node, select a random subset of predictor variables and find the best predictor and splitting value that provide two subsets (the daughter nodes) which maximizes the difference in the objective function. b. Repeat a. recursively on each daughter node until a stopping criterion is met. Calculate a cumulative hazard function (CHF) for each tree and average over all CHFs for the B trees to obtain the ensemble CHF. Compute the prediction error for the ensemble CHF using only the OOB data. All the Survival Forest models in PySurvival use this framework as the basis of the model fitting algorithm. The objective function is the main element that can differentiate then from one another.","title":"General algorithm"},{"location":"models/survival_forest.html#random-survival-forest-model","text":"At each node, we choose a predictor x from a subset of randomly selected predictor variables and a split value c . c is one of the unique values of x We assign each individual sample i to either the right node, if x_i \\leq c or left daughter node if x_i > c . Then we calculate the value of the log rank test such that: \\begin{equation} L(x, c) = \\frac{ \\sum^{N}_{i=1} \\left( d_{i, 1} - Y_{i,1} \\frac{d_i}{Y_i} \\right) } { \\sqrt{ \\sum^{N}_{i=1} \\frac{Y_{i,1}}{Y_i} \\left( 1 - \\frac{Y_{i,1}}{Y_i} \\right) \\left( \\frac{Y_i-d_i}{Y_i-1} \\right) d_i } } \\end{equation} with: j : Daughter node, j \\in \\{1, 2\\} d_{i,j} : Number of events at time t_i in daughter node j . Y_{i,j} : Number of units that experienced an event or are at risk at time t_i in daughter node j . d_i : Number of events at time t_i , so d_i=\\sum_j d_{i,j} Y_i : Number of units that experienced an event or at risk at time t_i , so Y_i=\\sum_j Y_{i,j} We loop through every x and c until we find x^{*} and c^{*} that satisfy |L(x^{*}, c^{*})| \\geq |L(x, c)| for every x and c .","title":"Random Survival Forest model"},{"location":"models/survival_forest.html#extremely-randomized-extra-survival-trees-model","text":"Extra Survival Trees models use the same objective function as the Random Survival Forest models. But for each predictor x , instead of using the unique values of x to find the best split value c^{*} , we use N_{splits} values drawn from a uniform distribution over the interval \\left[\\min(x), \\max(x)\\right] .","title":"Extremely Randomized (Extra) Survival Trees model"},{"location":"models/survival_forest.html#conditional-survival-forest-model","text":"Conditional Survival Forest models are constructed in a way that is a bit different from Random Survival Forest models: The objective function is given by testing the null hypothesis that there is independence between the response and the predictor. To do so, for each predictor variable x , compute the logrank score test statistic and its associated p-value: Let's consider n observations (T_1, \\delta_1), ... , (T_n, \\delta_n) . We will assume the predictor x has been ordered so that x_1 \\leq x_2 \\leq ... \\leq x_n . With \\gamma_j = \\sum^n_{i=1} 1_{T_i \\leq T_j} , we compute the logrank scores a_1, ..., a_n such that : \\begin{equation} a_i = \\delta_i - \\sum^{\\gamma_i}_{j=1} \\frac{\\delta_j}{(n-\\gamma_j + 1)} \\end{equation} For a predictor x and split value c , and within the right node ( x_i \\leq c ), we can now calculate : the sum of all scores S_{n, c} = \\sum_{i=1}^n 1_{x_i \\leq c} \\cdot a_i its expectation \\text{E}\\left[ S_{n, c} \\right] = m_c \\cdot \\overline{a} with m_c=\\sum_{i=1}^n 1_{x_i \\leq c} and \\overline{a}=\\frac{1}{n}\\sum_{i=1}^n a_i its variance \\text{Var}\\left[ S_{n, c} \\right] = \\frac{m_c n_c}{n(n-1)} \\sum_{i=1}^n \\left( a_i - \\overline{a} \\right)^2 with n_c = n-m_c We can obtain the score test statistic T_{n,c} = \\frac{ S_{n, c} - \\text{E}\\left[ S_{n, c} \\right] }{ \\sqrt{\\text{Var}\\left[ S_{n, c} \\right] } } and look for c^{*} such that |T_{n, c^{*}}| \\geq |T_{n, c}| . Finally, we compute the p-value associated with T_{n, c^{*}} . At each node, only for the predictors whose associated p-value is smaller than a specified value \\alpha , the predictor with the smallest p-value is selected as splitting candidate. However, if no predictor can be used then no split is performed.","title":"Conditional Survival Forest model"},{"location":"models/survival_forest.html#references","text":"Ishwaran H, Kogalur U, Blackstone E, Lauer M. Random survival forests. The Annals of Applied Statistics. 2008; 2(3):841\u2013860. ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R Weathers, Brandon and Cutler, Richard Dr., \"Comparison of Survival Curves Between Cox Proportional Hazards, Random Forests, and Conditional Inference Forests in Survival Analysis\" (2017). All Graduate Plan B and other Reports. 927. Wright, Marvin N., Theresa Dankowski and Andreas Ziegler. \"Random forests for survival analysis using maximally selected rank statistics.\"\" Statistics in medicine 36 8 (2017): 1272-1284. Geurts, Pierre & Ernst, Damien & Wehenkel, Louis. (2006). Extremely Randomized Trees. Machine Learning. 63. 3-42. 10.1007/s10994-006-6226-1.","title":"References"},{"location":"models/survival_svm.html","text":"h1, h2, h3 { color: #04A9F4; } Survival SVM model Instead of modeling the probability that an event will occur, we could look at Survival Analysis as a Ranking Problem. Indeed, the idea behind formulating the survival problem as a ranking problem is that in some applications, like clinical applications, one is only interested in defining risks groups, and not the prediction of the survival time, but whether the unit has a high or low risk of experiencing the event. Van Belle et al. developed the Rank Support Vector Machines (RankSVMs) and Polsterl et al. designed a straightforward algorithm to efficiently use the primal formulation, by computing a convex quadratic loss function, so that we can use the Newton optimization to minimize it, for a linear approach and a nonlinear/kernel based approach . Linear approach The objective function of ranking-based linear survival support vector machine is defined as: \\begin{equation} L= \\frac{1}{2} || \\vec{\\omega} ||^2 + \\frac{\\gamma}{2} \\sum_{i, j \\in P} \\max\\left(0, 1 - \\vec{\\omega}^T \\cdot \\left( \\vec{x_i}- \\vec{x_j} \\right) \\right)^2 \\end{equation} with P = \\{ (i, j) | T_i > T_j \\text{ and } \\delta_j = 1 \\} and p_0=|P| The objective function, gradient and Hessian can be expressed in matrix form as: \\begin{equation} \\begin{split} L(\\omega) & = \\frac{1}{2} \\vec{\\omega} \\cdot \\vec{\\omega}^T + \\frac{\\gamma}{2}\\left( p_0 + \\vec{\\omega}^T \\cdot X^T \\left(A \\cdot X \\cdot \\vec{\\omega} - 2\\vec{a}\\right) \\right) \\\\ \\frac{\\partial L}{\\partial \\omega} & = \\vec{\\omega} + \\gamma X^T \\cdot \\left( A \\cdot X \\cdot \\vec{\\omega} - \\vec{a} \\right) \\\\ \\frac{\\partial^2 L}{\\partial \\omega \\partial \\omega^T} & = I + \\gamma X^T \\cdot A \\cdot X \\\\ \\end{split} \\end{equation} with: \\gamma , the L2 regularization parameter I , the identity matrix [ \\vec{a} ]_i = l^{-}_{i} - l^{+}_{i} \\left[ A \\right]_{i,i} = l^{-}_{i} + l^{+}_{i} if i=j ; \\left[ A \\right]_{i,j} = -1 if i\\neq j and j \\in \\text{SV}_{i} ; and \\left[ A \\right]_{i,j} = 0 otherwise. \\text{SV}_{i}^{+} = \\{ s | T_s > T_i \\text{ and } \\omega^T x_s < \\omega^T x_i + 1 \\text{ and } \\delta_i = 1 \\} and l^{+}_{i} = |\\text{SV}_{i}^{+}| \\text{SV}_{i}^{+} = \\{ s | T_s < T_i \\text{ and } \\omega^T x_s > \\omega^T x_i - 1 \\text{ and } \\delta_s = 1 \\} and l^{-}_{i} = |\\text{SV}_{i}^{-}| \\text{SV}_{i} = \\text{SV}_{i}^{+} \\bigcup \\text{SV}_{i}^{-} Kernel approach It is possible to model non-linearities and interactions within the covariates by using kernel-based methods. \\begin{equation} L = \\frac{1}{2}||\\phi||^2 + \\frac{\\gamma}{2}\\sum_{i, j \\in P} \\max\\left( 0, 1 - (\\phi(x_i)-\\phi(x_j) \\right)^2 \\end{equation} The objective function, gradient and Hessian can be expressed in matrix form as: \\begin{equation} \\begin{split} L(\\beta) & = \\frac{1}{2}\\vec{\\beta} \\cdot K \\cdot \\vec{\\beta}^T+ \\frac{\\gamma}{2}\\left( p_0 + \\vec{\\beta}^T \\cdot K^T \\left(A \\cdot K \\cdot \\vec{\\beta}- 2\\vec{a}\\right) \\right) \\\\ \\frac{\\partial L}{\\partial \\beta} & = K \\cdot \\vec{\\beta} + \\gamma K \\left( A \\cdot K \\cdot \\vec{\\beta} - \\vec{a} \\right) \\\\ \\frac{\\partial^2 L}{\\partial \\beta \\partial \\beta^T} & = K + \\gamma K \\cdot A \\cdot K \\\\ \\end{split} \\end{equation} with: \\gamma , the L2 regularization parameter K is the n \\times n symmetric positive definite kernel matrix such that \\forall (i,j) \\in [\\![ 1, n ]\\!] \\times [\\![ 1, n ]\\!] , K_{i,j} = k(x_i, x_j) , with k , a kernel function and n , the number of samples. \\vec{K_s} = \\left[ k(x_s, x_1), k(x_s, x_2), ..., k(x_s, x_n)\\right] [ \\vec{a} ]_i = l^{-}_{i} - l^{+}_{i} \\left[ A \\right]_{i,i} = l^{-}_{i} + l^{+}_{i} if i=j ; \\left[ A \\right]_{i,j} = -1 if i\\neq j and j \\in \\text{SV}_{i} ; and \\left[ A \\right]_{i,j} = 0 otherwise. \\text{SV}_{i}^{+} = \\{ s | T_s > T_i \\text{ and } \\vec{K_s}^T \\cdot \\vec{\\beta} < \\vec{K_i}^T \\cdot \\vec{\\beta} + 1 \\text{ and } \\delta_i = 1 \\} and l^{+}_{i} = |\\text{SV}_{i}^{+}| \\text{SV}_{i}^{+} = \\{ s | T_s < T_i \\text{ and } \\vec{K_s}^T \\cdot \\vec{\\beta} > \\vec{K_i}^T \\cdot \\vec{\\beta} - 1 \\text{ and } \\delta_s = 1 \\} and l^{-}_{i} = |\\text{SV}_{i}^{-}| \\text{SV}_{i} = \\text{SV}_{i}^{+} \\bigcup \\text{SV}_{i}^{-} References Van Belle, Vanya, et al. \"Support vector machines for survival analysis.\" Proceedings of the Third International Conference on Computational Intelligence in Medicine and Healthcare (CIMED2007). 2007. P\u00f6lsterl, Sebastian, et al. \"Fast training of support vector machines for survival analysis.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015. Slides about \"Fast training of support vector machines for survival analysis.\" P\u00f6lsterl, Sebastian, et al. \"An Efficient Training Algorithm for Kernel Survival Support Vector Machines.\" arXiv preprint arXiv:1611.07054 (2016).","title":"Theory"},{"location":"models/survival_svm.html#survival-svm-model","text":"Instead of modeling the probability that an event will occur, we could look at Survival Analysis as a Ranking Problem. Indeed, the idea behind formulating the survival problem as a ranking problem is that in some applications, like clinical applications, one is only interested in defining risks groups, and not the prediction of the survival time, but whether the unit has a high or low risk of experiencing the event. Van Belle et al. developed the Rank Support Vector Machines (RankSVMs) and Polsterl et al. designed a straightforward algorithm to efficiently use the primal formulation, by computing a convex quadratic loss function, so that we can use the Newton optimization to minimize it, for a linear approach and a nonlinear/kernel based approach .","title":"Survival SVM model"},{"location":"models/survival_svm.html#linear-approach","text":"The objective function of ranking-based linear survival support vector machine is defined as: \\begin{equation} L= \\frac{1}{2} || \\vec{\\omega} ||^2 + \\frac{\\gamma}{2} \\sum_{i, j \\in P} \\max\\left(0, 1 - \\vec{\\omega}^T \\cdot \\left( \\vec{x_i}- \\vec{x_j} \\right) \\right)^2 \\end{equation} with P = \\{ (i, j) | T_i > T_j \\text{ and } \\delta_j = 1 \\} and p_0=|P| The objective function, gradient and Hessian can be expressed in matrix form as: \\begin{equation} \\begin{split} L(\\omega) & = \\frac{1}{2} \\vec{\\omega} \\cdot \\vec{\\omega}^T + \\frac{\\gamma}{2}\\left( p_0 + \\vec{\\omega}^T \\cdot X^T \\left(A \\cdot X \\cdot \\vec{\\omega} - 2\\vec{a}\\right) \\right) \\\\ \\frac{\\partial L}{\\partial \\omega} & = \\vec{\\omega} + \\gamma X^T \\cdot \\left( A \\cdot X \\cdot \\vec{\\omega} - \\vec{a} \\right) \\\\ \\frac{\\partial^2 L}{\\partial \\omega \\partial \\omega^T} & = I + \\gamma X^T \\cdot A \\cdot X \\\\ \\end{split} \\end{equation} with: \\gamma , the L2 regularization parameter I , the identity matrix [ \\vec{a} ]_i = l^{-}_{i} - l^{+}_{i} \\left[ A \\right]_{i,i} = l^{-}_{i} + l^{+}_{i} if i=j ; \\left[ A \\right]_{i,j} = -1 if i\\neq j and j \\in \\text{SV}_{i} ; and \\left[ A \\right]_{i,j} = 0 otherwise. \\text{SV}_{i}^{+} = \\{ s | T_s > T_i \\text{ and } \\omega^T x_s < \\omega^T x_i + 1 \\text{ and } \\delta_i = 1 \\} and l^{+}_{i} = |\\text{SV}_{i}^{+}| \\text{SV}_{i}^{+} = \\{ s | T_s < T_i \\text{ and } \\omega^T x_s > \\omega^T x_i - 1 \\text{ and } \\delta_s = 1 \\} and l^{-}_{i} = |\\text{SV}_{i}^{-}| \\text{SV}_{i} = \\text{SV}_{i}^{+} \\bigcup \\text{SV}_{i}^{-}","title":"Linear approach"},{"location":"models/survival_svm.html#kernel-approach","text":"It is possible to model non-linearities and interactions within the covariates by using kernel-based methods. \\begin{equation} L = \\frac{1}{2}||\\phi||^2 + \\frac{\\gamma}{2}\\sum_{i, j \\in P} \\max\\left( 0, 1 - (\\phi(x_i)-\\phi(x_j) \\right)^2 \\end{equation} The objective function, gradient and Hessian can be expressed in matrix form as: \\begin{equation} \\begin{split} L(\\beta) & = \\frac{1}{2}\\vec{\\beta} \\cdot K \\cdot \\vec{\\beta}^T+ \\frac{\\gamma}{2}\\left( p_0 + \\vec{\\beta}^T \\cdot K^T \\left(A \\cdot K \\cdot \\vec{\\beta}- 2\\vec{a}\\right) \\right) \\\\ \\frac{\\partial L}{\\partial \\beta} & = K \\cdot \\vec{\\beta} + \\gamma K \\left( A \\cdot K \\cdot \\vec{\\beta} - \\vec{a} \\right) \\\\ \\frac{\\partial^2 L}{\\partial \\beta \\partial \\beta^T} & = K + \\gamma K \\cdot A \\cdot K \\\\ \\end{split} \\end{equation} with: \\gamma , the L2 regularization parameter K is the n \\times n symmetric positive definite kernel matrix such that \\forall (i,j) \\in [\\![ 1, n ]\\!] \\times [\\![ 1, n ]\\!] , K_{i,j} = k(x_i, x_j) , with k , a kernel function and n , the number of samples. \\vec{K_s} = \\left[ k(x_s, x_1), k(x_s, x_2), ..., k(x_s, x_n)\\right] [ \\vec{a} ]_i = l^{-}_{i} - l^{+}_{i} \\left[ A \\right]_{i,i} = l^{-}_{i} + l^{+}_{i} if i=j ; \\left[ A \\right]_{i,j} = -1 if i\\neq j and j \\in \\text{SV}_{i} ; and \\left[ A \\right]_{i,j} = 0 otherwise. \\text{SV}_{i}^{+} = \\{ s | T_s > T_i \\text{ and } \\vec{K_s}^T \\cdot \\vec{\\beta} < \\vec{K_i}^T \\cdot \\vec{\\beta} + 1 \\text{ and } \\delta_i = 1 \\} and l^{+}_{i} = |\\text{SV}_{i}^{+}| \\text{SV}_{i}^{+} = \\{ s | T_s < T_i \\text{ and } \\vec{K_s}^T \\cdot \\vec{\\beta} > \\vec{K_i}^T \\cdot \\vec{\\beta} - 1 \\text{ and } \\delta_s = 1 \\} and l^{-}_{i} = |\\text{SV}_{i}^{-}| \\text{SV}_{i} = \\text{SV}_{i}^{+} \\bigcup \\text{SV}_{i}^{-}","title":"Kernel approach"},{"location":"models/survival_svm.html#references","text":"Van Belle, Vanya, et al. \"Support vector machines for survival analysis.\" Proceedings of the Third International Conference on Computational Intelligence in Medicine and Healthcare (CIMED2007). 2007. P\u00f6lsterl, Sebastian, et al. \"Fast training of support vector machines for survival analysis.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015. Slides about \"Fast training of support vector machines for survival analysis.\" P\u00f6lsterl, Sebastian, et al. \"An Efficient Training Algorithm for Kernel Survival Support Vector Machines.\" arXiv preprint arXiv:1611.07054 (2016).","title":"References"},{"location":"tutorials/churn.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Predicting when your customers will churn 1 - Introduction Customer churn/attrition, a.k.a the percentage of customers that stop using a company's products or services , is one of the most important metrics for a business, as it usually costs more to acquire new customers than it does to retain existing ones. Indeed, according to a study by Bain & Company , existing customers tend to buy more from a company over time, thus reducing the operating costs of the business and may refer the products they use to others. For example, in financial services, a 5% increase in customer retention produces more than a 25% increase in profit. By using Survival Analysis, not only companies can predict if customers are likely to stop doing business but also when that event might happen. 2 - Set up A software as a service (SaaS) company provides a suite of products for Small-to-Medium enterprises, such as data storage, Accounting, Travel and Expenses management as well as Payroll management. So as to help the CFO forecast the acquisition and marketing costs for the next fiscal year, the Data Science team wants to build a churn model to predict when customers are likely to stop their monthly subscription. Thus, once customers have been flagged as likely to churn within a certain time window, the company could take the necessary retention actions. 3 - Dataset 3.1 - Description and Overview The dataset the team wants to use contains the following features: Feature category Feature name Type Description Time months_active numerical Number of months since the customer started his/her subscription Event churned categorical Specifies if the customer stopped doing business with the company Products product_data_storage numerical Amount of cloud data storage purchased in Gigabytes Products product_travel_expense categorical Indicates if the customer is actively using and paying for the Travel and Expense management services or not. ( 'Active' , 'Free-Trial' , 'No' ) Products product_payroll categorical Indicates if the customer is actively using and paying for the Payroll management services or not. ( 'Active' , 'Free-Trial' , 'No' ) Products product_accounting categorical Indicates if the customer is actively using and paying for the Accounting services or not. ( 'Active' , 'Free-Trial' , 'No' ) Satisfaction csat_score numerical Customer Satisfaction Score (CSAT) is the measure of how products and services supplied by the company meet customer expectations. Satisfaction minutes_customer_support numerical Minutes the customer spent on the phone with the company customer support Marketing articles_viewed numerical Number of articles the customer viewed on the company website. Marketing smartphone_notifications_viewed numerical Number of smartphone notifications the customer viewed Marketing marketing_emails_clicked numerical Number of marketing emails the customer opened on Marketing social_media_ads_viewed numerical Number of social media ads the customer viewed Customer information company_size categorical Size of the company Customer information us_region categorical Region of the US where the customer's headquarter is located # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'churn' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 2 ) Here is an overview of the raw dataset: product_data_storage csat_score articles_viewed ... churned 1024 9 2 ... 0 2048 10 7 ... 0 3.2 - From categorical to numerical There are several categorical features that need to be encoded into one-hot vectors: product_travel_expense product_payroll product_accounting us_region company_size # Creating one-hot vectors categories = [ 'product_travel_expense' , 'product_payroll' , 'product_accounting' , 'us_region' , 'company_size' ] dataset = pd . get_dummies(raw_dataset, columns = categories, drop_first = True ) # Creating the time and event columns time_column = 'months_active' event_column = 'churned' # Extracting the features features = np . setdiff1d(dataset . columns, [time_column, event_column] ) . tolist() 4 - Exploratory Data Analysis As this tutorial is mainly designed to provide an example of how to use PySurvival, we will not do a thorough exploratory data analysis here but greatly encourage the reader to do so by checking the predictive maintenance tutorial that provides a detailed analysis. Here, we will just check if the dataset contains Null values or if it has duplicated rows. Then, we will take a look at feature correlations. 4.1 - Null values and duplicates The first thing to do is checking if the raw_dataset contains Null values and has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The raw_dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The raw_dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the dataset doesn't have any Null values or duplicates. 4.2 - Correlations Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 30 , 15 ), text_fontsize =10 ) Figure 1 - Correlations As we can see, there aren't any alarming correlations. 5 - Modeling 5.1 - Building the model So as to perform cross-validation later on and assess the performances of the model, let's split the dataset into training and testing sets. # Building training and testing sets from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( range (N), test_size = 0.35 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Conditional Survival Forest model to the training set. Note: The choice of the hyper-parameters was obtained using grid-search selection, not displayed in this tutorial. from pysurvival.models.survival_forest import ConditionalSurvivalForestModel # Fitting the model csf = ConditionalSurvivalForestModel(num_trees =200 ) csf . fit(X_train, T_train, E_train, max_features = 'sqrt' , max_depth =5 , min_node_size =20 , alpha =0.05 , minprop =0.1 ) 5.2 - Variables importance Having built a Survival Forest model allows us to compute the features importance: # Computing variables importance csf . variable_importance_table . head( 5 ) Here is the top 5 of the most important features. feature importance pct_importance csat_score 11.251287 0.176027 product_payroll_No 11.204996 0.175303 minutes_customer_support 9.167136 0.143421 product_accounting_No 7.768278 0.121535 product_payroll_Free-Trial 3.669896 0.057416 Thanks to the feature importance, we get a better understanding of what drives retention or churn. Here, the Accounting and Payroll Management products, score on the satisfaction survey as well as the amount of time spent on the phone with customer support play a primordial role. Note: The importance is the difference in prediction error between the perturbed and unperturbed error rate as depicted by Breiman et al . 6 - Cross Validation In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set: 6.1 - C-index The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(csf, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.83 6.2 - Brier Score The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score ibs = integrated_brier_score(csf, X_test, T_test, E_test, t_max =12 , figure_size = ( 15 , 5 )) print ( 'IBS: {:.2f}' . format(ibs)) Figure 2 - Conditional Survival Forest - Brier scores & Prediction error curve The IBS is equal to 0.13 on the entire model time axis. This indicates that the model will have good predictive abilities. 7 - Predictions 7.1 - Overall predictions Now that we have built a model that seems to provide great performances, let's compare the time series of the actual and predicted number of customers who stop doing business with the SaaS company, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(csf, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 3 - Conditional Survival Forest - Number of customers who churned The model provides very good results overall since on an entire 12 months window, it only makes an average absolute error of ~5 customers. 7.2 - Individual predictions Now that we know that we can provide reliable predictions for an entire cohort, let's compute the probability of remaining a customer for all times t . First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils.display , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = csf, X = X_test, use_log = False , num_bins =30 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 8.5 , 'color' : 'red' }, medium = { 'lower_bound' : 8.5 , 'upper_bound' : 12. , 'color' : 'green' }, high = { 'lower_bound' : 12. , 'upper_bound' : 25 , 'color' : 'blue' } ) Figure 4 - Conditional Survival Forest - Risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, it is possible to distinguish 3 main groups, low , medium and high risk groups. Because the C-index is high, the model will be able to properly rank the survival time of random units of each group, such that t_{high} \\leq t_{medium} \\leq t_{low} . Let's randomly select individual unit in each group and compare their probability of remaining a customer for all times t. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 5 )) # Selecting a random individual that experienced an event from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting an individual that experienced an event choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the Survival function for all times t survival = csf . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(csf . times, survival, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing Survival functions between {} risk grades\" . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 5 - Conditional Survival Forest - Predicting individual probability to remain a customer Here we can see that the model manages to provide great prediction of the event time. 8 - Conclusion We can now save our model so as to put it in production and score future customers. # Let's now save our model from pysurvival.utils import save_model save_model(csf, '/Users/xxx/Desktop/churn_csf.zip' ) In conclusion, we can see that it is possible to predict when customers will stop doing business with the company at different time points. The model will help the company be more pro-active when it comes to retaining their customers; and provide a better understanding of the reasons that drive churn. References: Churn definition from hubspot.com Bain & Company - Prescription for cutting costs Random Forests. Machine Learning, 45(1), 5-32","title":"Churn Prediction"},{"location":"tutorials/churn.html#predicting-when-your-customers-will-churn","text":"","title":"Predicting when your customers will churn"},{"location":"tutorials/churn.html#1-introduction","text":"Customer churn/attrition, a.k.a the percentage of customers that stop using a company's products or services , is one of the most important metrics for a business, as it usually costs more to acquire new customers than it does to retain existing ones. Indeed, according to a study by Bain & Company , existing customers tend to buy more from a company over time, thus reducing the operating costs of the business and may refer the products they use to others. For example, in financial services, a 5% increase in customer retention produces more than a 25% increase in profit. By using Survival Analysis, not only companies can predict if customers are likely to stop doing business but also when that event might happen.","title":"1 - Introduction"},{"location":"tutorials/churn.html#2-set-up","text":"A software as a service (SaaS) company provides a suite of products for Small-to-Medium enterprises, such as data storage, Accounting, Travel and Expenses management as well as Payroll management. So as to help the CFO forecast the acquisition and marketing costs for the next fiscal year, the Data Science team wants to build a churn model to predict when customers are likely to stop their monthly subscription. Thus, once customers have been flagged as likely to churn within a certain time window, the company could take the necessary retention actions.","title":"2 - Set up"},{"location":"tutorials/churn.html#3-dataset","text":"","title":"3 - Dataset"},{"location":"tutorials/churn.html#31-description-and-overview","text":"The dataset the team wants to use contains the following features: Feature category Feature name Type Description Time months_active numerical Number of months since the customer started his/her subscription Event churned categorical Specifies if the customer stopped doing business with the company Products product_data_storage numerical Amount of cloud data storage purchased in Gigabytes Products product_travel_expense categorical Indicates if the customer is actively using and paying for the Travel and Expense management services or not. ( 'Active' , 'Free-Trial' , 'No' ) Products product_payroll categorical Indicates if the customer is actively using and paying for the Payroll management services or not. ( 'Active' , 'Free-Trial' , 'No' ) Products product_accounting categorical Indicates if the customer is actively using and paying for the Accounting services or not. ( 'Active' , 'Free-Trial' , 'No' ) Satisfaction csat_score numerical Customer Satisfaction Score (CSAT) is the measure of how products and services supplied by the company meet customer expectations. Satisfaction minutes_customer_support numerical Minutes the customer spent on the phone with the company customer support Marketing articles_viewed numerical Number of articles the customer viewed on the company website. Marketing smartphone_notifications_viewed numerical Number of smartphone notifications the customer viewed Marketing marketing_emails_clicked numerical Number of marketing emails the customer opened on Marketing social_media_ads_viewed numerical Number of social media ads the customer viewed Customer information company_size categorical Size of the company Customer information us_region categorical Region of the US where the customer's headquarter is located # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'churn' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 2 ) Here is an overview of the raw dataset: product_data_storage csat_score articles_viewed ... churned 1024 9 2 ... 0 2048 10 7 ... 0","title":"3.1 - Description and Overview"},{"location":"tutorials/churn.html#32-from-categorical-to-numerical","text":"There are several categorical features that need to be encoded into one-hot vectors: product_travel_expense product_payroll product_accounting us_region company_size # Creating one-hot vectors categories = [ 'product_travel_expense' , 'product_payroll' , 'product_accounting' , 'us_region' , 'company_size' ] dataset = pd . get_dummies(raw_dataset, columns = categories, drop_first = True ) # Creating the time and event columns time_column = 'months_active' event_column = 'churned' # Extracting the features features = np . setdiff1d(dataset . columns, [time_column, event_column] ) . tolist()","title":"3.2 - From categorical to numerical"},{"location":"tutorials/churn.html#4-exploratory-data-analysis","text":"As this tutorial is mainly designed to provide an example of how to use PySurvival, we will not do a thorough exploratory data analysis here but greatly encourage the reader to do so by checking the predictive maintenance tutorial that provides a detailed analysis. Here, we will just check if the dataset contains Null values or if it has duplicated rows. Then, we will take a look at feature correlations.","title":"4 - Exploratory Data Analysis"},{"location":"tutorials/churn.html#41-null-values-and-duplicates","text":"The first thing to do is checking if the raw_dataset contains Null values and has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The raw_dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The raw_dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the dataset doesn't have any Null values or duplicates.","title":"4.1 - Null values and duplicates"},{"location":"tutorials/churn.html#42-correlations","text":"Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 30 , 15 ), text_fontsize =10 ) Figure 1 - Correlations As we can see, there aren't any alarming correlations.","title":"4.2 - Correlations"},{"location":"tutorials/churn.html#5-modeling","text":"","title":"5 - Modeling"},{"location":"tutorials/churn.html#51-building-the-model","text":"So as to perform cross-validation later on and assess the performances of the model, let's split the dataset into training and testing sets. # Building training and testing sets from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( range (N), test_size = 0.35 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Conditional Survival Forest model to the training set. Note: The choice of the hyper-parameters was obtained using grid-search selection, not displayed in this tutorial. from pysurvival.models.survival_forest import ConditionalSurvivalForestModel # Fitting the model csf = ConditionalSurvivalForestModel(num_trees =200 ) csf . fit(X_train, T_train, E_train, max_features = 'sqrt' , max_depth =5 , min_node_size =20 , alpha =0.05 , minprop =0.1 )","title":"5.1 - Building the model"},{"location":"tutorials/churn.html#52-variables-importance","text":"Having built a Survival Forest model allows us to compute the features importance: # Computing variables importance csf . variable_importance_table . head( 5 ) Here is the top 5 of the most important features. feature importance pct_importance csat_score 11.251287 0.176027 product_payroll_No 11.204996 0.175303 minutes_customer_support 9.167136 0.143421 product_accounting_No 7.768278 0.121535 product_payroll_Free-Trial 3.669896 0.057416 Thanks to the feature importance, we get a better understanding of what drives retention or churn. Here, the Accounting and Payroll Management products, score on the satisfaction survey as well as the amount of time spent on the phone with customer support play a primordial role. Note: The importance is the difference in prediction error between the perturbed and unperturbed error rate as depicted by Breiman et al .","title":"5.2 - Variables importance"},{"location":"tutorials/churn.html#6-cross-validation","text":"In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set:","title":"6 - Cross Validation"},{"location":"tutorials/churn.html#61-c-index","text":"The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(csf, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.83","title":"6.1 - C-index"},{"location":"tutorials/churn.html#62-brier-score","text":"The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score ibs = integrated_brier_score(csf, X_test, T_test, E_test, t_max =12 , figure_size = ( 15 , 5 )) print ( 'IBS: {:.2f}' . format(ibs)) Figure 2 - Conditional Survival Forest - Brier scores & Prediction error curve The IBS is equal to 0.13 on the entire model time axis. This indicates that the model will have good predictive abilities.","title":"6.2 - Brier Score"},{"location":"tutorials/churn.html#7-predictions","text":"","title":"7 - Predictions"},{"location":"tutorials/churn.html#71-overall-predictions","text":"Now that we have built a model that seems to provide great performances, let's compare the time series of the actual and predicted number of customers who stop doing business with the SaaS company, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(csf, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 3 - Conditional Survival Forest - Number of customers who churned The model provides very good results overall since on an entire 12 months window, it only makes an average absolute error of ~5 customers.","title":"7.1 - Overall predictions"},{"location":"tutorials/churn.html#72-individual-predictions","text":"Now that we know that we can provide reliable predictions for an entire cohort, let's compute the probability of remaining a customer for all times t . First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils.display , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = csf, X = X_test, use_log = False , num_bins =30 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 8.5 , 'color' : 'red' }, medium = { 'lower_bound' : 8.5 , 'upper_bound' : 12. , 'color' : 'green' }, high = { 'lower_bound' : 12. , 'upper_bound' : 25 , 'color' : 'blue' } ) Figure 4 - Conditional Survival Forest - Risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, it is possible to distinguish 3 main groups, low , medium and high risk groups. Because the C-index is high, the model will be able to properly rank the survival time of random units of each group, such that t_{high} \\leq t_{medium} \\leq t_{low} . Let's randomly select individual unit in each group and compare their probability of remaining a customer for all times t. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 5 )) # Selecting a random individual that experienced an event from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting an individual that experienced an event choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the Survival function for all times t survival = csf . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(csf . times, survival, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing Survival functions between {} risk grades\" . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 5 - Conditional Survival Forest - Predicting individual probability to remain a customer Here we can see that the model manages to provide great prediction of the event time.","title":"7.2 - Individual predictions"},{"location":"tutorials/churn.html#8-conclusion","text":"We can now save our model so as to put it in production and score future customers. # Let's now save our model from pysurvival.utils import save_model save_model(csf, '/Users/xxx/Desktop/churn_csf.zip' ) In conclusion, we can see that it is possible to predict when customers will stop doing business with the company at different time points. The model will help the company be more pro-active when it comes to retaining their customers; and provide a better understanding of the reasons that drive churn.","title":"8 - Conclusion"},{"location":"tutorials/churn.html#references","text":"Churn definition from hubspot.com Bain & Company - Prescription for cutting costs Random Forests. Machine Learning, 45(1), 5-32","title":"References:"},{"location":"tutorials/credit_risk.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Computing the speed of repayment of loans 1 - Introduction Credit Risk refers to the likelihood that a borrower will not be able to repay a loan contracted by a lender. Thus throughout the years, financial institutions have developed various ways to quantify that risk so as to limit their exposure. Here, instead of simply modeling whether a borrower will repay, by using Survival Analysis, it becomes possible to determine when this will happen. Indeed, it is easy to consider that fully repaying a loan is an explicit event , and therefore not having paid back the loan yet can be defined as the censored situation. By using this configuration, banks, credit unions, or fintech startups in the lending space can predict the speed of repayment of a loan. This will help these institutions mitigate losses due to bad debt, customize interest rates, improve cash flow and credit collections, and determine which customers are likely to bring in the most revenue throughout a variety of products. 2 - Set up In this tutorial, we will be using the German Credit dataset, which was originally provided by Professor Dr. Hans Hofmann of the University of Hamburg and available on the UCI Machine Learning Repository . The current version was adapted to be directly usable with a minimum amount of feature transformation.. 3 - Dataset 3.1 - Overview The dataset contains information useful to assess the borrowers creditworthiness as well as socio-demographic elements: Feature category Feature name Type Description Time duration numerical Duration in month Event full_repaid categorical Specifies if the loan was fully repaid Socio-Demographic age numerical Age of the borrower (in years) Socio-Demographic foreign_worker numerical Indicates if the borrower is a foreign worker Socio-Demographic personal_status categorical Gender and Marital status Socio-Demographic people_liable numerical Number of people being liable to provide maintenance for Socio-Demographic telephone numerical Indicates if the borrower owns a phone Employment employment_years categorical Years of employment at current job Employment job categorical Employment status Residence housing categorical Residential status of the borrower Residence present_residence numerical Years living at current residence Loan information amount numerical Amount of money borrowed Loan information installment_rate numerical Percentage of amount borrowed that will be charged by a lender to a borrower. Loan information purpose categorical Reason to get a loan Bank information checking_account_status categorical Status of the checking account Bank information credit_history categorical Credit history of the borrower Bank information number_of_credits numerical Number of existing credits at this bank Bank information other_installment_plans categorical Type of installments plans the borrower already has Bank information savings_account_status categorical Status of the saving account Collateral/Guarantor property categorical Type of valuable assets the borrower owns Collateral/Guarantor other_debtors categorical Indicate if someone else will be involved in the repayment or is guaranteeing the loan # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'credit_risk' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 3 ) Here is an overview of the raw dataset: checking_account_status duration credit_history ... foreign_worker full_repaid below_0 6 critical_account ... 1 1 0_to_200 48 existing_credit_paid ... 1 0 no_account 12 critical_account ... 1 1 3.2 - From categorical to numerical There are several categorical features that need to encoded into one-hot vectors: # From category to numerical category_columns = [ 'checking_account_status' , 'credit_history' , 'purpose' , 'savings_account_status' , 'employment_years' , 'personal_status' , 'other_debtors' , 'property' , 'other_installment_plans' , 'housing' , 'job' ] dataset = pd . get_dummies(raw_dataset, columns = category_columns, drop_first = True ) # Creating the time and event columns time_column = 'duration' event_column = 'full_repaid' # Creating the features features = np . setdiff1d(dataset . columns, [time_column, event_column] ) . tolist() 4 - Exploratory Data Analysis As this tutorial is mainly designed to provide an example of how to use PySurvival, we will not do a thorough exploratory data analysis here but greatly encourage the reader to do so by checking the predictive maintenance tutorial that provides a detailed analysis. Here, we will just check if the dataset contains Null values or if it has duplicated rows. Then, we will take a look at feature correlations. 4.1 - Null values and duplicates The first thing to do is checking if the raw_dataset contains Null values and has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The raw_dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The raw_dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the raw_dataset doesn't have any Null values or duplicates. 4.2 - Correlations Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 40 , 15 ), text_fontsize =7 ) Figure 1 - Correlations Based on the correlations chart, we should remove the following features credit_history_existing_credit_paid housing_own to_remove = [ 'credit_history_existing_credit_paid' , 'housing_own' ] features = np . setdiff1d(features, to_remove) . tolist() 5 - Modeling So as to perform cross-validation later on and assess the performances of the model, let's split the dataset into training and testing sets. # Building training and testing sets from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( range (N), test_size = 0.4 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Neural MTLR model to the training set. Note: The choice of the structure of the neural network was obtained using grid-search hyperparameters selection, not displayed in this tutorial. from pysurvival.models.multi_task import NeuralMultiTaskModel # Initializing the Neural MTLR with a time axis split into 100 intervals structure = [ { 'activation' : 'ReLU' , 'num_units' : 60 }, { 'activation' : 'Swish' , 'num_units' : 60 }, ] neural_mtlr = NeuralMultiTaskModel(bins =100 , structure = structure) # Fitting the model neural_mtlr . fit(X_train, T_train, E_train, init_method = 'orthogonal' , optimizer = 'rprop' , lr = 1e-4 , l2_reg = 1e-1 , l2_smooth = 1e-1 , batch_normalization = True , bn_and_dropout = True , dropout =0.5 , num_epochs = 500 ) We can take a look at the values of N-MTLR loss function to ensure that the fitting isn't incomplete from pysurvival.utils.display import display_loss_values display_loss_values(neural_mtlr) Figure 2 - Neural MTLR loss function values 6 - Cross Validation In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set: 6.1 - C-index The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(neural_mtlr, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.76 6.2 - Brier Score The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score integrated_brier_score(neural_mtlr, X_test, T_test, E_test, t_max =100 , figure_size = ( 20 , 6.5 ) ) #0.08 Figure 3 - Neural MTLR loss function values The IBS is equal to 0.08 on the entire model time axis. This indicates that the model will have good predictive abilities. 7 - Predictions 7.1 - Overall predictions Now that we have built a model that seems to provide great performances, let's compare the following: the time series of the actual and predicted number of loans that were fully repaid, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 4 - Actual vs Predicted - Number of loans that were fully repaid the time series of the actual and predicted number of loans that were still active, for each time t. results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = True , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 5 - Actual vs Predicted - Number of loans that were still active Both comparisons show that the model do a great job predicting the number of loans that were fully repaid ( average absolute error of 4.5 loan ) or that were still active ( average absolute error of 18.4 loans ) for all times t of the 70+ months time window. 7.2 - Individual predictions Now that we know that we can provide reliable predictions for an entire cohort, let's compute the speed of repayment at the individual level. The speed of repayment is given by \\text{Speed}(t) = 1 - \\text{Survival}(t) First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = neural_mtlr, X = X_test, use_log = True , num_bins =30 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 1.64 , 'color' : 'red' }, medium = { 'lower_bound' : 1.64 , 'upper_bound' : 1.8 , 'color' : 'green' }, high = { 'lower_bound' : 1.8 , 'upper_bound' : 3 , 'color' : 'blue' } ) Figure 6 - Creating risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, it is possible to distinguish 3 main groups, low , medium and high risk groups. Because the C-index is high, the model will be able to rank the survival times of a random unit of each group, such that t_{high} \\leq t_{medium} \\leq t_{low} . Let's randomly select individual unit in each group and compare their speed of repayment functions. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 5 )) # Selecting a random individual that experienced an event from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting an individual that experienced an event choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the CDF for all times t cdf = 1. - neural_mtlr . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(neural_mtlr . times, cdf, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing cumulative density functions between {} risk grades\" title = title . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . xlim( 0 , 65 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 7 - Predicting individual speed of repayment functions As we can see, the model manages to perfectly predict the event time, here it corresponds to a sudden increase in the individual speed of repayment function. 8 - Conclusion We can now save our model so as to put it in production and score future borrowers. # Let's now save our model from pysurvival.utils import save_model save_model(neural_mtlr, '/Users/xxx/Desktop/credit_neural_mtlr.zip' ) Thanks to Survival Analysis, we can see that it is indeed possible to predict the speed of repayment of loans and forecast the number of loans that will be fully repaid throughout time, which is a great advantage over classification modeling. References German Credit dataset - UCI Machine Learning Repository","title":"Credit Risk"},{"location":"tutorials/credit_risk.html#computing-the-speed-of-repayment-of-loans","text":"","title":"Computing the speed of repayment of loans"},{"location":"tutorials/credit_risk.html#1-introduction","text":"Credit Risk refers to the likelihood that a borrower will not be able to repay a loan contracted by a lender. Thus throughout the years, financial institutions have developed various ways to quantify that risk so as to limit their exposure. Here, instead of simply modeling whether a borrower will repay, by using Survival Analysis, it becomes possible to determine when this will happen. Indeed, it is easy to consider that fully repaying a loan is an explicit event , and therefore not having paid back the loan yet can be defined as the censored situation. By using this configuration, banks, credit unions, or fintech startups in the lending space can predict the speed of repayment of a loan. This will help these institutions mitigate losses due to bad debt, customize interest rates, improve cash flow and credit collections, and determine which customers are likely to bring in the most revenue throughout a variety of products.","title":"1 - Introduction"},{"location":"tutorials/credit_risk.html#2-set-up","text":"In this tutorial, we will be using the German Credit dataset, which was originally provided by Professor Dr. Hans Hofmann of the University of Hamburg and available on the UCI Machine Learning Repository . The current version was adapted to be directly usable with a minimum amount of feature transformation..","title":"2 - Set up"},{"location":"tutorials/credit_risk.html#3-dataset","text":"","title":"3 - Dataset"},{"location":"tutorials/credit_risk.html#31-overview","text":"The dataset contains information useful to assess the borrowers creditworthiness as well as socio-demographic elements: Feature category Feature name Type Description Time duration numerical Duration in month Event full_repaid categorical Specifies if the loan was fully repaid Socio-Demographic age numerical Age of the borrower (in years) Socio-Demographic foreign_worker numerical Indicates if the borrower is a foreign worker Socio-Demographic personal_status categorical Gender and Marital status Socio-Demographic people_liable numerical Number of people being liable to provide maintenance for Socio-Demographic telephone numerical Indicates if the borrower owns a phone Employment employment_years categorical Years of employment at current job Employment job categorical Employment status Residence housing categorical Residential status of the borrower Residence present_residence numerical Years living at current residence Loan information amount numerical Amount of money borrowed Loan information installment_rate numerical Percentage of amount borrowed that will be charged by a lender to a borrower. Loan information purpose categorical Reason to get a loan Bank information checking_account_status categorical Status of the checking account Bank information credit_history categorical Credit history of the borrower Bank information number_of_credits numerical Number of existing credits at this bank Bank information other_installment_plans categorical Type of installments plans the borrower already has Bank information savings_account_status categorical Status of the saving account Collateral/Guarantor property categorical Type of valuable assets the borrower owns Collateral/Guarantor other_debtors categorical Indicate if someone else will be involved in the repayment or is guaranteeing the loan # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'credit_risk' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 3 ) Here is an overview of the raw dataset: checking_account_status duration credit_history ... foreign_worker full_repaid below_0 6 critical_account ... 1 1 0_to_200 48 existing_credit_paid ... 1 0 no_account 12 critical_account ... 1 1","title":"3.1 - Overview"},{"location":"tutorials/credit_risk.html#32-from-categorical-to-numerical","text":"There are several categorical features that need to encoded into one-hot vectors: # From category to numerical category_columns = [ 'checking_account_status' , 'credit_history' , 'purpose' , 'savings_account_status' , 'employment_years' , 'personal_status' , 'other_debtors' , 'property' , 'other_installment_plans' , 'housing' , 'job' ] dataset = pd . get_dummies(raw_dataset, columns = category_columns, drop_first = True ) # Creating the time and event columns time_column = 'duration' event_column = 'full_repaid' # Creating the features features = np . setdiff1d(dataset . columns, [time_column, event_column] ) . tolist()","title":"3.2 - From categorical to numerical"},{"location":"tutorials/credit_risk.html#4-exploratory-data-analysis","text":"As this tutorial is mainly designed to provide an example of how to use PySurvival, we will not do a thorough exploratory data analysis here but greatly encourage the reader to do so by checking the predictive maintenance tutorial that provides a detailed analysis. Here, we will just check if the dataset contains Null values or if it has duplicated rows. Then, we will take a look at feature correlations.","title":"4 - Exploratory Data Analysis"},{"location":"tutorials/credit_risk.html#41-null-values-and-duplicates","text":"The first thing to do is checking if the raw_dataset contains Null values and has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The raw_dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The raw_dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the raw_dataset doesn't have any Null values or duplicates.","title":"4.1 - Null values and duplicates"},{"location":"tutorials/credit_risk.html#42-correlations","text":"Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 40 , 15 ), text_fontsize =7 ) Figure 1 - Correlations Based on the correlations chart, we should remove the following features credit_history_existing_credit_paid housing_own to_remove = [ 'credit_history_existing_credit_paid' , 'housing_own' ] features = np . setdiff1d(features, to_remove) . tolist()","title":"4.2 - Correlations"},{"location":"tutorials/credit_risk.html#5-modeling","text":"So as to perform cross-validation later on and assess the performances of the model, let's split the dataset into training and testing sets. # Building training and testing sets from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( range (N), test_size = 0.4 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Neural MTLR model to the training set. Note: The choice of the structure of the neural network was obtained using grid-search hyperparameters selection, not displayed in this tutorial. from pysurvival.models.multi_task import NeuralMultiTaskModel # Initializing the Neural MTLR with a time axis split into 100 intervals structure = [ { 'activation' : 'ReLU' , 'num_units' : 60 }, { 'activation' : 'Swish' , 'num_units' : 60 }, ] neural_mtlr = NeuralMultiTaskModel(bins =100 , structure = structure) # Fitting the model neural_mtlr . fit(X_train, T_train, E_train, init_method = 'orthogonal' , optimizer = 'rprop' , lr = 1e-4 , l2_reg = 1e-1 , l2_smooth = 1e-1 , batch_normalization = True , bn_and_dropout = True , dropout =0.5 , num_epochs = 500 ) We can take a look at the values of N-MTLR loss function to ensure that the fitting isn't incomplete from pysurvival.utils.display import display_loss_values display_loss_values(neural_mtlr) Figure 2 - Neural MTLR loss function values","title":"5 - Modeling"},{"location":"tutorials/credit_risk.html#6-cross-validation","text":"In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set:","title":"6 - Cross Validation"},{"location":"tutorials/credit_risk.html#61-c-index","text":"The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(neural_mtlr, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.76","title":"6.1 - C-index"},{"location":"tutorials/credit_risk.html#62-brier-score","text":"The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score integrated_brier_score(neural_mtlr, X_test, T_test, E_test, t_max =100 , figure_size = ( 20 , 6.5 ) ) #0.08 Figure 3 - Neural MTLR loss function values The IBS is equal to 0.08 on the entire model time axis. This indicates that the model will have good predictive abilities.","title":"6.2 - Brier Score"},{"location":"tutorials/credit_risk.html#7-predictions","text":"","title":"7 - Predictions"},{"location":"tutorials/credit_risk.html#71-overall-predictions","text":"Now that we have built a model that seems to provide great performances, let's compare the following: the time series of the actual and predicted number of loans that were fully repaid, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 4 - Actual vs Predicted - Number of loans that were fully repaid the time series of the actual and predicted number of loans that were still active, for each time t. results = compare_to_actual(neural_mtlr, X_test, T_test, E_test, is_at_risk = True , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 5 - Actual vs Predicted - Number of loans that were still active Both comparisons show that the model do a great job predicting the number of loans that were fully repaid ( average absolute error of 4.5 loan ) or that were still active ( average absolute error of 18.4 loans ) for all times t of the 70+ months time window.","title":"7.1 - Overall predictions"},{"location":"tutorials/credit_risk.html#72-individual-predictions","text":"Now that we know that we can provide reliable predictions for an entire cohort, let's compute the speed of repayment at the individual level. The speed of repayment is given by \\text{Speed}(t) = 1 - \\text{Survival}(t) First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = neural_mtlr, X = X_test, use_log = True , num_bins =30 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 1.64 , 'color' : 'red' }, medium = { 'lower_bound' : 1.64 , 'upper_bound' : 1.8 , 'color' : 'green' }, high = { 'lower_bound' : 1.8 , 'upper_bound' : 3 , 'color' : 'blue' } ) Figure 6 - Creating risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, it is possible to distinguish 3 main groups, low , medium and high risk groups. Because the C-index is high, the model will be able to rank the survival times of a random unit of each group, such that t_{high} \\leq t_{medium} \\leq t_{low} . Let's randomly select individual unit in each group and compare their speed of repayment functions. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 5 )) # Selecting a random individual that experienced an event from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting an individual that experienced an event choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the CDF for all times t cdf = 1. - neural_mtlr . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(neural_mtlr . times, cdf, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing cumulative density functions between {} risk grades\" title = title . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . xlim( 0 , 65 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 7 - Predicting individual speed of repayment functions As we can see, the model manages to perfectly predict the event time, here it corresponds to a sudden increase in the individual speed of repayment function.","title":"7.2 - Individual predictions"},{"location":"tutorials/credit_risk.html#8-conclusion","text":"We can now save our model so as to put it in production and score future borrowers. # Let's now save our model from pysurvival.utils import save_model save_model(neural_mtlr, '/Users/xxx/Desktop/credit_neural_mtlr.zip' ) Thanks to Survival Analysis, we can see that it is indeed possible to predict the speed of repayment of loans and forecast the number of loans that will be fully repaid throughout time, which is a great advantage over classification modeling.","title":"8 - Conclusion"},{"location":"tutorials/credit_risk.html#references","text":"German Credit dataset - UCI Machine Learning Repository","title":"References"},{"location":"tutorials/employee_retention.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Knowing when your employees will quit 1 - Introduction Employees attrition can be very costly for companies: reports show that it costs employers 33% of an employee's annual salary to hire a replacement if that worker leaves. Moreover, it can jeopardize productivity, cause loss of knowledge and curb staff morale. Thus, providing solutions that could predict employee turnover could be greatly beneficial for companies. Furthermore, by using Survival Analysis and taking into account the time dimension, predicting when an employee will quit becomes possible. 2 - Dataset In this tutorial, we will use the human resources dataset Employee Attrition dataset to demonstrate the usefulness of Survival Analysis. 2.1 - Overview Here, we will consider the following features: Feature category Feature name Type Description Time time_spend_company numerical Time spent at the company Event left categorical Specifies if the employee left the company Evaluation/Scoring satisfaction numerical Employee satisfaction level Evaluation/Scoring last_evaluation numerical Last evaluation score Day-to-Day activities number_projects numerical Number of projects assigned to the employee Day-to-Day activities average_monthly_hour numerical Average monthly hours worked Day-to-Day activities work_accident numerical Whether the employee has had a work accident Department department categorical Department name/Specialized functional area within the company Salary salary categorical Salary category # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'employee_attrition' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 3 ) Here is an overview of the raw dataset: satisfaction_level last_evaluation number_projects ... department salary 0.38 0.53 2 ... sales low 0.80 0.86 5 ... sales medium 0.11 0.88 7 ... sales medium 2.2 - From categorical to numerical Let's encode the categorical features into one-hot vectors and define the modeling features: # Creating the time and event columns time_column = 'time_spend_company' event_column = 'left' # Creating one-hot vectors category_columns = [ 'department' , 'salary' ] dataset = pd . get_dummies(raw_dataset, columns = category_columns, drop_first = True ) dataset . head() # Creating the features features = np . setdiff1d(dataset . columns, [time_column, event_column] ) . tolist() 3 - Exploratory Data Analysis As this tutorial is mainly designed to provide an example of how to use PySurvival, we will not do a thorough exploratory data analysis here but greatly encourage the reader to do so by checking the predictive maintenance tutorial that provides a detailed analysis. Here, we will just check if the dataset contains Null values or if it has duplicated rows. Then, we will take a look at feature correlations. 3.1 - Null values and duplicates The first thing to do is checking if the dataset contains Null values and has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the dataset doesn't have any Null values but had 3,008 duplicated rows, that we removed. 3.2 - Correlations Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 20 , 10 ), text_fontsize =10 ) Figure 1 - Correlations This shows that there is a pretty big correlation between the features salaray_low and salary_medium . So we will be removing salaray_low . del dataset[ 'salary_low' ] features = np . setdiff1d( dataset . columns, [time_column, event_column] ) . tolist() 4 - Modeling As there are ~15,000 rows, we will first downsample the dataset to speed up computations, in case the computer that you are using cannot handle that size. Then, so as to perform cross-validation later on and assess the performance of the model, we will split the dataset into training and testing sets. # Downsampling the dataset to speed up computations indexes_choices = np . random . choice(N, int (N *0.3 ), replace = False ) . tolist() # Building training and testing sets # from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( indexes_choices, test_size = 0.4 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Conditional Survival Forest model (CSF) to the training set. Note: The choice of the model and hyperparameters was obtained using grid-search selection, not displayed in this tutorial. from pysurvival.models.survival_forest import ConditionalSurvivalForestModel # Fitting the model csf = ConditionalSurvivalForestModel(num_trees =200 ) csf . fit(X_train, T_train, E_train, max_features = 'sqrt' , alpha =0.05 , minprop =0.1 , max_depth =5 , min_node_size =30 ) 5 - Cross Validation In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set: 5.1 - C-index The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(csf, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.89 5.2 - Brier Score The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score ibs = integrated_brier_score(csf, X_test, T_test, E_test, t_max =12 , figure_size = ( 15 , 5 )) print ( 'IBS: {:.2f}' . format(ibs)) Figure 2 - Conditional Survival Forest - Brier score & Prediction error curve The IBS is equal to 0.12 on the entire model time axis. This indicates that the model has good predictive abilities. 6 - Predictions 6.1 - Overall predictions Now that we have built a model that seems to provide great performances, let's compare the time series of the actual and predicted number of employees who left the company, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(csf, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 3 - Actual vs Predicted - Number of employees who left the company 6.2 - Individual predictions Now that we know that we can provide reliable predictions for an entire cohort, let's compute the probability of remaining an employee for all times t. First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils.display , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = csf, X = X_test, use_log = False , num_bins =50 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 3.5 , 'color' : 'red' }, high = { 'lower_bound' : 3.5 , 'upper_bound' : 10 , 'color' : 'blue' } ) Figure 4 - Creating risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, it is possible to distinguish 2 main groups, low and high risk groups. Because the C-index is high, the model will be able to perfectly rank the survival times of a random unit of each group, such that t_{high} \\leq t_{low} . Let's randomly select individual unit in each group and compare their speed of repayment functions. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 8 )) # Selecting a random individual that experienced failure from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting an individual that experienced an event choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the Survival function for all times t survival = csf . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(csf . times, survival, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing Survival functions between {} risk grades\" . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 5 - Predicting individual probability to remain in the company 7 - Conclusion We can now save our model so as to put it in production and score future employees. # Let's now save our model from pysurvival.utils import save_model save_model(csf, '/Users/xxx/Desktop/employee_csf.zip' ) In conclusion, we can see that it is possible to predict the number of employees that will leave the company at different time points. Moreover, thanks to the feature importance of the CSF model, we can understand the reasons behind an employee decision to leave: # Computing variables importance csf . variable_importance_table . head( 5 ) Here is the top 5 of the most important features. feature importance pct_importance number_project 11.544101 0.341608 satisfaction_level 6.603040 0.195394 Work_accident 5.465851 0.161743 average_montly_hours 4.353429 0.128825 last_evaluation 4.118671 0.121878 Note: The importance is the difference in prediction error between the perturbed and unperturbed error rate as depicted by Breiman et al . References 2017 report by Employee Benefit News (EBN) HR Dive - Study: Turnover costs employers $15,000 per worker Employee Attrition dataset Kaggle Competition - Employee Churn Prediction","title":"Employee Retention"},{"location":"tutorials/employee_retention.html#knowing-when-your-employees-will-quit","text":"","title":"Knowing when your employees will quit"},{"location":"tutorials/employee_retention.html#1-introduction","text":"Employees attrition can be very costly for companies: reports show that it costs employers 33% of an employee's annual salary to hire a replacement if that worker leaves. Moreover, it can jeopardize productivity, cause loss of knowledge and curb staff morale. Thus, providing solutions that could predict employee turnover could be greatly beneficial for companies. Furthermore, by using Survival Analysis and taking into account the time dimension, predicting when an employee will quit becomes possible.","title":"1 - Introduction"},{"location":"tutorials/employee_retention.html#2-dataset","text":"In this tutorial, we will use the human resources dataset Employee Attrition dataset to demonstrate the usefulness of Survival Analysis.","title":"2 - Dataset"},{"location":"tutorials/employee_retention.html#21-overview","text":"Here, we will consider the following features: Feature category Feature name Type Description Time time_spend_company numerical Time spent at the company Event left categorical Specifies if the employee left the company Evaluation/Scoring satisfaction numerical Employee satisfaction level Evaluation/Scoring last_evaluation numerical Last evaluation score Day-to-Day activities number_projects numerical Number of projects assigned to the employee Day-to-Day activities average_monthly_hour numerical Average monthly hours worked Day-to-Day activities work_accident numerical Whether the employee has had a work accident Department department categorical Department name/Specialized functional area within the company Salary salary categorical Salary category # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'employee_attrition' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 3 ) Here is an overview of the raw dataset: satisfaction_level last_evaluation number_projects ... department salary 0.38 0.53 2 ... sales low 0.80 0.86 5 ... sales medium 0.11 0.88 7 ... sales medium","title":"2.1 - Overview"},{"location":"tutorials/employee_retention.html#22-from-categorical-to-numerical","text":"Let's encode the categorical features into one-hot vectors and define the modeling features: # Creating the time and event columns time_column = 'time_spend_company' event_column = 'left' # Creating one-hot vectors category_columns = [ 'department' , 'salary' ] dataset = pd . get_dummies(raw_dataset, columns = category_columns, drop_first = True ) dataset . head() # Creating the features features = np . setdiff1d(dataset . columns, [time_column, event_column] ) . tolist()","title":"2.2 - From categorical to numerical"},{"location":"tutorials/employee_retention.html#3-exploratory-data-analysis","text":"As this tutorial is mainly designed to provide an example of how to use PySurvival, we will not do a thorough exploratory data analysis here but greatly encourage the reader to do so by checking the predictive maintenance tutorial that provides a detailed analysis. Here, we will just check if the dataset contains Null values or if it has duplicated rows. Then, we will take a look at feature correlations.","title":"3 - Exploratory Data Analysis"},{"location":"tutorials/employee_retention.html#31-null-values-and-duplicates","text":"The first thing to do is checking if the dataset contains Null values and has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the dataset doesn't have any Null values but had 3,008 duplicated rows, that we removed.","title":"3.1 - Null values and duplicates"},{"location":"tutorials/employee_retention.html#32-correlations","text":"Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 20 , 10 ), text_fontsize =10 ) Figure 1 - Correlations This shows that there is a pretty big correlation between the features salaray_low and salary_medium . So we will be removing salaray_low . del dataset[ 'salary_low' ] features = np . setdiff1d( dataset . columns, [time_column, event_column] ) . tolist()","title":"3.2 - Correlations"},{"location":"tutorials/employee_retention.html#4-modeling","text":"As there are ~15,000 rows, we will first downsample the dataset to speed up computations, in case the computer that you are using cannot handle that size. Then, so as to perform cross-validation later on and assess the performance of the model, we will split the dataset into training and testing sets. # Downsampling the dataset to speed up computations indexes_choices = np . random . choice(N, int (N *0.3 ), replace = False ) . tolist() # Building training and testing sets # from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( indexes_choices, test_size = 0.4 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Conditional Survival Forest model (CSF) to the training set. Note: The choice of the model and hyperparameters was obtained using grid-search selection, not displayed in this tutorial. from pysurvival.models.survival_forest import ConditionalSurvivalForestModel # Fitting the model csf = ConditionalSurvivalForestModel(num_trees =200 ) csf . fit(X_train, T_train, E_train, max_features = 'sqrt' , alpha =0.05 , minprop =0.1 , max_depth =5 , min_node_size =30 )","title":"4 - Modeling"},{"location":"tutorials/employee_retention.html#5-cross-validation","text":"In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set:","title":"5 - Cross Validation"},{"location":"tutorials/employee_retention.html#51-c-index","text":"The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(csf, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.89","title":"5.1 - C-index"},{"location":"tutorials/employee_retention.html#52-brier-score","text":"The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score ibs = integrated_brier_score(csf, X_test, T_test, E_test, t_max =12 , figure_size = ( 15 , 5 )) print ( 'IBS: {:.2f}' . format(ibs)) Figure 2 - Conditional Survival Forest - Brier score & Prediction error curve The IBS is equal to 0.12 on the entire model time axis. This indicates that the model has good predictive abilities.","title":"5.2 - Brier Score"},{"location":"tutorials/employee_retention.html#6-predictions","text":"","title":"6 - Predictions"},{"location":"tutorials/employee_retention.html#61-overall-predictions","text":"Now that we have built a model that seems to provide great performances, let's compare the time series of the actual and predicted number of employees who left the company, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(csf, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 3 - Actual vs Predicted - Number of employees who left the company","title":"6.1 - Overall predictions"},{"location":"tutorials/employee_retention.html#62-individual-predictions","text":"Now that we know that we can provide reliable predictions for an entire cohort, let's compute the probability of remaining an employee for all times t. First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils.display , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = csf, X = X_test, use_log = False , num_bins =50 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 3.5 , 'color' : 'red' }, high = { 'lower_bound' : 3.5 , 'upper_bound' : 10 , 'color' : 'blue' } ) Figure 4 - Creating risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, it is possible to distinguish 2 main groups, low and high risk groups. Because the C-index is high, the model will be able to perfectly rank the survival times of a random unit of each group, such that t_{high} \\leq t_{low} . Let's randomly select individual unit in each group and compare their speed of repayment functions. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 8 )) # Selecting a random individual that experienced failure from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting an individual that experienced an event choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the Survival function for all times t survival = csf . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(csf . times, survival, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing Survival functions between {} risk grades\" . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 5 - Predicting individual probability to remain in the company","title":"6.2 - Individual predictions"},{"location":"tutorials/employee_retention.html#7-conclusion","text":"We can now save our model so as to put it in production and score future employees. # Let's now save our model from pysurvival.utils import save_model save_model(csf, '/Users/xxx/Desktop/employee_csf.zip' ) In conclusion, we can see that it is possible to predict the number of employees that will leave the company at different time points. Moreover, thanks to the feature importance of the CSF model, we can understand the reasons behind an employee decision to leave: # Computing variables importance csf . variable_importance_table . head( 5 ) Here is the top 5 of the most important features. feature importance pct_importance number_project 11.544101 0.341608 satisfaction_level 6.603040 0.195394 Work_accident 5.465851 0.161743 average_montly_hours 4.353429 0.128825 last_evaluation 4.118671 0.121878 Note: The importance is the difference in prediction error between the perturbed and unperturbed error rate as depicted by Breiman et al .","title":"7 - Conclusion"},{"location":"tutorials/employee_retention.html#references","text":"2017 report by Employee Benefit News (EBN) HR Dive - Study: Turnover costs employers $15,000 per worker Employee Attrition dataset Kaggle Competition - Employee Churn Prediction","title":"References"},{"location":"tutorials/maintenance.html","text":"h1, h2, h3, h4 { color: #04A9F4; } Predicting when a machine will break 1 - Introduction Predictive Maintenance (PdM) is a great application of Survival Analysis since it consists in predicting when equipment failure will occur and therefore alerting the maintenance team to prevent that failure. Indeed, accurately modeling if and when a machine will break is crucial for industrial and manufacturing businesses as it can help: maintain a safe work environment by ensuring that machines are working properly increase productivity by preventing unplanned reactive maintenance and minimizing downtime optimize costs by removing the need for too many unnecessary checks or repairs of components -- a.k.a preventative maintenance Within the past couple of years, thanks to the use of Internet of Things (IoT) technologies , a plethora of data has been generated by various sensors on machines, mechanical and electrical components, such as temperature, vibration, voltage or pressure. This type of information can be used to predict future failures. 2 - Set up We will consider that a manufacturing company uses many machines to build their final products. The factory is using IoT technologies via smart sensors to measure and save various kind of inputs from the physical environment and the state of their equipment. Unfortunately, every time a machine breaks the production is stopped, which costs the company thousands of dollars in repair and late delivery fees. The factory manager asks the company's Data Science team if it is possible to find a way to be more pro-active so as to optimize spending. 3 - Dataset The dataset the team wants to use contains the following features: Feature category Feature name Type Description Time lifetime numerical Number of weeks the machine has been active Event broken numerical Specifies if the machine was broken or hasn't been broken yet for the corresponding weeks in activity IoT measure pressureInd numerical The pressure index is used to quantify the flow of liquid through pipes, as a sudden drop of pressure can indicate a leak IoT measure moistureInd numerical The moisture index is a measure of the relative humidity in the air. It is important to keep track of it as excessive humidity can create mold and damage the equipment IoT measure temperatureInd numerical The temperature index of the machine is computed using voltage devices called thermocouples that translate a change in voltage into temperature measure. It is recorded to avoid damages to electric circuits, fire or even explosion Company feature team categorical This indicator specifies which team is using the machine Machine feature provider categorical This indicator specifies the name of the machine manufacturer 4 - Exploratory Data Analysis Let's perform an exploratory data analysis (EDA) so as to understand what the data look like and start answering interesting questions about our problem. # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'maintenance' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 3 ) Here is an overview of the raw dataset: lifetime broken pressureInd moistureInd temperatureInd team provider 56 0 92.17 104.23 96.51 TeamA Provider4 81 1 72.07 103.06 87.27 TeamC Provider4 60 0 96.27 77.80 112.19 TeamA Provider1 The following command is also very useful so as to assess the type of variables we're dealing with: raw_dataset . info() There are 3 numerical features ( pressureInd , moistureInd , temperatureInd ) and 2 categorical features ( team , provider ). Let's encode the categorical variables as one-hot vectors and define the modeling features: # Defining the time and event column time_column = 'lifetime' event_column = 'broken' # Encoding the categorical variables as one-hot vectors categories = [ 'provider' , 'team' ] dataset = pd . get_dummies(raw_dataset, columns = categories, drop_first = True ) # Defining the modeling features features = np . setdiff1d(dataset . columns, [ 'lifetime' , 'broken' ]) . tolist() 4.1 - Null values and duplicates The first thing to do is checking if the dataset contains Null values and if it has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the dataset doesn't have any Null values or duplicates. 4.2 - Visual exploration and statistics Let's check out/visualize the feature statistics: 4.2.1 - Numerical features We will display the boxplot and histogram of each feature for feature in [ 'pressureInd' , 'moistureInd' , 'temperatureInd' ]: # Creating an empty chart fig, ((ax1, ax2)) = plt . subplots( 1 , 2 , figsize = ( 15 , 4 )) # Extracting the feature values x = raw_dataset[feature] . values # Boxplot ax1 . boxplot(x) ax1 . set_title( 'Boxplot for {}' . format(feature) ) # Histogram ax2 . hist(x, bins =20 ) ax2 . set_title( 'Histogram for {}' . format(feature) ) # Display plt . show() Figure 1 - Boxplot and histogram for moistureInd Figure 2 - Boxplot and histogram for pressureInd Figure 3 - Boxplot and histogram for temperatureInd These features have very few outliers ( here, there's no real need to remove them, but you can if you prefer ) and seem to follow normal distributions. 4.2.2 - Categorical features We will display the occurrences of the categories in a barchart for each feature from collections import Counter for feature in [ 'team' , 'provider' ]: # Creating an empty chart fig, ax = plt . subplots(figsize = ( 15 , 4 )) # Extracting the feature values x = raw_dataset[feature] . values # Counting the number of occurrences for each category data = Counter(x) category = list (data . keys()) counts = list (data . values()) # Boxplot ax . bar(category, counts) # Display plt . title( 'Barchart for {}' . format(feature) ) plt . show() Figure 4 - Barchart for team Figure 5 - Barchart for provider These features seem to be uniformly distributed. 4.2.3 - Time & Event We will display the occurrences of event and censorship, as well as the distribution of the time output variable for both situations. # Creating an empty chart fig, ((ax1, ax2)) = plt . subplots( 1 , 2 , figsize = ( 15 , 4 )) # Counting the number of occurrences for each category data = Counter(raw_dataset[ 'broken' ] . replace({ 0 : 'not broken yet' , 1 : 'broken' })) category = list (data . keys()) counts = list (data . values()) idx = range ( len (counts)) # Displaying the occurrences of the event/censoring ax1 . bar(idx, counts) ax1 . set_xticks(idx) ax1 . set_xticklabels(category) ax1 . set_title( 'Occurences of the event/censoring' , fontsize =15 ) # Showing the histogram of the survival times for the censoring time_0 = raw_dataset . loc[ raw_dataset[ 'broken' ] == 0 , 'lifetime' ] ax2 . hist(time_0, bins =30 , alpha =0.3 , color = 'blue' , label = 'not broken yet' ) # Showing the histogram of the survival times for the events time_1 = raw_dataset . loc[ raw_dataset[ 'broken' ] == 1 , 'lifetime' ] ax2 . hist(time_1, bins =20 , alpha =0.7 , color = 'black' , label = 'broken' ) ax2 . set_title( 'Histogram - survival time' , fontsize =15 ) # Displaying everything side-by-side plt . legend(fontsize =15 ) plt . show() Figure 6 - Time/Event summary Here, we can see that 2/3 of the data is censored and that the failures start happening when the machine has been active for at least 60 weeks. 4.3 - Correlations Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 15 , 5 )) Figure 7 - Correlations As we can see, there aren't any alarming correlations. 5 - Modeling So as to perform cross-validation later on and assess the performances of the model, let's split the dataset into training and testing sets. # Building training and testing sets from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( range (N), test_size = 0.4 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Linear MTLR model to the training set. from pysurvival.models.multi_task import LinearMultiTaskModel # Initializing the MTLR with a time axis split into 300 intervals linear_mtlr = LinearMultiTaskModel(bins =300 ) # Fitting the model linear_mtlr . fit(X_train, T_train, E_train, num_epochs = 1000 , init_method = 'orthogonal' , optimizer = 'rmsprop' , lr = 1e-3 , l2_reg = 3 , l2_smooth = 3 , ) We can take a look at the loss function values from pysurvival.utils.display import display_loss_values display_loss_values(linear_mtlr, figure_size = ( 7 , 4 )) Figure 8 - Linear MTLR loss function values 6 - Cross Validation In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set: 6.1 - C-index The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(linear_mtlr, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.92 As the c-index (0.92 here) is close to 1, it seems that the model will yield satisfactory results in terms of survival times predictions. 6.2 - Brier Score The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score integrated_brier_score(linear_mtlr, X_test, T_test, E_test, t_max =100 , figure_size = ( 20 , 6.5 ) ) Figure 9 - Linear MTLR - Brier scores & Prediction error curve The IBS is very close to 0.0 on the entire model time axis. This indicates that the model will have very good predictive abilities. 7 - Predictions 7.1 - Overall predictions Now that we have built a model that seems to provide great performances, let's compare the time series of the actual and predicted number of machines experiencing a failure, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(linear_mtlr, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 10 - Actual vs Predicted - Number of machines experiencing a failure Based on the performance metrics, it was expected that the time series would be very close; here the model makes an average error of ~1 machine throughout the entire timeline. 7.2 - Individual predictions Now that we know that we can provide reliable predictions for an entire cohort. Let's compute the survival predictions at the individual level. First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = linear_mtlr, X = X_test, use_log = True , num_bins =50 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 1.8 , 'color' : 'red' }, medium = { 'lower_bound' : 1.8 , 'upper_bound' : 1.93 , 'color' : 'green' }, high = { 'lower_bound' : 1.93 , 'upper_bound' : 2.1 , 'color' : 'blue' } ) Figure 11 - Creating Risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, we can see that 3 main groups, low , medium and high risk groups, can be created. Because the C-index is high, the model will be able to perfectly rank the survival times of a random unit of each group, such that t_{high} \\leq t_{medium} \\leq t_{low} . Let's randomly select individual unit in each group and compare the survival functions. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 8 )) # Selecting a random individual that experienced failure from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting a machine that experienced failure from each group choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the Survival function for all times t survival = linear_mtlr . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(linear_mtlr . times, survival, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing Survival functions between {} risk grades\" . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 12 - Predicting individual survival functions As we can see, the model manages to perfectly predict the event time, here it corresponds to a sudden drop in the individual survival function. 8 - Conclusion We can now save our model so as to put it in production and score future machines. # Let's now save our model from pysurvival.utils import save_model save_model(linear_mtlr, '/Users/xxx/Desktop/pdm_linear_mtlr.zip' ) In this example, we have shown that it is possible to predict with great degree of certainty when a machine will fail. The Data Science team could predict the machines survival function every day, so that 1 or 2 weeks before the machine is supposed to fail, the factory manager is notified so that the necessary actions can be taken. References https://en.wikipedia.org/wiki/Predictive_maintenance Essec Business School - Course in Business Analytics Maintenance dataset https://github.com/nicolasfguillaume/Strategic-Business-Analytics-with-R/blob/master/module4.md","title":"Predictive Maintenance"},{"location":"tutorials/maintenance.html#predicting-when-a-machine-will-break","text":"","title":"Predicting when a machine will break"},{"location":"tutorials/maintenance.html#1-introduction","text":"Predictive Maintenance (PdM) is a great application of Survival Analysis since it consists in predicting when equipment failure will occur and therefore alerting the maintenance team to prevent that failure. Indeed, accurately modeling if and when a machine will break is crucial for industrial and manufacturing businesses as it can help: maintain a safe work environment by ensuring that machines are working properly increase productivity by preventing unplanned reactive maintenance and minimizing downtime optimize costs by removing the need for too many unnecessary checks or repairs of components -- a.k.a preventative maintenance Within the past couple of years, thanks to the use of Internet of Things (IoT) technologies , a plethora of data has been generated by various sensors on machines, mechanical and electrical components, such as temperature, vibration, voltage or pressure. This type of information can be used to predict future failures.","title":"1 - Introduction"},{"location":"tutorials/maintenance.html#2-set-up","text":"We will consider that a manufacturing company uses many machines to build their final products. The factory is using IoT technologies via smart sensors to measure and save various kind of inputs from the physical environment and the state of their equipment. Unfortunately, every time a machine breaks the production is stopped, which costs the company thousands of dollars in repair and late delivery fees. The factory manager asks the company's Data Science team if it is possible to find a way to be more pro-active so as to optimize spending.","title":"2 - Set up"},{"location":"tutorials/maintenance.html#3-dataset","text":"The dataset the team wants to use contains the following features: Feature category Feature name Type Description Time lifetime numerical Number of weeks the machine has been active Event broken numerical Specifies if the machine was broken or hasn't been broken yet for the corresponding weeks in activity IoT measure pressureInd numerical The pressure index is used to quantify the flow of liquid through pipes, as a sudden drop of pressure can indicate a leak IoT measure moistureInd numerical The moisture index is a measure of the relative humidity in the air. It is important to keep track of it as excessive humidity can create mold and damage the equipment IoT measure temperatureInd numerical The temperature index of the machine is computed using voltage devices called thermocouples that translate a change in voltage into temperature measure. It is recorded to avoid damages to electric circuits, fire or even explosion Company feature team categorical This indicator specifies which team is using the machine Machine feature provider categorical This indicator specifies the name of the machine manufacturer","title":"3 - Dataset"},{"location":"tutorials/maintenance.html#4-exploratory-data-analysis","text":"Let's perform an exploratory data analysis (EDA) so as to understand what the data look like and start answering interesting questions about our problem. # Importing modules import pandas as pd import numpy as np from matplotlib import pyplot as plt from pysurvival.datasets import Dataset % pylab inline # Reading the dataset raw_dataset = Dataset( 'maintenance' ) . load() print ( \"The raw_dataset has the following shape: {}.\" . format(raw_dataset . shape)) raw_dataset . head( 3 ) Here is an overview of the raw dataset: lifetime broken pressureInd moistureInd temperatureInd team provider 56 0 92.17 104.23 96.51 TeamA Provider4 81 1 72.07 103.06 87.27 TeamC Provider4 60 0 96.27 77.80 112.19 TeamA Provider1 The following command is also very useful so as to assess the type of variables we're dealing with: raw_dataset . info() There are 3 numerical features ( pressureInd , moistureInd , temperatureInd ) and 2 categorical features ( team , provider ). Let's encode the categorical variables as one-hot vectors and define the modeling features: # Defining the time and event column time_column = 'lifetime' event_column = 'broken' # Encoding the categorical variables as one-hot vectors categories = [ 'provider' , 'team' ] dataset = pd . get_dummies(raw_dataset, columns = categories, drop_first = True ) # Defining the modeling features features = np . setdiff1d(dataset . columns, [ 'lifetime' , 'broken' ]) . tolist()","title":"4 - Exploratory Data Analysis"},{"location":"tutorials/maintenance.html#41-null-values-and-duplicates","text":"The first thing to do is checking if the dataset contains Null values and if it has duplicated rows. # Checking for null values N_null = sum (dataset[features] . isnull() . sum()) print ( \"The dataset contains {} null values\" . format(N_null)) #0 null values # Removing duplicates if there exist N_dupli = sum (dataset . duplicated(keep = 'first' )) dataset = dataset . drop_duplicates(keep = 'first' ) . reset_index(drop = True ) print ( \"The dataset contains {} duplicates\" . format(N_dupli)) # Number of samples in the dataset N = dataset . shape[ 0 ] As it turns out the dataset doesn't have any Null values or duplicates.","title":"4.1 - Null values and duplicates"},{"location":"tutorials/maintenance.html#42-visual-exploration-and-statistics","text":"Let's check out/visualize the feature statistics:","title":"4.2 - Visual exploration and statistics"},{"location":"tutorials/maintenance.html#421-numerical-features","text":"We will display the boxplot and histogram of each feature for feature in [ 'pressureInd' , 'moistureInd' , 'temperatureInd' ]: # Creating an empty chart fig, ((ax1, ax2)) = plt . subplots( 1 , 2 , figsize = ( 15 , 4 )) # Extracting the feature values x = raw_dataset[feature] . values # Boxplot ax1 . boxplot(x) ax1 . set_title( 'Boxplot for {}' . format(feature) ) # Histogram ax2 . hist(x, bins =20 ) ax2 . set_title( 'Histogram for {}' . format(feature) ) # Display plt . show() Figure 1 - Boxplot and histogram for moistureInd Figure 2 - Boxplot and histogram for pressureInd Figure 3 - Boxplot and histogram for temperatureInd These features have very few outliers ( here, there's no real need to remove them, but you can if you prefer ) and seem to follow normal distributions.","title":"4.2.1 - Numerical features"},{"location":"tutorials/maintenance.html#422-categorical-features","text":"We will display the occurrences of the categories in a barchart for each feature from collections import Counter for feature in [ 'team' , 'provider' ]: # Creating an empty chart fig, ax = plt . subplots(figsize = ( 15 , 4 )) # Extracting the feature values x = raw_dataset[feature] . values # Counting the number of occurrences for each category data = Counter(x) category = list (data . keys()) counts = list (data . values()) # Boxplot ax . bar(category, counts) # Display plt . title( 'Barchart for {}' . format(feature) ) plt . show() Figure 4 - Barchart for team Figure 5 - Barchart for provider These features seem to be uniformly distributed.","title":"4.2.2 - Categorical features"},{"location":"tutorials/maintenance.html#423-time-event","text":"We will display the occurrences of event and censorship, as well as the distribution of the time output variable for both situations. # Creating an empty chart fig, ((ax1, ax2)) = plt . subplots( 1 , 2 , figsize = ( 15 , 4 )) # Counting the number of occurrences for each category data = Counter(raw_dataset[ 'broken' ] . replace({ 0 : 'not broken yet' , 1 : 'broken' })) category = list (data . keys()) counts = list (data . values()) idx = range ( len (counts)) # Displaying the occurrences of the event/censoring ax1 . bar(idx, counts) ax1 . set_xticks(idx) ax1 . set_xticklabels(category) ax1 . set_title( 'Occurences of the event/censoring' , fontsize =15 ) # Showing the histogram of the survival times for the censoring time_0 = raw_dataset . loc[ raw_dataset[ 'broken' ] == 0 , 'lifetime' ] ax2 . hist(time_0, bins =30 , alpha =0.3 , color = 'blue' , label = 'not broken yet' ) # Showing the histogram of the survival times for the events time_1 = raw_dataset . loc[ raw_dataset[ 'broken' ] == 1 , 'lifetime' ] ax2 . hist(time_1, bins =20 , alpha =0.7 , color = 'black' , label = 'broken' ) ax2 . set_title( 'Histogram - survival time' , fontsize =15 ) # Displaying everything side-by-side plt . legend(fontsize =15 ) plt . show() Figure 6 - Time/Event summary Here, we can see that 2/3 of the data is censored and that the failures start happening when the machine has been active for at least 60 weeks.","title":"4.2.3 - Time &amp; Event"},{"location":"tutorials/maintenance.html#43-correlations","text":"Let's compute and visualize the correlation between the features from pysurvival.utils.display import correlation_matrix correlation_matrix(dataset[features], figure_size = ( 15 , 5 )) Figure 7 - Correlations As we can see, there aren't any alarming correlations.","title":"4.3 - Correlations"},{"location":"tutorials/maintenance.html#5-modeling","text":"So as to perform cross-validation later on and assess the performances of the model, let's split the dataset into training and testing sets. # Building training and testing sets from sklearn.model_selection import train_test_split index_train, index_test = train_test_split( range (N), test_size = 0.4 ) data_train = dataset . loc[index_train] . reset_index( drop = True ) data_test = dataset . loc[index_test] . reset_index( drop = True ) # Creating the X, T and E inputs X_train, X_test = data_train[features], data_test[features] T_train, T_test = data_train[time_column], data_test[time_column] E_train, E_test = data_train[event_column], data_test[event_column] Let's now fit a Linear MTLR model to the training set. from pysurvival.models.multi_task import LinearMultiTaskModel # Initializing the MTLR with a time axis split into 300 intervals linear_mtlr = LinearMultiTaskModel(bins =300 ) # Fitting the model linear_mtlr . fit(X_train, T_train, E_train, num_epochs = 1000 , init_method = 'orthogonal' , optimizer = 'rmsprop' , lr = 1e-3 , l2_reg = 3 , l2_smooth = 3 , ) We can take a look at the loss function values from pysurvival.utils.display import display_loss_values display_loss_values(linear_mtlr, figure_size = ( 7 , 4 )) Figure 8 - Linear MTLR loss function values","title":"5 - Modeling"},{"location":"tutorials/maintenance.html#6-cross-validation","text":"In order to assess the model performance, we previously split the original dataset into training and testing sets, so that we can now compute its performance metrics on the testing set:","title":"6 - Cross Validation"},{"location":"tutorials/maintenance.html#61-c-index","text":"The C-index represents the global assessment of the model discrimination power: this is the model\u2019s ability to correctly provide a reliable ranking of the survival times based on the individual risk scores . In general, when the C-index is close to 1, the model has an almost perfect discriminatory power; but if it is close to 0.5, it has no ability to discriminate between low and high risk subjects. from pysurvival.utils.metrics import concordance_index c_index = concordance_index(linear_mtlr, X_test, T_test, E_test) print ( 'C-index: {:.2f}' . format(c_index)) #0.92 As the c-index (0.92 here) is close to 1, it seems that the model will yield satisfactory results in terms of survival times predictions.","title":"6.1 - C-index"},{"location":"tutorials/maintenance.html#62-brier-score","text":"The Brier score measures the average discrepancies between the status and the estimated probabilities at a given time. Thus, the lower the score ( usually below 0.25 ), the better the predictive performance. To assess the overall error measure across multiple time points, the Integrated Brier Score (IBS) is usually computed as well. from pysurvival.utils.display import integrated_brier_score integrated_brier_score(linear_mtlr, X_test, T_test, E_test, t_max =100 , figure_size = ( 20 , 6.5 ) ) Figure 9 - Linear MTLR - Brier scores & Prediction error curve The IBS is very close to 0.0 on the entire model time axis. This indicates that the model will have very good predictive abilities.","title":"6.2 - Brier Score"},{"location":"tutorials/maintenance.html#7-predictions","text":"","title":"7 - Predictions"},{"location":"tutorials/maintenance.html#71-overall-predictions","text":"Now that we have built a model that seems to provide great performances, let's compare the time series of the actual and predicted number of machines experiencing a failure, for each time t. from pysurvival.utils.display import compare_to_actual results = compare_to_actual(linear_mtlr, X_test, T_test, E_test, is_at_risk = False , figure_size = ( 16 , 6 ), metrics = [ 'rmse' , 'mean' , 'median' ]) Figure 10 - Actual vs Predicted - Number of machines experiencing a failure Based on the performance metrics, it was expected that the time series would be very close; here the model makes an average error of ~1 machine throughout the entire timeline.","title":"7.1 - Overall predictions"},{"location":"tutorials/maintenance.html#72-individual-predictions","text":"Now that we know that we can provide reliable predictions for an entire cohort. Let's compute the survival predictions at the individual level. First, we can construct the risk groups based on risk scores distribution. The helper function create_risk_groups , which can be found in pysurvival.utils , will help us do that: from pysurvival.utils.display import create_risk_groups risk_groups = create_risk_groups(model = linear_mtlr, X = X_test, use_log = True , num_bins =50 , figure_size = ( 20 , 4 ), low = { 'lower_bound' : 0 , 'upper_bound' : 1.8 , 'color' : 'red' }, medium = { 'lower_bound' : 1.8 , 'upper_bound' : 1.93 , 'color' : 'green' }, high = { 'lower_bound' : 1.93 , 'upper_bound' : 2.1 , 'color' : 'blue' } ) Figure 11 - Creating Risk groups Note: The current choice of the lower and upper bounds for each group is based on my intuition; so feel free to change the values so as to match your situation instead. Here, we can see that 3 main groups, low , medium and high risk groups, can be created. Because the C-index is high, the model will be able to perfectly rank the survival times of a random unit of each group, such that t_{high} \\leq t_{medium} \\leq t_{low} . Let's randomly select individual unit in each group and compare the survival functions. To demonstrate our point, we will purposely select units which experienced an event to visualize the actual time of event. # Initializing the figure fig, ax = plt . subplots(figsize = ( 15 , 8 )) # Selecting a random individual that experienced failure from each group groups = [] for i, (label, (color, indexes)) in enumerate (risk_groups . items()) : # Selecting the individuals that belong to this group if len (indexes) == 0 : continue X = X_test . values[indexes, :] T = T_test . values[indexes] E = E_test . values[indexes] # Randomly extracting a machine that experienced failure from each group choices = np . argwhere((E ==1. )) . flatten() if len (choices) == 0 : continue k = np . random . choice( choices, 1 )[ 0 ] # Saving the time of event t = T[k] # Computing the Survival function for all times t survival = linear_mtlr . predict_survival(X[k, :]) . flatten() # Displaying the functions label_ = '{} risk' . format(label) plt . plot(linear_mtlr . times, survival, color = color, label = label_, lw =2 ) groups . append(label) # Actual time plt . axvline(x = t, color = color, ls = '--' ) ax . annotate( 'T={:.1f}' . format(t), xy = (t, 0.5* ( 1.+0.2* i)), xytext = (t, 0.5* ( 1.+0.2* i)), fontsize =12 ) # Show everything groups_str = ', ' . join(groups) title = \"Comparing Survival functions between {} risk grades\" . format(groups_str) plt . legend(fontsize =12 ) plt . title(title, fontsize =15 ) plt . ylim( 0 , 1.05 ) plt . show() Figure 12 - Predicting individual survival functions As we can see, the model manages to perfectly predict the event time, here it corresponds to a sudden drop in the individual survival function.","title":"7.2 - Individual predictions"},{"location":"tutorials/maintenance.html#8-conclusion","text":"We can now save our model so as to put it in production and score future machines. # Let's now save our model from pysurvival.utils import save_model save_model(linear_mtlr, '/Users/xxx/Desktop/pdm_linear_mtlr.zip' ) In this example, we have shown that it is possible to predict with great degree of certainty when a machine will fail. The Data Science team could predict the machines survival function every day, so that 1 or 2 weeks before the machine is supposed to fail, the factory manager is notified so that the necessary actions can be taken.","title":"8 - Conclusion"},{"location":"tutorials/maintenance.html#references","text":"https://en.wikipedia.org/wiki/Predictive_maintenance Essec Business School - Course in Business Analytics Maintenance dataset https://github.com/nicolasfguillaume/Strategic-Business-Analytics-with-R/blob/master/module4.md","title":"References"}]}
